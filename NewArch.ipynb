{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfb776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models,Sequential,constraints\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Dropout,Dense,Flatten,Layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot,colors\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57348630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "spectra=pd.read_csv('Spectra_Checked_baseline.csv',header=0)\n",
    "print((spectra.isnull().any()).any())\n",
    "X=spectra.iloc[:,1:].values\n",
    "labels=pd.read_csv('Labels_Checked_baseline.csv',header=0)\n",
    "Y=labels.iloc[:,1:].values\n",
    "X=X.transpose()\n",
    "Y=Y.transpose()\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_train=np.expand_dims(X_train,axis=2)\n",
    "X_test=np.expand_dims(X_test,axis=2)\n",
    "df=pd.read_csv('Basis_Spectra_Checked.csv',header=0)\n",
    "basis=df.iloc[:,2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7986520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lorentz_layer(Layer):\n",
    "    def __init__(self, units=5,kernel_constraint=None,\n",
    "               bias_constraint=None,**kwargs):\n",
    "        super(Lorentz_layer,self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.w = tf.Variable(name=\"kernel\",   initial_value=90*tf.eye(input_shape[-1], num_columns=self.units,\n",
    "                 dtype='float32'),constraint=self.kernel_constraint,trainable=True)\n",
    "        \n",
    "        b_init = tf.keras.initializers.RandomUniform(minval=0, maxval=100, seed=None)\n",
    "        self.b = tf.Variable(name=\"bias\",initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "                             constraint=self.bias_constraint,\n",
    "                             trainable=True)\n",
    "        self.x=(np.linspace(0,3600,901,dtype='float32')).reshape(901,1)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x0=tf.matmul(inputs,self.w)+K.epsilon()\n",
    "        #m='-----------------------------------------'\n",
    "        #tf.print(m)\n",
    "        gamma=self.b+K.epsilon()\n",
    "        val=(1/(np.pi*gamma))*(1/(1+(tf.subtract(self.x,tf.expand_dims(x0,axis=1))/gamma)**2))\n",
    "        #val=tf.divide(tf.subtract(val, tf.reduce_min(val)), tf.subtract(tf.reduce_max(val), tf.reduce_min(val)))\n",
    "        return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d031e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum_layer(Layer):\n",
    "    def __init__(self, units=1,kernel_constraint=None,\n",
    "               bias_constraint=None,**kwargs):\n",
    "        super(sum_layer,self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "    def build(self,input_shape):\n",
    "        w_init = tf.keras.initializers.RandomUniform(minval=0.01, maxval=1, seed=None)\n",
    "        self.w = tf.Variable(name=\"kernel\",   initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                 dtype='float32'),constraint=self.kernel_constraint,trainable=True)\n",
    "        \n",
    "        self.b = None\n",
    "    def call(self,inputs):\n",
    "        val=tf.matmul(inputs,self.w)\n",
    "    \n",
    "        val=tf.divide(tf.subtract(val, tf.reduce_min(val)), tf.subtract(tf.reduce_max(val), tf.reduce_min(val))+K.epsilon())\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33d31be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 901, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 901, 10)           60        \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 450, 10)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 450, 10)           510       \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 225, 10)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 225, 10)           0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2250)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                45020     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 15)                315       \n",
      "                                                                 \n",
      " Labels (Dense)              (None, 13)                208       \n",
      "                                                                 \n",
      " X0 (Dense)                  (None, 20)                280       \n",
      "                                                                 \n",
      " lorentz_layer_3 (Lorentz_la  (None, 901, 20)          420       \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " sum_layer_3 (sum_layer)     (None, 901, 1)            20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,833\n",
      "Trainable params: 46,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_layer=Lorentz_layer(units=20,kernel_constraint=constraints.NonNeg(),bias_constraint=constraints.NonNeg())\n",
    "final_layer=sum_layer(kernel_constraint=constraints.NonNeg())\n",
    "Input_layer=keras.Input(shape=(901,1))\n",
    "conv1=Conv1D(filters=10, kernel_size=5,activation='relu',padding='same')\n",
    "pool1=MaxPooling1D()\n",
    "pool2=MaxPooling1D()\n",
    "conv2=Conv1D(filters=10, kernel_size=5,activation='relu',padding='same')\n",
    "drop=Dropout(0.2)\n",
    "flat=Flatten()\n",
    "Dense1=Dense(20,activation='relu')\n",
    "Dense2=Dense(15,activation='relu')\n",
    "Dense3=Dense(20,activation='relu',name='X0')\n",
    "Sig=Dense(13,activation='sigmoid',name='Labels')\n",
    "x=conv1(Input_layer)\n",
    "x=pool1(x)\n",
    "x=conv2(x)\n",
    "x=pool2(x)\n",
    "x=drop(x)\n",
    "x=flat(x)\n",
    "x=Dense1(x)\n",
    "x=Dense2(x)\n",
    "op_label=Sig(x)\n",
    "y=Dense3(op_label)\n",
    "spec=my_layer(y)\n",
    "op_spec=final_layer(spec)\n",
    "\n",
    "model=keras.Model(Input_layer,[op_label,op_spec])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18a3652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.21286307  0.59215356]\n",
      " [ 0.57663092  3.76239067]\n",
      " [ 0.51149425 22.25      ]\n",
      " [ 1.22554606  0.84456806]\n",
      " [ 0.5482158   5.68502203]\n",
      " [ 0.51217092 21.04076087]\n",
      " [ 0.56700351  4.23114754]\n",
      " [ 0.53651608  7.34629981]\n",
      " [ 0.5258047  10.18815789]\n",
      " [ 0.57749105  3.72617902]\n",
      " [ 0.56222771  4.51750292]\n",
      " [ 0.66406518  2.02378463]\n",
      " [ 0.52874898  9.195962  ]]\n"
     ]
    }
   ],
   "source": [
    "def calculating_class_weights(y_true):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', classes=[0.,1.], y=y_true[:, i])\n",
    "    return weights\n",
    "weights=calculating_class_weights(y_train)\n",
    "print(weights)\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        y_true=tf.cast(y_true,tf.float32)\n",
    "        y_pred=tf.cast(y_pred,tf.float32)\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e39d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDerror(y_true,y_pred):\n",
    "    x=K.softmax(y_true,axis=1)\n",
    "    y=K.softmax(y_pred,axis=1)\n",
    "    #x = K.clip(x, K.epsilon(), 1)\n",
    "    #y = K.clip(y, K.epsilon(), 1)\n",
    "    kl=keras.losses.KLDivergence()\n",
    "    return kl(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "526d7a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "242/242 [==============================] - 6s 19ms/step - loss: 0.5815 - Labels_loss: 0.5587 - sum_layer_3_loss: 2.2800e-05\n",
      "Epoch 2/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.4539 - Labels_loss: 0.4326 - sum_layer_3_loss: 2.1296e-05\n",
      "Epoch 3/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.4066 - Labels_loss: 0.3864 - sum_layer_3_loss: 2.0205e-05\n",
      "Epoch 4/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3808 - Labels_loss: 0.3611 - sum_layer_3_loss: 1.9698e-05\n",
      "Epoch 5/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3648 - Labels_loss: 0.3452 - sum_layer_3_loss: 1.9584e-05\n",
      "Epoch 6/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3518 - Labels_loss: 0.3323 - sum_layer_3_loss: 1.9522e-05\n",
      "Epoch 7/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3413 - Labels_loss: 0.3218 - sum_layer_3_loss: 1.9464e-05\n",
      "Epoch 8/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3293 - Labels_loss: 0.3099 - sum_layer_3_loss: 1.9417e-05\n",
      "Epoch 9/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3228 - Labels_loss: 0.3034 - sum_layer_3_loss: 1.9368e-05\n",
      "Epoch 10/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3156 - Labels_loss: 0.2962 - sum_layer_3_loss: 1.9330e-05\n",
      "Epoch 11/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3100 - Labels_loss: 0.2907 - sum_layer_3_loss: 1.9258e-05\n",
      "Epoch 12/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.3037 - Labels_loss: 0.2846 - sum_layer_3_loss: 1.9166e-05\n",
      "Epoch 13/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2993 - Labels_loss: 0.2802 - sum_layer_3_loss: 1.9041e-05\n",
      "Epoch 14/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2946 - Labels_loss: 0.2756 - sum_layer_3_loss: 1.8943e-05\n",
      "Epoch 15/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2903 - Labels_loss: 0.2713 - sum_layer_3_loss: 1.8906e-05\n",
      "Epoch 16/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2860 - Labels_loss: 0.2672 - sum_layer_3_loss: 1.8841e-05\n",
      "Epoch 17/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2850 - Labels_loss: 0.2662 - sum_layer_3_loss: 1.8832e-05\n",
      "Epoch 18/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2806 - Labels_loss: 0.2618 - sum_layer_3_loss: 1.8798e-05\n",
      "Epoch 19/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2754 - Labels_loss: 0.2566 - sum_layer_3_loss: 1.8770e-05\n",
      "Epoch 20/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2706 - Labels_loss: 0.2518 - sum_layer_3_loss: 1.8729e-05\n",
      "Epoch 21/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2720 - Labels_loss: 0.2533 - sum_layer_3_loss: 1.8682e-05\n",
      "Epoch 22/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2680 - Labels_loss: 0.2494 - sum_layer_3_loss: 1.8626e-05\n",
      "Epoch 23/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2627 - Labels_loss: 0.2441 - sum_layer_3_loss: 1.8550e-05\n",
      "Epoch 24/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2592 - Labels_loss: 0.2408 - sum_layer_3_loss: 1.8430e-05\n",
      "Epoch 25/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2581 - Labels_loss: 0.2398 - sum_layer_3_loss: 1.8321e-05\n",
      "Epoch 26/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2544 - Labels_loss: 0.2362 - sum_layer_3_loss: 1.8227e-05\n",
      "Epoch 27/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2522 - Labels_loss: 0.2340 - sum_layer_3_loss: 1.8153e-05\n",
      "Epoch 28/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2493 - Labels_loss: 0.2312 - sum_layer_3_loss: 1.8082e-05\n",
      "Epoch 29/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2482 - Labels_loss: 0.2302 - sum_layer_3_loss: 1.8003e-05\n",
      "Epoch 30/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2480 - Labels_loss: 0.2301 - sum_layer_3_loss: 1.7939e-05\n",
      "Epoch 31/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2424 - Labels_loss: 0.2245 - sum_layer_3_loss: 1.7882e-05\n",
      "Epoch 32/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2393 - Labels_loss: 0.2214 - sum_layer_3_loss: 1.7840e-05\n",
      "Epoch 33/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2389 - Labels_loss: 0.2211 - sum_layer_3_loss: 1.7786e-05\n",
      "Epoch 34/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2341 - Labels_loss: 0.2164 - sum_layer_3_loss: 1.7749e-05\n",
      "Epoch 35/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2328 - Labels_loss: 0.2151 - sum_layer_3_loss: 1.7709e-05\n",
      "Epoch 36/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2320 - Labels_loss: 0.2143 - sum_layer_3_loss: 1.7683e-05\n",
      "Epoch 37/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2324 - Labels_loss: 0.2148 - sum_layer_3_loss: 1.7674e-05\n",
      "Epoch 38/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2298 - Labels_loss: 0.2122 - sum_layer_3_loss: 1.7651e-05\n",
      "Epoch 39/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2296 - Labels_loss: 0.2119 - sum_layer_3_loss: 1.7623e-05\n",
      "Epoch 40/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2277 - Labels_loss: 0.2101 - sum_layer_3_loss: 1.7596e-05\n",
      "Epoch 41/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2260 - Labels_loss: 0.2084 - sum_layer_3_loss: 1.7588e-05\n",
      "Epoch 42/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2248 - Labels_loss: 0.2072 - sum_layer_3_loss: 1.7569e-05\n",
      "Epoch 43/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.2227 - Labels_loss: 0.2052 - sum_layer_3_loss: 1.7565e-05\n",
      "Epoch 44/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2213 - Labels_loss: 0.2037 - sum_layer_3_loss: 1.7548e-05\n",
      "Epoch 45/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2203 - Labels_loss: 0.2028 - sum_layer_3_loss: 1.7534e-05\n",
      "Epoch 46/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2166 - Labels_loss: 0.1991 - sum_layer_3_loss: 1.7519e-05\n",
      "Epoch 47/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2185 - Labels_loss: 0.2010 - sum_layer_3_loss: 1.7501e-05\n",
      "Epoch 48/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2141 - Labels_loss: 0.1966 - sum_layer_3_loss: 1.7510e-05\n",
      "Epoch 49/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2131 - Labels_loss: 0.1956 - sum_layer_3_loss: 1.7494e-05\n",
      "Epoch 50/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2149 - Labels_loss: 0.1975 - sum_layer_3_loss: 1.7480e-05\n",
      "Epoch 51/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2151 - Labels_loss: 0.1977 - sum_layer_3_loss: 1.7479e-05\n",
      "Epoch 52/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2133 - Labels_loss: 0.1959 - sum_layer_3_loss: 1.7467e-05\n",
      "Epoch 53/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2106 - Labels_loss: 0.1932 - sum_layer_3_loss: 1.7426e-05\n",
      "Epoch 54/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2093 - Labels_loss: 0.1918 - sum_layer_3_loss: 1.7426e-05\n",
      "Epoch 55/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2149 - Labels_loss: 0.1975 - sum_layer_3_loss: 1.7416e-05\n",
      "Epoch 56/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2120 - Labels_loss: 0.1946 - sum_layer_3_loss: 1.7406e-05\n",
      "Epoch 57/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2041 - Labels_loss: 0.1867 - sum_layer_3_loss: 1.7405e-05\n",
      "Epoch 58/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2060 - Labels_loss: 0.1887 - sum_layer_3_loss: 1.7386e-05\n",
      "Epoch 59/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2051 - Labels_loss: 0.1877 - sum_layer_3_loss: 1.7397e-05\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2067 - Labels_loss: 0.1893 - sum_layer_3_loss: 1.7381e-05\n",
      "Epoch 61/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2016 - Labels_loss: 0.1843 - sum_layer_3_loss: 1.7358e-05\n",
      "Epoch 62/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2042 - Labels_loss: 0.1868 - sum_layer_3_loss: 1.7359e-05\n",
      "Epoch 63/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2022 - Labels_loss: 0.1849 - sum_layer_3_loss: 1.7352e-05\n",
      "Epoch 64/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2001 - Labels_loss: 0.1828 - sum_layer_3_loss: 1.7350e-05\n",
      "Epoch 65/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2008 - Labels_loss: 0.1835 - sum_layer_3_loss: 1.7325e-05\n",
      "Epoch 66/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.2009 - Labels_loss: 0.1836 - sum_layer_3_loss: 1.7339e-05\n",
      "Epoch 67/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1990 - Labels_loss: 0.1817 - sum_layer_3_loss: 1.7331e-05\n",
      "Epoch 68/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1968 - Labels_loss: 0.1794 - sum_layer_3_loss: 1.7316e-05\n",
      "Epoch 69/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1973 - Labels_loss: 0.1800 - sum_layer_3_loss: 1.7323e-05\n",
      "Epoch 70/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1936 - Labels_loss: 0.1763 - sum_layer_3_loss: 1.7328e-05\n",
      "Epoch 71/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.2001 - Labels_loss: 0.1828 - sum_layer_3_loss: 1.7322e-05\n",
      "Epoch 72/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1954 - Labels_loss: 0.1784 - sum_layer_3_loss: 1.6986e-05\n",
      "Epoch 73/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1966 - Labels_loss: 0.1797 - sum_layer_3_loss: 1.6866e-05\n",
      "Epoch 74/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1895 - Labels_loss: 0.1727 - sum_layer_3_loss: 1.6827e-05\n",
      "Epoch 75/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1899 - Labels_loss: 0.1731 - sum_layer_3_loss: 1.6823e-05\n",
      "Epoch 76/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1913 - Labels_loss: 0.1745 - sum_layer_3_loss: 1.6817e-05\n",
      "Epoch 77/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1951 - Labels_loss: 0.1783 - sum_layer_3_loss: 1.6816e-05\n",
      "Epoch 78/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1873 - Labels_loss: 0.1705 - sum_layer_3_loss: 1.6807e-05\n",
      "Epoch 79/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1903 - Labels_loss: 0.1735 - sum_layer_3_loss: 1.6760e-05\n",
      "Epoch 80/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1908 - Labels_loss: 0.1740 - sum_layer_3_loss: 1.6804e-05\n",
      "Epoch 81/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1866 - Labels_loss: 0.1699 - sum_layer_3_loss: 1.6781e-05\n",
      "Epoch 82/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1878 - Labels_loss: 0.1710 - sum_layer_3_loss: 1.6801e-05\n",
      "Epoch 83/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1873 - Labels_loss: 0.1705 - sum_layer_3_loss: 1.6784e-05\n",
      "Epoch 84/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1835 - Labels_loss: 0.1668 - sum_layer_3_loss: 1.6786e-05\n",
      "Epoch 85/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1890 - Labels_loss: 0.1722 - sum_layer_3_loss: 1.6763e-05\n",
      "Epoch 86/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1890 - Labels_loss: 0.1722 - sum_layer_3_loss: 1.6802e-05\n",
      "Epoch 87/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1845 - Labels_loss: 0.1678 - sum_layer_3_loss: 1.6781e-05\n",
      "Epoch 88/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1823 - Labels_loss: 0.1656 - sum_layer_3_loss: 1.6777e-05\n",
      "Epoch 89/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1820 - Labels_loss: 0.1652 - sum_layer_3_loss: 1.6809e-05\n",
      "Epoch 90/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1875 - Labels_loss: 0.1707 - sum_layer_3_loss: 1.6818e-05\n",
      "Epoch 91/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1861 - Labels_loss: 0.1693 - sum_layer_3_loss: 1.6758e-05\n",
      "Epoch 92/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1868 - Labels_loss: 0.1701 - sum_layer_3_loss: 1.6759e-05\n",
      "Epoch 93/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1833 - Labels_loss: 0.1665 - sum_layer_3_loss: 1.6800e-05\n",
      "Epoch 94/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1922 - Labels_loss: 0.1755 - sum_layer_3_loss: 1.6778e-05\n",
      "Epoch 95/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1804 - Labels_loss: 0.1636 - sum_layer_3_loss: 1.6762e-05\n",
      "Epoch 96/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1810 - Labels_loss: 0.1642 - sum_layer_3_loss: 1.6743e-05\n",
      "Epoch 97/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1810 - Labels_loss: 0.1642 - sum_layer_3_loss: 1.6783e-05\n",
      "Epoch 98/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1764 - Labels_loss: 0.1597 - sum_layer_3_loss: 1.6754e-05\n",
      "Epoch 99/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1768 - Labels_loss: 0.1600 - sum_layer_3_loss: 1.6740e-05\n",
      "Epoch 100/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1786 - Labels_loss: 0.1619 - sum_layer_3_loss: 1.6708e-05\n",
      "Epoch 101/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1771 - Labels_loss: 0.1603 - sum_layer_3_loss: 1.6770e-05\n",
      "Epoch 102/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1808 - Labels_loss: 0.1641 - sum_layer_3_loss: 1.6737e-05\n",
      "Epoch 103/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1763 - Labels_loss: 0.1596 - sum_layer_3_loss: 1.6734e-05\n",
      "Epoch 104/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1795 - Labels_loss: 0.1628 - sum_layer_3_loss: 1.6741e-05\n",
      "Epoch 105/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1757 - Labels_loss: 0.1590 - sum_layer_3_loss: 1.6715e-05\n",
      "Epoch 106/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1776 - Labels_loss: 0.1609 - sum_layer_3_loss: 1.6728e-05\n",
      "Epoch 107/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1759 - Labels_loss: 0.1592 - sum_layer_3_loss: 1.6711e-05\n",
      "Epoch 108/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1750 - Labels_loss: 0.1582 - sum_layer_3_loss: 1.6720e-05\n",
      "Epoch 109/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1733 - Labels_loss: 0.1566 - sum_layer_3_loss: 1.6726e-05\n",
      "Epoch 110/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1722 - Labels_loss: 0.1554 - sum_layer_3_loss: 1.6751e-05\n",
      "Epoch 111/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1728 - Labels_loss: 0.1560 - sum_layer_3_loss: 1.6722e-05\n",
      "Epoch 112/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1743 - Labels_loss: 0.1576 - sum_layer_3_loss: 1.6719e-05\n",
      "Epoch 113/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1709 - Labels_loss: 0.1542 - sum_layer_3_loss: 1.6734e-05\n",
      "Epoch 114/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1710 - Labels_loss: 0.1543 - sum_layer_3_loss: 1.6683e-05\n",
      "Epoch 115/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1760 - Labels_loss: 0.1593 - sum_layer_3_loss: 1.6708e-05\n",
      "Epoch 116/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1705 - Labels_loss: 0.1538 - sum_layer_3_loss: 1.6738e-05\n",
      "Epoch 117/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1684 - Labels_loss: 0.1517 - sum_layer_3_loss: 1.6659e-05\n",
      "Epoch 118/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1697 - Labels_loss: 0.1530 - sum_layer_3_loss: 1.6679e-05\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1684 - Labels_loss: 0.1517 - sum_layer_3_loss: 1.6687e-05\n",
      "Epoch 120/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1714 - Labels_loss: 0.1548 - sum_layer_3_loss: 1.6650e-05\n",
      "Epoch 121/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1690 - Labels_loss: 0.1523 - sum_layer_3_loss: 1.6698e-05\n",
      "Epoch 122/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1725 - Labels_loss: 0.1558 - sum_layer_3_loss: 1.6718e-05\n",
      "Epoch 123/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1683 - Labels_loss: 0.1516 - sum_layer_3_loss: 1.6656e-05\n",
      "Epoch 124/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1665 - Labels_loss: 0.1499 - sum_layer_3_loss: 1.6664e-05\n",
      "Epoch 125/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1690 - Labels_loss: 0.1524 - sum_layer_3_loss: 1.6647e-05\n",
      "Epoch 126/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1675 - Labels_loss: 0.1509 - sum_layer_3_loss: 1.6661e-05\n",
      "Epoch 127/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1645 - Labels_loss: 0.1478 - sum_layer_3_loss: 1.6677e-05\n",
      "Epoch 128/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1651 - Labels_loss: 0.1485 - sum_layer_3_loss: 1.6623e-05\n",
      "Epoch 129/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1700 - Labels_loss: 0.1534 - sum_layer_3_loss: 1.6612e-05\n",
      "Epoch 130/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1668 - Labels_loss: 0.1502 - sum_layer_3_loss: 1.6605e-05\n",
      "Epoch 131/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1677 - Labels_loss: 0.1511 - sum_layer_3_loss: 1.6611e-05\n",
      "Epoch 132/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1622 - Labels_loss: 0.1456 - sum_layer_3_loss: 1.6599e-05\n",
      "Epoch 133/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1643 - Labels_loss: 0.1477 - sum_layer_3_loss: 1.6626e-05\n",
      "Epoch 134/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1640 - Labels_loss: 0.1474 - sum_layer_3_loss: 1.6627e-05\n",
      "Epoch 135/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1653 - Labels_loss: 0.1487 - sum_layer_3_loss: 1.6596e-05\n",
      "Epoch 136/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1673 - Labels_loss: 0.1507 - sum_layer_3_loss: 1.6589e-05\n",
      "Epoch 137/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1649 - Labels_loss: 0.1483 - sum_layer_3_loss: 1.6549e-05\n",
      "Epoch 138/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1647 - Labels_loss: 0.1482 - sum_layer_3_loss: 1.6562e-05\n",
      "Epoch 139/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1651 - Labels_loss: 0.1486 - sum_layer_3_loss: 1.6530e-05\n",
      "Epoch 140/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1657 - Labels_loss: 0.1492 - sum_layer_3_loss: 1.6523e-05\n",
      "Epoch 141/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1665 - Labels_loss: 0.1499 - sum_layer_3_loss: 1.6549e-05\n",
      "Epoch 142/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1636 - Labels_loss: 0.1472 - sum_layer_3_loss: 1.6480e-05\n",
      "Epoch 143/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1605 - Labels_loss: 0.1440 - sum_layer_3_loss: 1.6488e-05\n",
      "Epoch 144/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1611 - Labels_loss: 0.1446 - sum_layer_3_loss: 1.6472e-05\n",
      "Epoch 145/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1614 - Labels_loss: 0.1449 - sum_layer_3_loss: 1.6455e-05\n",
      "Epoch 146/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1600 - Labels_loss: 0.1436 - sum_layer_3_loss: 1.6474e-05\n",
      "Epoch 147/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1643 - Labels_loss: 0.1479 - sum_layer_3_loss: 1.6439e-05\n",
      "Epoch 148/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1595 - Labels_loss: 0.1431 - sum_layer_3_loss: 1.6443e-05\n",
      "Epoch 149/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1594 - Labels_loss: 0.1429 - sum_layer_3_loss: 1.6453e-05\n",
      "Epoch 150/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1579 - Labels_loss: 0.1414 - sum_layer_3_loss: 1.6449e-05\n",
      "Epoch 151/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1597 - Labels_loss: 0.1432 - sum_layer_3_loss: 1.6413e-05\n",
      "Epoch 152/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1590 - Labels_loss: 0.1425 - sum_layer_3_loss: 1.6414e-05\n",
      "Epoch 153/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1610 - Labels_loss: 0.1446 - sum_layer_3_loss: 1.6451e-05\n",
      "Epoch 154/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1583 - Labels_loss: 0.1419 - sum_layer_3_loss: 1.6428e-05\n",
      "Epoch 155/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1648 - Labels_loss: 0.1484 - sum_layer_3_loss: 1.6416e-05\n",
      "Epoch 156/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1578 - Labels_loss: 0.1414 - sum_layer_3_loss: 1.6386e-05\n",
      "Epoch 157/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1688 - Labels_loss: 0.1524 - sum_layer_3_loss: 1.6375e-05\n",
      "Epoch 158/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1635 - Labels_loss: 0.1471 - sum_layer_3_loss: 1.6387e-05\n",
      "Epoch 159/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1581 - Labels_loss: 0.1417 - sum_layer_3_loss: 1.6358e-05\n",
      "Epoch 160/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1600 - Labels_loss: 0.1436 - sum_layer_3_loss: 1.6369e-05\n",
      "Epoch 161/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1544 - Labels_loss: 0.1381 - sum_layer_3_loss: 1.6333e-05\n",
      "Epoch 162/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1569 - Labels_loss: 0.1406 - sum_layer_3_loss: 1.6324e-05\n",
      "Epoch 163/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1660 - Labels_loss: 0.1496 - sum_layer_3_loss: 1.6362e-05\n",
      "Epoch 164/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1618 - Labels_loss: 0.1454 - sum_layer_3_loss: 1.6347e-05\n",
      "Epoch 165/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1582 - Labels_loss: 0.1418 - sum_layer_3_loss: 1.6334e-05\n",
      "Epoch 166/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1534 - Labels_loss: 0.1370 - sum_layer_3_loss: 1.6344e-05\n",
      "Epoch 167/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1586 - Labels_loss: 0.1423 - sum_layer_3_loss: 1.6309e-05\n",
      "Epoch 168/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1570 - Labels_loss: 0.1407 - sum_layer_3_loss: 1.6318e-05\n",
      "Epoch 169/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1541 - Labels_loss: 0.1378 - sum_layer_3_loss: 1.6318e-05\n",
      "Epoch 170/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1578 - Labels_loss: 0.1415 - sum_layer_3_loss: 1.6322e-05\n",
      "Epoch 171/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1535 - Labels_loss: 0.1372 - sum_layer_3_loss: 1.6305e-05\n",
      "Epoch 172/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1591 - Labels_loss: 0.1428 - sum_layer_3_loss: 1.6301e-05\n",
      "Epoch 173/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1525 - Labels_loss: 0.1362 - sum_layer_3_loss: 1.6302e-05\n",
      "Epoch 174/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1540 - Labels_loss: 0.1377 - sum_layer_3_loss: 1.6300e-05\n",
      "Epoch 175/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1577 - Labels_loss: 0.1414 - sum_layer_3_loss: 1.6279e-05\n",
      "Epoch 176/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1528 - Labels_loss: 0.1365 - sum_layer_3_loss: 1.6291e-05\n",
      "Epoch 177/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1532 - Labels_loss: 0.1369 - sum_layer_3_loss: 1.6322e-05\n",
      "Epoch 178/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1548 - Labels_loss: 0.1385 - sum_layer_3_loss: 1.6281e-05\n",
      "Epoch 179/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1585 - Labels_loss: 0.1422 - sum_layer_3_loss: 1.6308e-05\n",
      "Epoch 180/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1571 - Labels_loss: 0.1408 - sum_layer_3_loss: 1.6282e-05\n",
      "Epoch 181/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1544 - Labels_loss: 0.1381 - sum_layer_3_loss: 1.6299e-05\n",
      "Epoch 182/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1543 - Labels_loss: 0.1381 - sum_layer_3_loss: 1.6276e-05\n",
      "Epoch 183/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1515 - Labels_loss: 0.1352 - sum_layer_3_loss: 1.6275e-05\n",
      "Epoch 184/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1564 - Labels_loss: 0.1401 - sum_layer_3_loss: 1.6294e-05\n",
      "Epoch 185/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1545 - Labels_loss: 0.1382 - sum_layer_3_loss: 1.6287e-05\n",
      "Epoch 186/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1553 - Labels_loss: 0.1390 - sum_layer_3_loss: 1.6286e-05\n",
      "Epoch 187/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1522 - Labels_loss: 0.1359 - sum_layer_3_loss: 1.6290e-05\n",
      "Epoch 188/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1538 - Labels_loss: 0.1375 - sum_layer_3_loss: 1.6270e-05\n",
      "Epoch 189/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1545 - Labels_loss: 0.1382 - sum_layer_3_loss: 1.6263e-05\n",
      "Epoch 190/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1515 - Labels_loss: 0.1352 - sum_layer_3_loss: 1.6261e-05\n",
      "Epoch 191/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1532 - Labels_loss: 0.1370 - sum_layer_3_loss: 1.6258e-05\n",
      "Epoch 192/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1511 - Labels_loss: 0.1348 - sum_layer_3_loss: 1.6247e-05\n",
      "Epoch 193/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1533 - Labels_loss: 0.1371 - sum_layer_3_loss: 1.6241e-05\n",
      "Epoch 194/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1642 - Labels_loss: 0.1479 - sum_layer_3_loss: 1.6258e-05\n",
      "Epoch 195/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1491 - Labels_loss: 0.1329 - sum_layer_3_loss: 1.6224e-05\n",
      "Epoch 196/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1527 - Labels_loss: 0.1364 - sum_layer_3_loss: 1.6253e-05\n",
      "Epoch 197/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1637 - Labels_loss: 0.1475 - sum_layer_3_loss: 1.6240e-05\n",
      "Epoch 198/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1513 - Labels_loss: 0.1350 - sum_layer_3_loss: 1.6252e-05\n",
      "Epoch 199/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1535 - Labels_loss: 0.1372 - sum_layer_3_loss: 1.6247e-05\n",
      "Epoch 200/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1526 - Labels_loss: 0.1363 - sum_layer_3_loss: 1.6253e-05\n",
      "Epoch 201/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1550 - Labels_loss: 0.1388 - sum_layer_3_loss: 1.6270e-05\n",
      "Epoch 202/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1468 - Labels_loss: 0.1306 - sum_layer_3_loss: 1.6219e-05\n",
      "Epoch 203/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1477 - Labels_loss: 0.1314 - sum_layer_3_loss: 1.6264e-05\n",
      "Epoch 204/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1500 - Labels_loss: 0.1338 - sum_layer_3_loss: 1.6269e-05\n",
      "Epoch 205/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1468 - Labels_loss: 0.1305 - sum_layer_3_loss: 1.6242e-05\n",
      "Epoch 206/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1475 - Labels_loss: 0.1313 - sum_layer_3_loss: 1.6235e-05\n",
      "Epoch 207/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1489 - Labels_loss: 0.1326 - sum_layer_3_loss: 1.6276e-05\n",
      "Epoch 208/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1488 - Labels_loss: 0.1326 - sum_layer_3_loss: 1.6257e-05\n",
      "Epoch 209/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1466 - Labels_loss: 0.1304 - sum_layer_3_loss: 1.6232e-05\n",
      "Epoch 210/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1452 - Labels_loss: 0.1289 - sum_layer_3_loss: 1.6231e-05\n",
      "Epoch 211/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1480 - Labels_loss: 0.1318 - sum_layer_3_loss: 1.6255e-05\n",
      "Epoch 212/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1480 - Labels_loss: 0.1318 - sum_layer_3_loss: 1.6227e-05\n",
      "Epoch 213/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1465 - Labels_loss: 0.1303 - sum_layer_3_loss: 1.6248e-05\n",
      "Epoch 214/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1487 - Labels_loss: 0.1324 - sum_layer_3_loss: 1.6230e-05\n",
      "Epoch 215/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1517 - Labels_loss: 0.1355 - sum_layer_3_loss: 1.6230e-05\n",
      "Epoch 216/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1536 - Labels_loss: 0.1374 - sum_layer_3_loss: 1.6253e-05\n",
      "Epoch 217/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1539 - Labels_loss: 0.1376 - sum_layer_3_loss: 1.6231e-05\n",
      "Epoch 218/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1460 - Labels_loss: 0.1298 - sum_layer_3_loss: 1.6224e-05\n",
      "Epoch 219/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1486 - Labels_loss: 0.1324 - sum_layer_3_loss: 1.6239e-05\n",
      "Epoch 220/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1443 - Labels_loss: 0.1281 - sum_layer_3_loss: 1.6227e-05\n",
      "Epoch 221/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1462 - Labels_loss: 0.1300 - sum_layer_3_loss: 1.6230e-05\n",
      "Epoch 222/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1526 - Labels_loss: 0.1364 - sum_layer_3_loss: 1.6212e-05\n",
      "Epoch 223/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1470 - Labels_loss: 0.1308 - sum_layer_3_loss: 1.6235e-05\n",
      "Epoch 224/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1437 - Labels_loss: 0.1274 - sum_layer_3_loss: 1.6225e-05\n",
      "Epoch 225/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1457 - Labels_loss: 0.1295 - sum_layer_3_loss: 1.6202e-05\n",
      "Epoch 226/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1463 - Labels_loss: 0.1301 - sum_layer_3_loss: 1.6219e-05\n",
      "Epoch 227/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1489 - Labels_loss: 0.1327 - sum_layer_3_loss: 1.6214e-05\n",
      "Epoch 228/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1559 - Labels_loss: 0.1396 - sum_layer_3_loss: 1.6215e-05\n",
      "Epoch 229/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1447 - Labels_loss: 0.1285 - sum_layer_3_loss: 1.6213e-05\n",
      "Epoch 230/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1463 - Labels_loss: 0.1301 - sum_layer_3_loss: 1.6218e-05\n",
      "Epoch 231/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1487 - Labels_loss: 0.1325 - sum_layer_3_loss: 1.6226e-05\n",
      "Epoch 232/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1477 - Labels_loss: 0.1315 - sum_layer_3_loss: 1.6216e-05\n",
      "Epoch 233/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1490 - Labels_loss: 0.1328 - sum_layer_3_loss: 1.6213e-05\n",
      "Epoch 234/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1463 - Labels_loss: 0.1301 - sum_layer_3_loss: 1.6244e-05\n",
      "Epoch 235/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1423 - Labels_loss: 0.1261 - sum_layer_3_loss: 1.6196e-05\n",
      "Epoch 236/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1430 - Labels_loss: 0.1268 - sum_layer_3_loss: 1.6210e-05\n",
      "Epoch 237/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1424 - Labels_loss: 0.1262 - sum_layer_3_loss: 1.6251e-05\n",
      "Epoch 238/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1417 - Labels_loss: 0.1254 - sum_layer_3_loss: 1.6217e-05\n",
      "Epoch 239/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1415 - Labels_loss: 0.1253 - sum_layer_3_loss: 1.6197e-05\n",
      "Epoch 240/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1404 - Labels_loss: 0.1242 - sum_layer_3_loss: 1.6194e-05\n",
      "Epoch 241/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1514 - Labels_loss: 0.1351 - sum_layer_3_loss: 1.6240e-05\n",
      "Epoch 242/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1480 - Labels_loss: 0.1318 - sum_layer_3_loss: 1.6221e-05\n",
      "Epoch 243/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1420 - Labels_loss: 0.1258 - sum_layer_3_loss: 1.6206e-05\n",
      "Epoch 244/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1454 - Labels_loss: 0.1292 - sum_layer_3_loss: 1.6196e-05\n",
      "Epoch 245/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1460 - Labels_loss: 0.1298 - sum_layer_3_loss: 1.6202e-05\n",
      "Epoch 246/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1410 - Labels_loss: 0.1248 - sum_layer_3_loss: 1.6215e-05\n",
      "Epoch 247/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1388 - Labels_loss: 0.1226 - sum_layer_3_loss: 1.6236e-05\n",
      "Epoch 248/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1403 - Labels_loss: 0.1241 - sum_layer_3_loss: 1.6209e-05\n",
      "Epoch 249/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1392 - Labels_loss: 0.1230 - sum_layer_3_loss: 1.6192e-05\n",
      "Epoch 250/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1411 - Labels_loss: 0.1249 - sum_layer_3_loss: 1.6178e-05\n",
      "Epoch 251/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1408 - Labels_loss: 0.1246 - sum_layer_3_loss: 1.6185e-05\n",
      "Epoch 252/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1385 - Labels_loss: 0.1223 - sum_layer_3_loss: 1.6188e-05\n",
      "Epoch 253/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1381 - Labels_loss: 0.1219 - sum_layer_3_loss: 1.6188e-05\n",
      "Epoch 254/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1425 - Labels_loss: 0.1263 - sum_layer_3_loss: 1.6200e-05\n",
      "Epoch 255/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1460 - Labels_loss: 0.1298 - sum_layer_3_loss: 1.6223e-05\n",
      "Epoch 256/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1419 - Labels_loss: 0.1257 - sum_layer_3_loss: 1.6208e-05\n",
      "Epoch 257/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1359 - Labels_loss: 0.1197 - sum_layer_3_loss: 1.6186e-05\n",
      "Epoch 258/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1348 - Labels_loss: 0.1187 - sum_layer_3_loss: 1.6174e-05\n",
      "Epoch 259/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1398 - Labels_loss: 0.1236 - sum_layer_3_loss: 1.6166e-05\n",
      "Epoch 260/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1393 - Labels_loss: 0.1232 - sum_layer_3_loss: 1.6185e-05\n",
      "Epoch 261/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1386 - Labels_loss: 0.1224 - sum_layer_3_loss: 1.6178e-05\n",
      "Epoch 262/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1395 - Labels_loss: 0.1234 - sum_layer_3_loss: 1.6184e-05\n",
      "Epoch 263/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1417 - Labels_loss: 0.1255 - sum_layer_3_loss: 1.6197e-05\n",
      "Epoch 264/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1423 - Labels_loss: 0.1261 - sum_layer_3_loss: 1.6185e-05\n",
      "Epoch 265/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1389 - Labels_loss: 0.1227 - sum_layer_3_loss: 1.6211e-05\n",
      "Epoch 266/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1371 - Labels_loss: 0.1210 - sum_layer_3_loss: 1.6166e-05\n",
      "Epoch 267/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1382 - Labels_loss: 0.1220 - sum_layer_3_loss: 1.6186e-05\n",
      "Epoch 268/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1377 - Labels_loss: 0.1216 - sum_layer_3_loss: 1.6166e-05\n",
      "Epoch 269/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1390 - Labels_loss: 0.1229 - sum_layer_3_loss: 1.6144e-05\n",
      "Epoch 270/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1380 - Labels_loss: 0.1218 - sum_layer_3_loss: 1.6152e-05\n",
      "Epoch 271/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1365 - Labels_loss: 0.1204 - sum_layer_3_loss: 1.6177e-05\n",
      "Epoch 272/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1384 - Labels_loss: 0.1223 - sum_layer_3_loss: 1.6145e-05\n",
      "Epoch 273/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1378 - Labels_loss: 0.1217 - sum_layer_3_loss: 1.6143e-05\n",
      "Epoch 274/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1395 - Labels_loss: 0.1233 - sum_layer_3_loss: 1.6157e-05\n",
      "Epoch 275/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1429 - Labels_loss: 0.1268 - sum_layer_3_loss: 1.6130e-05\n",
      "Epoch 276/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1381 - Labels_loss: 0.1220 - sum_layer_3_loss: 1.6142e-05\n",
      "Epoch 277/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1377 - Labels_loss: 0.1215 - sum_layer_3_loss: 1.6166e-05\n",
      "Epoch 278/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1418 - Labels_loss: 0.1255 - sum_layer_3_loss: 1.6210e-05\n",
      "Epoch 279/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1348 - Labels_loss: 0.1186 - sum_layer_3_loss: 1.6153e-05\n",
      "Epoch 280/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1345 - Labels_loss: 0.1184 - sum_layer_3_loss: 1.6147e-05\n",
      "Epoch 281/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1359 - Labels_loss: 0.1198 - sum_layer_3_loss: 1.6164e-05\n",
      "Epoch 282/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1331 - Labels_loss: 0.1169 - sum_layer_3_loss: 1.6152e-05\n",
      "Epoch 283/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1338 - Labels_loss: 0.1177 - sum_layer_3_loss: 1.6148e-05\n",
      "Epoch 284/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1378 - Labels_loss: 0.1216 - sum_layer_3_loss: 1.6204e-05\n",
      "Epoch 285/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1321 - Labels_loss: 0.1160 - sum_layer_3_loss: 1.6144e-05\n",
      "Epoch 286/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1352 - Labels_loss: 0.1191 - sum_layer_3_loss: 1.6131e-05\n",
      "Epoch 287/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1369 - Labels_loss: 0.1208 - sum_layer_3_loss: 1.6137e-05\n",
      "Epoch 288/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1381 - Labels_loss: 0.1219 - sum_layer_3_loss: 1.6158e-05\n",
      "Epoch 289/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1358 - Labels_loss: 0.1197 - sum_layer_3_loss: 1.6143e-05\n",
      "Epoch 290/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1382 - Labels_loss: 0.1221 - sum_layer_3_loss: 1.6144e-05\n",
      "Epoch 291/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1376 - Labels_loss: 0.1215 - sum_layer_3_loss: 1.6150e-05\n",
      "Epoch 292/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1371 - Labels_loss: 0.1210 - sum_layer_3_loss: 1.6148e-05\n",
      "Epoch 293/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1398 - Labels_loss: 0.1237 - sum_layer_3_loss: 1.6121e-05\n",
      "Epoch 294/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1310 - Labels_loss: 0.1148 - sum_layer_3_loss: 1.6149e-05\n",
      "Epoch 295/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1472 - Labels_loss: 0.1311 - sum_layer_3_loss: 1.6160e-05\n",
      "Epoch 296/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1385 - Labels_loss: 0.1224 - sum_layer_3_loss: 1.6129e-05\n",
      "Epoch 297/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1320 - Labels_loss: 0.1159 - sum_layer_3_loss: 1.6125e-05\n",
      "Epoch 298/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1322 - Labels_loss: 0.1161 - sum_layer_3_loss: 1.6143e-05\n",
      "Epoch 299/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1342 - Labels_loss: 0.1180 - sum_layer_3_loss: 1.6148e-05\n",
      "Epoch 300/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1290 - Labels_loss: 0.1129 - sum_layer_3_loss: 1.6130e-05\n",
      "Epoch 301/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1332 - Labels_loss: 0.1171 - sum_layer_3_loss: 1.6131e-05\n",
      "Epoch 302/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1312 - Labels_loss: 0.1151 - sum_layer_3_loss: 1.6100e-05\n",
      "Epoch 303/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1434 - Labels_loss: 0.1273 - sum_layer_3_loss: 1.6136e-05\n",
      "Epoch 304/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1355 - Labels_loss: 0.1194 - sum_layer_3_loss: 1.6121e-05\n",
      "Epoch 305/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1391 - Labels_loss: 0.1229 - sum_layer_3_loss: 1.6217e-05\n",
      "Epoch 306/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1344 - Labels_loss: 0.1182 - sum_layer_3_loss: 1.6167e-05\n",
      "Epoch 307/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1299 - Labels_loss: 0.1138 - sum_layer_3_loss: 1.6130e-05\n",
      "Epoch 308/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1311 - Labels_loss: 0.1150 - sum_layer_3_loss: 1.6109e-05\n",
      "Epoch 309/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1329 - Labels_loss: 0.1168 - sum_layer_3_loss: 1.6137e-05\n",
      "Epoch 310/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1296 - Labels_loss: 0.1135 - sum_layer_3_loss: 1.6163e-05\n",
      "Epoch 311/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1347 - Labels_loss: 0.1185 - sum_layer_3_loss: 1.6143e-05\n",
      "Epoch 312/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1309 - Labels_loss: 0.1147 - sum_layer_3_loss: 1.6180e-05\n",
      "Epoch 313/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1313 - Labels_loss: 0.1152 - sum_layer_3_loss: 1.6119e-05\n",
      "Epoch 314/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1306 - Labels_loss: 0.1145 - sum_layer_3_loss: 1.6126e-05\n",
      "Epoch 315/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1283 - Labels_loss: 0.1121 - sum_layer_3_loss: 1.6128e-05\n",
      "Epoch 316/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1311 - Labels_loss: 0.1149 - sum_layer_3_loss: 1.6117e-05\n",
      "Epoch 317/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1320 - Labels_loss: 0.1159 - sum_layer_3_loss: 1.6103e-05\n",
      "Epoch 318/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1309 - Labels_loss: 0.1147 - sum_layer_3_loss: 1.6198e-05\n",
      "Epoch 319/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1294 - Labels_loss: 0.1132 - sum_layer_3_loss: 1.6127e-05\n",
      "Epoch 320/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1306 - Labels_loss: 0.1145 - sum_layer_3_loss: 1.6107e-05\n",
      "Epoch 321/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1310 - Labels_loss: 0.1149 - sum_layer_3_loss: 1.6097e-05\n",
      "Epoch 322/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1338 - Labels_loss: 0.1177 - sum_layer_3_loss: 1.6113e-05\n",
      "Epoch 323/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1286 - Labels_loss: 0.1124 - sum_layer_3_loss: 1.6146e-05\n",
      "Epoch 324/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1322 - Labels_loss: 0.1161 - sum_layer_3_loss: 1.6126e-05\n",
      "Epoch 325/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1326 - Labels_loss: 0.1164 - sum_layer_3_loss: 1.6146e-05\n",
      "Epoch 326/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1303 - Labels_loss: 0.1141 - sum_layer_3_loss: 1.6115e-05\n",
      "Epoch 327/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1295 - Labels_loss: 0.1134 - sum_layer_3_loss: 1.6086e-05\n",
      "Epoch 328/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1305 - Labels_loss: 0.1144 - sum_layer_3_loss: 1.6133e-05\n",
      "Epoch 329/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1285 - Labels_loss: 0.1124 - sum_layer_3_loss: 1.6162e-05\n",
      "Epoch 330/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1308 - Labels_loss: 0.1147 - sum_layer_3_loss: 1.6090e-05\n",
      "Epoch 331/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1321 - Labels_loss: 0.1161 - sum_layer_3_loss: 1.6063e-05\n",
      "Epoch 332/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1273 - Labels_loss: 0.1112 - sum_layer_3_loss: 1.6183e-05\n",
      "Epoch 333/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1292 - Labels_loss: 0.1131 - sum_layer_3_loss: 1.6091e-05\n",
      "Epoch 334/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1301 - Labels_loss: 0.1140 - sum_layer_3_loss: 1.6084e-05\n",
      "Epoch 335/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1295 - Labels_loss: 0.1134 - sum_layer_3_loss: 1.6073e-05\n",
      "Epoch 336/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1331 - Labels_loss: 0.1170 - sum_layer_3_loss: 1.6066e-05\n",
      "Epoch 337/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1314 - Labels_loss: 0.1153 - sum_layer_3_loss: 1.6075e-05\n",
      "Epoch 338/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1289 - Labels_loss: 0.1128 - sum_layer_3_loss: 1.6074e-05\n",
      "Epoch 339/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1279 - Labels_loss: 0.1118 - sum_layer_3_loss: 1.6060e-05\n",
      "Epoch 340/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1269 - Labels_loss: 0.1108 - sum_layer_3_loss: 1.6095e-05\n",
      "Epoch 341/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1279 - Labels_loss: 0.1118 - sum_layer_3_loss: 1.6083e-05\n",
      "Epoch 342/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1380 - Labels_loss: 0.1219 - sum_layer_3_loss: 1.6098e-05\n",
      "Epoch 343/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1322 - Labels_loss: 0.1161 - sum_layer_3_loss: 1.6060e-05\n",
      "Epoch 344/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1273 - Labels_loss: 0.1112 - sum_layer_3_loss: 1.6077e-05\n",
      "Epoch 345/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1290 - Labels_loss: 0.1129 - sum_layer_3_loss: 1.6087e-05\n",
      "Epoch 346/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1294 - Labels_loss: 0.1133 - sum_layer_3_loss: 1.6088e-05\n",
      "Epoch 347/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1275 - Labels_loss: 0.1115 - sum_layer_3_loss: 1.6054e-05\n",
      "Epoch 348/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1304 - Labels_loss: 0.1143 - sum_layer_3_loss: 1.6070e-05\n",
      "Epoch 349/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1253 - Labels_loss: 0.1092 - sum_layer_3_loss: 1.6082e-05\n",
      "Epoch 350/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1270 - Labels_loss: 0.1109 - sum_layer_3_loss: 1.6060e-05\n",
      "Epoch 351/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1242 - Labels_loss: 0.1081 - sum_layer_3_loss: 1.6049e-05\n",
      "Epoch 352/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1343 - Labels_loss: 0.1182 - sum_layer_3_loss: 1.6049e-05\n",
      "Epoch 353/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1262 - Labels_loss: 0.1101 - sum_layer_3_loss: 1.6099e-05\n",
      "Epoch 354/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1244 - Labels_loss: 0.1084 - sum_layer_3_loss: 1.6019e-05\n",
      "Epoch 355/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1265 - Labels_loss: 0.1104 - sum_layer_3_loss: 1.6022e-05\n",
      "Epoch 356/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1277 - Labels_loss: 0.1117 - sum_layer_3_loss: 1.6016e-05\n",
      "Epoch 357/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1219 - Labels_loss: 0.1058 - sum_layer_3_loss: 1.6071e-05\n",
      "Epoch 358/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1300 - Labels_loss: 0.1140 - sum_layer_3_loss: 1.6028e-05\n",
      "Epoch 359/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1250 - Labels_loss: 0.1090 - sum_layer_3_loss: 1.6022e-05\n",
      "Epoch 360/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1258 - Labels_loss: 0.1098 - sum_layer_3_loss: 1.6015e-05\n",
      "Epoch 361/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1328 - Labels_loss: 0.1167 - sum_layer_3_loss: 1.6082e-05\n",
      "Epoch 362/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1287 - Labels_loss: 0.1127 - sum_layer_3_loss: 1.6018e-05\n",
      "Epoch 363/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1266 - Labels_loss: 0.1106 - sum_layer_3_loss: 1.5983e-05\n",
      "Epoch 364/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1237 - Labels_loss: 0.1077 - sum_layer_3_loss: 1.6012e-05\n",
      "Epoch 365/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1237 - Labels_loss: 0.1078 - sum_layer_3_loss: 1.5988e-05\n",
      "Epoch 366/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1275 - Labels_loss: 0.1115 - sum_layer_3_loss: 1.5992e-05\n",
      "Epoch 367/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1256 - Labels_loss: 0.1096 - sum_layer_3_loss: 1.5957e-05\n",
      "Epoch 368/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1247 - Labels_loss: 0.1087 - sum_layer_3_loss: 1.5965e-05\n",
      "Epoch 369/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1250 - Labels_loss: 0.1090 - sum_layer_3_loss: 1.5935e-05\n",
      "Epoch 370/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1275 - Labels_loss: 0.1115 - sum_layer_3_loss: 1.5958e-05\n",
      "Epoch 371/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1232 - Labels_loss: 0.1072 - sum_layer_3_loss: 1.5957e-05\n",
      "Epoch 372/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1255 - Labels_loss: 0.1096 - sum_layer_3_loss: 1.5968e-05\n",
      "Epoch 373/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1266 - Labels_loss: 0.1106 - sum_layer_3_loss: 1.5938e-05\n",
      "Epoch 374/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1268 - Labels_loss: 0.1109 - sum_layer_3_loss: 1.5944e-05\n",
      "Epoch 375/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1393 - Labels_loss: 0.1233 - sum_layer_3_loss: 1.5991e-05\n",
      "Epoch 376/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1285 - Labels_loss: 0.1125 - sum_layer_3_loss: 1.5932e-05\n",
      "Epoch 377/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1316 - Labels_loss: 0.1157 - sum_layer_3_loss: 1.5956e-05\n",
      "Epoch 378/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1239 - Labels_loss: 0.1079 - sum_layer_3_loss: 1.5926e-05\n",
      "Epoch 379/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1235 - Labels_loss: 0.1075 - sum_layer_3_loss: 1.5930e-05\n",
      "Epoch 380/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1245 - Labels_loss: 0.1086 - sum_layer_3_loss: 1.5936e-05\n",
      "Epoch 381/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1264 - Labels_loss: 0.1105 - sum_layer_3_loss: 1.5921e-05\n",
      "Epoch 382/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1210 - Labels_loss: 0.1051 - sum_layer_3_loss: 1.5910e-05\n",
      "Epoch 383/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1257 - Labels_loss: 0.1098 - sum_layer_3_loss: 1.5932e-05\n",
      "Epoch 384/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1254 - Labels_loss: 0.1095 - sum_layer_3_loss: 1.5922e-05\n",
      "Epoch 385/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1219 - Labels_loss: 0.1060 - sum_layer_3_loss: 1.5946e-05\n",
      "Epoch 386/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1238 - Labels_loss: 0.1079 - sum_layer_3_loss: 1.5914e-05\n",
      "Epoch 387/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1241 - Labels_loss: 0.1082 - sum_layer_3_loss: 1.5902e-05\n",
      "Epoch 388/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1237 - Labels_loss: 0.1078 - sum_layer_3_loss: 1.5904e-05\n",
      "Epoch 389/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1294 - Labels_loss: 0.1135 - sum_layer_3_loss: 1.5914e-05\n",
      "Epoch 390/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1238 - Labels_loss: 0.1079 - sum_layer_3_loss: 1.5909e-05\n",
      "Epoch 391/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1229 - Labels_loss: 0.1071 - sum_layer_3_loss: 1.5887e-05\n",
      "Epoch 392/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1204 - Labels_loss: 0.1045 - sum_layer_3_loss: 1.5881e-05\n",
      "Epoch 393/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1220 - Labels_loss: 0.1060 - sum_layer_3_loss: 1.5914e-05\n",
      "Epoch 394/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1240 - Labels_loss: 0.1082 - sum_layer_3_loss: 1.5878e-05\n",
      "Epoch 395/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1285 - Labels_loss: 0.1126 - sum_layer_3_loss: 1.5888e-05\n",
      "Epoch 396/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1302 - Labels_loss: 0.1143 - sum_layer_3_loss: 1.5889e-05\n",
      "Epoch 397/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1230 - Labels_loss: 0.1071 - sum_layer_3_loss: 1.5888e-05\n",
      "Epoch 398/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1230 - Labels_loss: 0.1072 - sum_layer_3_loss: 1.5893e-05\n",
      "Epoch 399/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1289 - Labels_loss: 0.1130 - sum_layer_3_loss: 1.5933e-05\n",
      "Epoch 400/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1267 - Labels_loss: 0.1108 - sum_layer_3_loss: 1.5919e-05\n",
      "Epoch 401/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1221 - Labels_loss: 0.1063 - sum_layer_3_loss: 1.5875e-05\n",
      "Epoch 402/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1332 - Labels_loss: 0.1172 - sum_layer_3_loss: 1.5934e-05\n",
      "Epoch 403/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1190 - Labels_loss: 0.1031 - sum_layer_3_loss: 1.5881e-05\n",
      "Epoch 404/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1196 - Labels_loss: 0.1037 - sum_layer_3_loss: 1.5893e-05\n",
      "Epoch 405/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1225 - Labels_loss: 0.1066 - sum_layer_3_loss: 1.5876e-05\n",
      "Epoch 406/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1235 - Labels_loss: 0.1076 - sum_layer_3_loss: 1.5896e-05\n",
      "Epoch 407/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1200 - Labels_loss: 0.1041 - sum_layer_3_loss: 1.5885e-05\n",
      "Epoch 408/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1234 - Labels_loss: 0.1075 - sum_layer_3_loss: 1.5915e-05\n",
      "Epoch 409/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1197 - Labels_loss: 0.1038 - sum_layer_3_loss: 1.5919e-05\n",
      "Epoch 410/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1205 - Labels_loss: 0.1046 - sum_layer_3_loss: 1.5904e-05\n",
      "Epoch 411/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1215 - Labels_loss: 0.1056 - sum_layer_3_loss: 1.5912e-05\n",
      "Epoch 412/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1236 - Labels_loss: 0.1077 - sum_layer_3_loss: 1.5923e-05\n",
      "Epoch 413/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1201 - Labels_loss: 0.1043 - sum_layer_3_loss: 1.5859e-05\n",
      "Epoch 414/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1246 - Labels_loss: 0.1087 - sum_layer_3_loss: 1.5913e-05\n",
      "Epoch 415/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1185 - Labels_loss: 0.1027 - sum_layer_3_loss: 1.5880e-05\n",
      "Epoch 416/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1237 - Labels_loss: 0.1078 - sum_layer_3_loss: 1.5889e-05\n",
      "Epoch 417/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1233 - Labels_loss: 0.1075 - sum_layer_3_loss: 1.5882e-05\n",
      "Epoch 418/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1244 - Labels_loss: 0.1086 - sum_layer_3_loss: 1.5880e-05\n",
      "Epoch 419/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1297 - Labels_loss: 0.1137 - sum_layer_3_loss: 1.5918e-05\n",
      "Epoch 420/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1213 - Labels_loss: 0.1054 - sum_layer_3_loss: 1.5894e-05\n",
      "Epoch 421/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1221 - Labels_loss: 0.1062 - sum_layer_3_loss: 1.5887e-05\n",
      "Epoch 422/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1215 - Labels_loss: 0.1056 - sum_layer_3_loss: 1.5884e-05\n",
      "Epoch 423/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1198 - Labels_loss: 0.1039 - sum_layer_3_loss: 1.5878e-05\n",
      "Epoch 424/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1192 - Labels_loss: 0.1034 - sum_layer_3_loss: 1.5861e-05\n",
      "Epoch 425/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1209 - Labels_loss: 0.1050 - sum_layer_3_loss: 1.5852e-05\n",
      "Epoch 426/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1227 - Labels_loss: 0.1068 - sum_layer_3_loss: 1.5867e-05\n",
      "Epoch 427/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1214 - Labels_loss: 0.1055 - sum_layer_3_loss: 1.5855e-05\n",
      "Epoch 428/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1232 - Labels_loss: 0.1073 - sum_layer_3_loss: 1.5874e-05\n",
      "Epoch 429/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1225 - Labels_loss: 0.1066 - sum_layer_3_loss: 1.5912e-05\n",
      "Epoch 430/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1186 - Labels_loss: 0.1027 - sum_layer_3_loss: 1.5850e-05\n",
      "Epoch 431/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1197 - Labels_loss: 0.1038 - sum_layer_3_loss: 1.5877e-05\n",
      "Epoch 432/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1246 - Labels_loss: 0.1087 - sum_layer_3_loss: 1.5869e-05\n",
      "Epoch 433/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1213 - Labels_loss: 0.1055 - sum_layer_3_loss: 1.5874e-05\n",
      "Epoch 434/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1297 - Labels_loss: 0.1139 - sum_layer_3_loss: 1.5867e-05\n",
      "Epoch 435/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1255 - Labels_loss: 0.1096 - sum_layer_3_loss: 1.5915e-05\n",
      "Epoch 436/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1267 - Labels_loss: 0.1108 - sum_layer_3_loss: 1.5906e-05\n",
      "Epoch 437/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1192 - Labels_loss: 0.1033 - sum_layer_3_loss: 1.5866e-05\n",
      "Epoch 438/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1233 - Labels_loss: 0.1075 - sum_layer_3_loss: 1.5868e-05\n",
      "Epoch 439/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1191 - Labels_loss: 0.1032 - sum_layer_3_loss: 1.5865e-05\n",
      "Epoch 440/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1166 - Labels_loss: 0.1008 - sum_layer_3_loss: 1.5859e-05\n",
      "Epoch 441/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1201 - Labels_loss: 0.1043 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 442/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1214 - Labels_loss: 0.1056 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 443/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1177 - Labels_loss: 0.1018 - sum_layer_3_loss: 1.5866e-05\n",
      "Epoch 444/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1202 - Labels_loss: 0.1044 - sum_layer_3_loss: 1.5863e-05\n",
      "Epoch 445/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1169 - Labels_loss: 0.1010 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 446/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1179 - Labels_loss: 0.1019 - sum_layer_3_loss: 1.5986e-05\n",
      "Epoch 447/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1175 - Labels_loss: 0.1017 - sum_layer_3_loss: 1.5869e-05\n",
      "Epoch 448/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1192 - Labels_loss: 0.1034 - sum_layer_3_loss: 1.5854e-05\n",
      "Epoch 449/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1201 - Labels_loss: 0.1042 - sum_layer_3_loss: 1.5844e-05\n",
      "Epoch 450/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1178 - Labels_loss: 0.1019 - sum_layer_3_loss: 1.5903e-05\n",
      "Epoch 451/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1200 - Labels_loss: 0.1041 - sum_layer_3_loss: 1.5917e-05\n",
      "Epoch 452/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1193 - Labels_loss: 0.1034 - sum_layer_3_loss: 1.5884e-05\n",
      "Epoch 453/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1158 - Labels_loss: 0.0999 - sum_layer_3_loss: 1.5886e-05\n",
      "Epoch 454/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1220 - Labels_loss: 0.1062 - sum_layer_3_loss: 1.5854e-05\n",
      "Epoch 455/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1204 - Labels_loss: 0.1045 - sum_layer_3_loss: 1.5860e-05\n",
      "Epoch 456/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1223 - Labels_loss: 0.1064 - sum_layer_3_loss: 1.5842e-05\n",
      "Epoch 457/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1210 - Labels_loss: 0.1052 - sum_layer_3_loss: 1.5838e-05\n",
      "Epoch 458/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1190 - Labels_loss: 0.1031 - sum_layer_3_loss: 1.5871e-05\n",
      "Epoch 459/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1210 - Labels_loss: 0.1052 - sum_layer_3_loss: 1.5853e-05\n",
      "Epoch 460/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1164 - Labels_loss: 0.1006 - sum_layer_3_loss: 1.5846e-05\n",
      "Epoch 461/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1178 - Labels_loss: 0.1019 - sum_layer_3_loss: 1.5857e-05\n",
      "Epoch 462/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1161 - Labels_loss: 0.1003 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 463/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1174 - Labels_loss: 0.1014 - sum_layer_3_loss: 1.5920e-05\n",
      "Epoch 464/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0993 - sum_layer_3_loss: 1.5868e-05\n",
      "Epoch 465/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1149 - Labels_loss: 0.0991 - sum_layer_3_loss: 1.5861e-05\n",
      "Epoch 466/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1144 - Labels_loss: 0.0986 - sum_layer_3_loss: 1.5846e-05\n",
      "Epoch 467/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1189 - Labels_loss: 0.1031 - sum_layer_3_loss: 1.5848e-05\n",
      "Epoch 468/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1349 - Labels_loss: 0.1190 - sum_layer_3_loss: 1.5895e-05\n",
      "Epoch 469/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1273 - Labels_loss: 0.1114 - sum_layer_3_loss: 1.5915e-05\n",
      "Epoch 470/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1182 - Labels_loss: 0.1023 - sum_layer_3_loss: 1.5896e-05\n",
      "Epoch 471/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1200 - Labels_loss: 0.1041 - sum_layer_3_loss: 1.5882e-05\n",
      "Epoch 472/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0993 - sum_layer_3_loss: 1.5904e-05\n",
      "Epoch 473/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1138 - Labels_loss: 0.0979 - sum_layer_3_loss: 1.5871e-05\n",
      "Epoch 474/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1172 - Labels_loss: 0.1014 - sum_layer_3_loss: 1.5849e-05\n",
      "Epoch 475/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1205 - Labels_loss: 0.1047 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 476/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1174 - Labels_loss: 0.1015 - sum_layer_3_loss: 1.5910e-05\n",
      "Epoch 477/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1193 - Labels_loss: 0.1034 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 478/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1147 - Labels_loss: 0.0988 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 479/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1205 - Labels_loss: 0.1047 - sum_layer_3_loss: 1.5825e-05\n",
      "Epoch 480/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1371 - Labels_loss: 0.1212 - sum_layer_3_loss: 1.5887e-05\n",
      "Epoch 481/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1144 - Labels_loss: 0.0986 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 482/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1239 - Labels_loss: 0.1080 - sum_layer_3_loss: 1.5843e-05\n",
      "Epoch 483/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1253 - Labels_loss: 0.1094 - sum_layer_3_loss: 1.5917e-05\n",
      "Epoch 484/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1184 - Labels_loss: 0.1026 - sum_layer_3_loss: 1.5884e-05\n",
      "Epoch 485/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1138 - Labels_loss: 0.0979 - sum_layer_3_loss: 1.5912e-05\n",
      "Epoch 486/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1170 - Labels_loss: 0.1011 - sum_layer_3_loss: 1.5889e-05\n",
      "Epoch 487/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1148 - Labels_loss: 0.0989 - sum_layer_3_loss: 1.5881e-05\n",
      "Epoch 488/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1175 - Labels_loss: 0.1017 - sum_layer_3_loss: 1.5850e-05\n",
      "Epoch 489/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1168 - Labels_loss: 0.1009 - sum_layer_3_loss: 1.5853e-05\n",
      "Epoch 490/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1173 - Labels_loss: 0.1015 - sum_layer_3_loss: 1.5851e-05\n",
      "Epoch 491/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1143 - Labels_loss: 0.0984 - sum_layer_3_loss: 1.5883e-05\n",
      "Epoch 492/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1166 - Labels_loss: 0.1008 - sum_layer_3_loss: 1.5863e-05\n",
      "Epoch 493/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1159 - Labels_loss: 0.1000 - sum_layer_3_loss: 1.5876e-05\n",
      "Epoch 494/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1145 - Labels_loss: 0.0987 - sum_layer_3_loss: 1.5867e-05\n",
      "Epoch 495/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1158 - Labels_loss: 0.0999 - sum_layer_3_loss: 1.5861e-05\n",
      "Epoch 496/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1144 - Labels_loss: 0.0986 - sum_layer_3_loss: 1.5880e-05\n",
      "Epoch 497/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1201 - Labels_loss: 0.1042 - sum_layer_3_loss: 1.5865e-05\n",
      "Epoch 498/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1124 - Labels_loss: 0.0966 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 499/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1162 - Labels_loss: 0.1003 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 500/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1142 - Labels_loss: 0.0984 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 501/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1172 - Labels_loss: 0.1013 - sum_layer_3_loss: 1.5854e-05\n",
      "Epoch 502/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1177 - Labels_loss: 0.1018 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 503/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1145 - Labels_loss: 0.0986 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 504/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1149 - Labels_loss: 0.0990 - sum_layer_3_loss: 1.5849e-05\n",
      "Epoch 505/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1132 - Labels_loss: 0.0974 - sum_layer_3_loss: 1.5807e-05\n",
      "Epoch 506/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1178 - Labels_loss: 0.1020 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 507/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1165 - Labels_loss: 0.1006 - sum_layer_3_loss: 1.5872e-05\n",
      "Epoch 508/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0994 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 509/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0994 - sum_layer_3_loss: 1.5821e-05\n",
      "Epoch 510/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1164 - Labels_loss: 0.1006 - sum_layer_3_loss: 1.5855e-05\n",
      "Epoch 511/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.1115 - Labels_loss: 0.0957 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 512/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1145 - Labels_loss: 0.0987 - sum_layer_3_loss: 1.5819e-05\n",
      "Epoch 513/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0993 - sum_layer_3_loss: 1.5826e-05\n",
      "Epoch 514/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1156 - Labels_loss: 0.0997 - sum_layer_3_loss: 1.5873e-05\n",
      "Epoch 515/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1162 - Labels_loss: 0.1003 - sum_layer_3_loss: 1.5845e-05\n",
      "Epoch 516/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1150 - Labels_loss: 0.0992 - sum_layer_3_loss: 1.5803e-05\n",
      "Epoch 517/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1148 - Labels_loss: 0.0990 - sum_layer_3_loss: 1.5832e-05\n",
      "Epoch 518/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1158 - Labels_loss: 0.0999 - sum_layer_3_loss: 1.5867e-05\n",
      "Epoch 519/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1138 - Labels_loss: 0.0980 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 520/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1146 - Labels_loss: 0.0988 - sum_layer_3_loss: 1.5855e-05\n",
      "Epoch 521/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1172 - Labels_loss: 0.1014 - sum_layer_3_loss: 1.5798e-05\n",
      "Epoch 522/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1155 - Labels_loss: 0.0996 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 523/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1147 - Labels_loss: 0.0989 - sum_layer_3_loss: 1.5813e-05\n",
      "Epoch 524/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1155 - Labels_loss: 0.0996 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 525/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1152 - Labels_loss: 0.0994 - sum_layer_3_loss: 1.5848e-05\n",
      "Epoch 526/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1129 - Labels_loss: 0.0971 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 527/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1158 - Labels_loss: 0.1000 - sum_layer_3_loss: 1.5855e-05\n",
      "Epoch 528/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1156 - Labels_loss: 0.0998 - sum_layer_3_loss: 1.5830e-05\n",
      "Epoch 529/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1132 - Labels_loss: 0.0973 - sum_layer_3_loss: 1.5886e-05\n",
      "Epoch 530/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1146 - Labels_loss: 0.0988 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 531/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1127 - Labels_loss: 0.0968 - sum_layer_3_loss: 1.5918e-05\n",
      "Epoch 532/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1128 - Labels_loss: 0.0969 - sum_layer_3_loss: 1.5876e-05\n",
      "Epoch 533/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1183 - Labels_loss: 0.1025 - sum_layer_3_loss: 1.5840e-05\n",
      "Epoch 534/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1112 - Labels_loss: 0.0953 - sum_layer_3_loss: 1.5837e-05\n",
      "Epoch 535/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1132 - Labels_loss: 0.0973 - sum_layer_3_loss: 1.5847e-05\n",
      "Epoch 536/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1158 - Labels_loss: 0.1000 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 537/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1157 - Labels_loss: 0.0998 - sum_layer_3_loss: 1.5853e-05\n",
      "Epoch 538/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1144 - Labels_loss: 0.0986 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 539/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1124 - Labels_loss: 0.0965 - sum_layer_3_loss: 1.5883e-05\n",
      "Epoch 540/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1136 - Labels_loss: 0.0977 - sum_layer_3_loss: 1.5821e-05\n",
      "Epoch 541/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1110 - Labels_loss: 0.0952 - sum_layer_3_loss: 1.5828e-05\n",
      "Epoch 542/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1134 - Labels_loss: 0.0975 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 543/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1128 - Labels_loss: 0.0969 - sum_layer_3_loss: 1.5884e-05\n",
      "Epoch 544/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1138 - Labels_loss: 0.0979 - sum_layer_3_loss: 1.5875e-05\n",
      "Epoch 545/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1130 - Labels_loss: 0.0971 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 546/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1146 - Labels_loss: 0.0987 - sum_layer_3_loss: 1.5820e-05\n",
      "Epoch 547/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1156 - Labels_loss: 0.0998 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 548/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1138 - Labels_loss: 0.0979 - sum_layer_3_loss: 1.5829e-05\n",
      "Epoch 549/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1116 - Labels_loss: 0.0958 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 550/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1116 - Labels_loss: 0.0958 - sum_layer_3_loss: 1.5859e-05\n",
      "Epoch 551/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1146 - Labels_loss: 0.0988 - sum_layer_3_loss: 1.5878e-05\n",
      "Epoch 552/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1102 - Labels_loss: 0.0944 - sum_layer_3_loss: 1.5826e-05\n",
      "Epoch 553/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1130 - Labels_loss: 0.0972 - sum_layer_3_loss: 1.5857e-05\n",
      "Epoch 554/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1115 - Labels_loss: 0.0957 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 555/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1172 - Labels_loss: 0.1014 - sum_layer_3_loss: 1.5838e-05\n",
      "Epoch 556/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1115 - Labels_loss: 0.0957 - sum_layer_3_loss: 1.5809e-05\n",
      "Epoch 557/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1131 - Labels_loss: 0.0973 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 558/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1105 - Labels_loss: 0.0947 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 559/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1135 - Labels_loss: 0.0976 - sum_layer_3_loss: 1.5857e-05\n",
      "Epoch 560/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5806e-05\n",
      "Epoch 561/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1105 - Labels_loss: 0.0947 - sum_layer_3_loss: 1.5785e-05\n",
      "Epoch 562/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1107 - Labels_loss: 0.0949 - sum_layer_3_loss: 1.5792e-05\n",
      "Epoch 563/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1114 - Labels_loss: 0.0955 - sum_layer_3_loss: 1.5843e-05\n",
      "Epoch 564/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1102 - Labels_loss: 0.0944 - sum_layer_3_loss: 1.5795e-05\n",
      "Epoch 565/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1092 - Labels_loss: 0.0934 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 566/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1075 - Labels_loss: 0.0917 - sum_layer_3_loss: 1.5806e-05\n",
      "Epoch 567/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1211 - Labels_loss: 0.1052 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 568/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1127 - Labels_loss: 0.0968 - sum_layer_3_loss: 1.5844e-05\n",
      "Epoch 569/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1136 - Labels_loss: 0.0978 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 570/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1116 - Labels_loss: 0.0958 - sum_layer_3_loss: 1.5803e-05\n",
      "Epoch 571/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1120 - Labels_loss: 0.0962 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 572/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1119 - Labels_loss: 0.0960 - sum_layer_3_loss: 1.5834e-05\n",
      "Epoch 573/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1177 - Labels_loss: 0.1019 - sum_layer_3_loss: 1.5865e-05\n",
      "Epoch 574/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1102 - Labels_loss: 0.0944 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 575/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1117 - Labels_loss: 0.0959 - sum_layer_3_loss: 1.5858e-05\n",
      "Epoch 576/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1103 - Labels_loss: 0.0945 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 577/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1099 - Labels_loss: 0.0941 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 578/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1086 - Labels_loss: 0.0927 - sum_layer_3_loss: 1.5821e-05\n",
      "Epoch 579/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1082 - Labels_loss: 0.0923 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 580/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1064 - Labels_loss: 0.0905 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 581/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1126 - Labels_loss: 0.0968 - sum_layer_3_loss: 1.5778e-05\n",
      "Epoch 582/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5787e-05\n",
      "Epoch 583/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1166 - Labels_loss: 0.1008 - sum_layer_3_loss: 1.5812e-05\n",
      "Epoch 584/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1100 - Labels_loss: 0.0941 - sum_layer_3_loss: 1.5844e-05\n",
      "Epoch 585/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1079 - Labels_loss: 0.0920 - sum_layer_3_loss: 1.5854e-05\n",
      "Epoch 586/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1075 - Labels_loss: 0.0917 - sum_layer_3_loss: 1.5787e-05\n",
      "Epoch 587/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1120 - Labels_loss: 0.0962 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 588/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1281 - Labels_loss: 0.1123 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 589/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1112 - Labels_loss: 0.0954 - sum_layer_3_loss: 1.5849e-05\n",
      "Epoch 590/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1089 - Labels_loss: 0.0931 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 591/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1063 - Labels_loss: 0.0905 - sum_layer_3_loss: 1.5821e-05\n",
      "Epoch 592/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1089 - Labels_loss: 0.0931 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 593/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1097 - Labels_loss: 0.0938 - sum_layer_3_loss: 1.5832e-05\n",
      "Epoch 594/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1082 - Labels_loss: 0.0924 - sum_layer_3_loss: 1.5835e-05\n",
      "Epoch 595/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1122 - Labels_loss: 0.0964 - sum_layer_3_loss: 1.5812e-05\n",
      "Epoch 596/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1101 - Labels_loss: 0.0943 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 597/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1066 - Labels_loss: 0.0908 - sum_layer_3_loss: 1.5844e-05\n",
      "Epoch 598/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1119 - Labels_loss: 0.0961 - sum_layer_3_loss: 1.5832e-05\n",
      "Epoch 599/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1095 - Labels_loss: 0.0937 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 600/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1096 - Labels_loss: 0.0938 - sum_layer_3_loss: 1.5799e-05\n",
      "Epoch 601/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1087 - Labels_loss: 0.0929 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 602/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1098 - Labels_loss: 0.0940 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 603/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 604/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1091 - Labels_loss: 0.0933 - sum_layer_3_loss: 1.5802e-05\n",
      "Epoch 605/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1100 - Labels_loss: 0.0942 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 606/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1078 - Labels_loss: 0.0920 - sum_layer_3_loss: 1.5797e-05\n",
      "Epoch 607/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5799e-05\n",
      "Epoch 608/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1129 - Labels_loss: 0.0971 - sum_layer_3_loss: 1.5802e-05\n",
      "Epoch 609/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1101 - Labels_loss: 0.0943 - sum_layer_3_loss: 1.5811e-05\n",
      "Epoch 610/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1121 - Labels_loss: 0.0963 - sum_layer_3_loss: 1.5811e-05\n",
      "Epoch 611/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1085 - Labels_loss: 0.0927 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 612/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1125 - Labels_loss: 0.0968 - sum_layer_3_loss: 1.5790e-05\n",
      "Epoch 613/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1081 - Labels_loss: 0.0923 - sum_layer_3_loss: 1.5810e-05\n",
      "Epoch 614/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1089 - Labels_loss: 0.0931 - sum_layer_3_loss: 1.5819e-05\n",
      "Epoch 615/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5846e-05\n",
      "Epoch 616/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1092 - Labels_loss: 0.0934 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 617/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1069 - Labels_loss: 0.0911 - sum_layer_3_loss: 1.5815e-05\n",
      "Epoch 618/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1059 - Labels_loss: 0.0901 - sum_layer_3_loss: 1.5799e-05\n",
      "Epoch 619/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1072 - Labels_loss: 0.0914 - sum_layer_3_loss: 1.5790e-05\n",
      "Epoch 620/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 621/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1177 - Labels_loss: 0.1019 - sum_layer_3_loss: 1.5810e-05\n",
      "Epoch 622/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1042 - Labels_loss: 0.0884 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 623/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1063 - Labels_loss: 0.0904 - sum_layer_3_loss: 1.5852e-05\n",
      "Epoch 624/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1078 - Labels_loss: 0.0920 - sum_layer_3_loss: 1.5801e-05\n",
      "Epoch 625/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1101 - Labels_loss: 0.0943 - sum_layer_3_loss: 1.5806e-05\n",
      "Epoch 626/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1103 - Labels_loss: 0.0945 - sum_layer_3_loss: 1.5822e-05\n",
      "Epoch 627/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1116 - Labels_loss: 0.0958 - sum_layer_3_loss: 1.5812e-05\n",
      "Epoch 628/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1249 - Labels_loss: 0.1091 - sum_layer_3_loss: 1.5843e-05\n",
      "Epoch 629/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1113 - Labels_loss: 0.0955 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 630/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1123 - Labels_loss: 0.0964 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 631/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1079 - Labels_loss: 0.0921 - sum_layer_3_loss: 1.5807e-05\n",
      "Epoch 632/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1120 - Labels_loss: 0.0961 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 633/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1147 - Labels_loss: 0.0990 - sum_layer_3_loss: 1.5777e-05\n",
      "Epoch 634/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1060 - Labels_loss: 0.0902 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 635/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1077 - Labels_loss: 0.0918 - sum_layer_3_loss: 1.5817e-05\n",
      "Epoch 636/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1069 - Labels_loss: 0.0911 - sum_layer_3_loss: 1.5810e-05\n",
      "Epoch 637/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1109 - Labels_loss: 0.0950 - sum_layer_3_loss: 1.5805e-05\n",
      "Epoch 638/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1064 - Labels_loss: 0.0906 - sum_layer_3_loss: 1.5810e-05\n",
      "Epoch 639/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1097 - Labels_loss: 0.0939 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 640/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1096 - Labels_loss: 0.0938 - sum_layer_3_loss: 1.5795e-05\n",
      "Epoch 641/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1058 - Labels_loss: 0.0900 - sum_layer_3_loss: 1.5845e-05\n",
      "Epoch 642/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1108 - Labels_loss: 0.0950 - sum_layer_3_loss: 1.5790e-05\n",
      "Epoch 643/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1126 - Labels_loss: 0.0968 - sum_layer_3_loss: 1.5828e-05\n",
      "Epoch 644/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1041 - Labels_loss: 0.0883 - sum_layer_3_loss: 1.5786e-05\n",
      "Epoch 645/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1077 - Labels_loss: 0.0919 - sum_layer_3_loss: 1.5800e-05\n",
      "Epoch 646/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1082 - Labels_loss: 0.0924 - sum_layer_3_loss: 1.5798e-05\n",
      "Epoch 647/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1065 - Labels_loss: 0.0907 - sum_layer_3_loss: 1.5796e-05\n",
      "Epoch 648/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1053 - Labels_loss: 0.0895 - sum_layer_3_loss: 1.5798e-05\n",
      "Epoch 649/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1078 - Labels_loss: 0.0920 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 650/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1174 - Labels_loss: 0.1016 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 651/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1068 - Labels_loss: 0.0909 - sum_layer_3_loss: 1.5811e-05\n",
      "Epoch 652/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1040 - Labels_loss: 0.0882 - sum_layer_3_loss: 1.5809e-05\n",
      "Epoch 653/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1102 - Labels_loss: 0.0944 - sum_layer_3_loss: 1.5795e-05\n",
      "Epoch 654/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1141 - Labels_loss: 0.0982 - sum_layer_3_loss: 1.5881e-05\n",
      "Epoch 655/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1133 - Labels_loss: 0.0974 - sum_layer_3_loss: 1.5825e-05\n",
      "Epoch 656/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1084 - Labels_loss: 0.0926 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 657/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1059 - Labels_loss: 0.0900 - sum_layer_3_loss: 1.5889e-05\n",
      "Epoch 658/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1048 - Labels_loss: 0.0889 - sum_layer_3_loss: 1.5809e-05\n",
      "Epoch 659/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1060 - Labels_loss: 0.0903 - sum_layer_3_loss: 1.5770e-05\n",
      "Epoch 660/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1073 - Labels_loss: 0.0915 - sum_layer_3_loss: 1.5783e-05\n",
      "Epoch 661/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1072 - Labels_loss: 0.0913 - sum_layer_3_loss: 1.5849e-05\n",
      "Epoch 662/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1044 - Labels_loss: 0.0885 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 663/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1104 - Labels_loss: 0.0946 - sum_layer_3_loss: 1.5801e-05\n",
      "Epoch 664/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1074 - Labels_loss: 0.0916 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 665/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1048 - Labels_loss: 0.0889 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 666/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1034 - Labels_loss: 0.0876 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 667/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1039 - Labels_loss: 0.0881 - sum_layer_3_loss: 1.5780e-05\n",
      "Epoch 668/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1085 - Labels_loss: 0.0927 - sum_layer_3_loss: 1.5758e-05\n",
      "Epoch 669/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1157 - Labels_loss: 0.0999 - sum_layer_3_loss: 1.5831e-05\n",
      "Epoch 670/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1048 - Labels_loss: 0.0891 - sum_layer_3_loss: 1.5791e-05\n",
      "Epoch 671/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1100 - Labels_loss: 0.0941 - sum_layer_3_loss: 1.5836e-05\n",
      "Epoch 672/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1099 - Labels_loss: 0.0941 - sum_layer_3_loss: 1.5864e-05\n",
      "Epoch 673/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1093 - Labels_loss: 0.0935 - sum_layer_3_loss: 1.5822e-05\n",
      "Epoch 674/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1061 - Labels_loss: 0.0902 - sum_layer_3_loss: 1.5819e-05\n",
      "Epoch 675/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1108 - Labels_loss: 0.0950 - sum_layer_3_loss: 1.5842e-05\n",
      "Epoch 676/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1067 - Labels_loss: 0.0909 - sum_layer_3_loss: 1.5819e-05\n",
      "Epoch 677/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1065 - Labels_loss: 0.0907 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 678/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1054 - Labels_loss: 0.0896 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 679/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1037 - Labels_loss: 0.0878 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 680/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1120 - Labels_loss: 0.0962 - sum_layer_3_loss: 1.5839e-05\n",
      "Epoch 681/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1085 - Labels_loss: 0.0927 - sum_layer_3_loss: 1.5779e-05\n",
      "Epoch 682/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1049 - Labels_loss: 0.0891 - sum_layer_3_loss: 1.5775e-05\n",
      "Epoch 683/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1074 - Labels_loss: 0.0916 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 684/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1045 - Labels_loss: 0.0887 - sum_layer_3_loss: 1.5788e-05\n",
      "Epoch 685/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1066 - Labels_loss: 0.0908 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 686/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1086 - Labels_loss: 0.0928 - sum_layer_3_loss: 1.5827e-05\n",
      "Epoch 687/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1103 - Labels_loss: 0.0944 - sum_layer_3_loss: 1.5835e-05\n",
      "Epoch 688/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1077 - Labels_loss: 0.0919 - sum_layer_3_loss: 1.5805e-05\n",
      "Epoch 689/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1088 - Labels_loss: 0.0930 - sum_layer_3_loss: 1.5840e-05\n",
      "Epoch 690/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1038 - Labels_loss: 0.0880 - sum_layer_3_loss: 1.5788e-05\n",
      "Epoch 691/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1047 - Labels_loss: 0.0889 - sum_layer_3_loss: 1.5788e-05\n",
      "Epoch 692/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1051 - Labels_loss: 0.0893 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 693/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1044 - Labels_loss: 0.0886 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 694/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1029 - Labels_loss: 0.0871 - sum_layer_3_loss: 1.5805e-05\n",
      "Epoch 695/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1040 - Labels_loss: 0.0883 - sum_layer_3_loss: 1.5777e-05\n",
      "Epoch 696/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1043 - Labels_loss: 0.0885 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 697/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1037 - Labels_loss: 0.0880 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 698/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1077 - Labels_loss: 0.0919 - sum_layer_3_loss: 1.5785e-05\n",
      "Epoch 699/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1050 - Labels_loss: 0.0892 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 700/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1053 - Labels_loss: 0.0895 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 701/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5786e-05\n",
      "Epoch 702/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1076 - Labels_loss: 0.0918 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 703/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1075 - Labels_loss: 0.0917 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 704/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1020 - Labels_loss: 0.0862 - sum_layer_3_loss: 1.5806e-05\n",
      "Epoch 705/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1046 - Labels_loss: 0.0888 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 706/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1026 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5826e-05\n",
      "Epoch 707/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1066 - Labels_loss: 0.0908 - sum_layer_3_loss: 1.5797e-05\n",
      "Epoch 708/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1035 - Labels_loss: 0.0877 - sum_layer_3_loss: 1.5779e-05\n",
      "Epoch 709/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1056 - Labels_loss: 0.0899 - sum_layer_3_loss: 1.5780e-05\n",
      "Epoch 710/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1069 - Labels_loss: 0.0911 - sum_layer_3_loss: 1.5787e-05\n",
      "Epoch 711/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1011 - Labels_loss: 0.0853 - sum_layer_3_loss: 1.5784e-05\n",
      "Epoch 712/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1286 - Labels_loss: 0.1128 - sum_layer_3_loss: 1.5816e-05\n",
      "Epoch 713/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1127 - Labels_loss: 0.0969 - sum_layer_3_loss: 1.5818e-05\n",
      "Epoch 714/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1049 - Labels_loss: 0.0891 - sum_layer_3_loss: 1.5797e-05\n",
      "Epoch 715/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1028 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5792e-05\n",
      "Epoch 716/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0848 - sum_layer_3_loss: 1.5799e-05\n",
      "Epoch 717/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1008 - Labels_loss: 0.0850 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 718/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0869 - sum_layer_3_loss: 1.5806e-05\n",
      "Epoch 719/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1063 - Labels_loss: 0.0905 - sum_layer_3_loss: 1.5791e-05\n",
      "Epoch 720/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1036 - Labels_loss: 0.0879 - sum_layer_3_loss: 1.5771e-05\n",
      "Epoch 721/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1023 - Labels_loss: 0.0865 - sum_layer_3_loss: 1.5813e-05\n",
      "Epoch 722/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1019 - Labels_loss: 0.0861 - sum_layer_3_loss: 1.5816e-05\n",
      "Epoch 723/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1051 - Labels_loss: 0.0894 - sum_layer_3_loss: 1.5796e-05\n",
      "Epoch 724/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1066 - Labels_loss: 0.0908 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 725/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1064 - Labels_loss: 0.0906 - sum_layer_3_loss: 1.5817e-05\n",
      "Epoch 726/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1042 - Labels_loss: 0.0883 - sum_layer_3_loss: 1.5820e-05\n",
      "Epoch 727/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1065 - Labels_loss: 0.0907 - sum_layer_3_loss: 1.5830e-05\n",
      "Epoch 728/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1022 - Labels_loss: 0.0864 - sum_layer_3_loss: 1.5755e-05\n",
      "Epoch 729/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1058 - Labels_loss: 0.0900 - sum_layer_3_loss: 1.5762e-05\n",
      "Epoch 730/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1051 - Labels_loss: 0.0893 - sum_layer_3_loss: 1.5812e-05\n",
      "Epoch 731/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1061 - Labels_loss: 0.0902 - sum_layer_3_loss: 1.5825e-05\n",
      "Epoch 732/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1075 - Labels_loss: 0.0917 - sum_layer_3_loss: 1.5822e-05\n",
      "Epoch 733/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0869 - sum_layer_3_loss: 1.5784e-05\n",
      "Epoch 734/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1014 - Labels_loss: 0.0857 - sum_layer_3_loss: 1.5783e-05\n",
      "Epoch 735/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1014 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5779e-05\n",
      "Epoch 736/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1037 - Labels_loss: 0.0879 - sum_layer_3_loss: 1.5780e-05\n",
      "Epoch 737/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1081 - Labels_loss: 0.0923 - sum_layer_3_loss: 1.5808e-05\n",
      "Epoch 738/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1002 - Labels_loss: 0.0844 - sum_layer_3_loss: 1.5793e-05\n",
      "Epoch 739/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1055 - Labels_loss: 0.0897 - sum_layer_3_loss: 1.5790e-05\n",
      "Epoch 740/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1029 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5809e-05\n",
      "Epoch 741/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1051 - Labels_loss: 0.0893 - sum_layer_3_loss: 1.5785e-05\n",
      "Epoch 742/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1017 - Labels_loss: 0.0860 - sum_layer_3_loss: 1.5760e-05\n",
      "Epoch 743/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0848 - sum_layer_3_loss: 1.5802e-05\n",
      "Epoch 744/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1054 - Labels_loss: 0.0897 - sum_layer_3_loss: 1.5771e-05\n",
      "Epoch 745/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1079 - Labels_loss: 0.0921 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 746/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1031 - Labels_loss: 0.0874 - sum_layer_3_loss: 1.5780e-05\n",
      "Epoch 747/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1045 - Labels_loss: 0.0887 - sum_layer_3_loss: 1.5820e-05\n",
      "Epoch 748/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1034 - Labels_loss: 0.0877 - sum_layer_3_loss: 1.5779e-05\n",
      "Epoch 749/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1023 - Labels_loss: 0.0865 - sum_layer_3_loss: 1.5776e-05\n",
      "Epoch 750/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1048 - Labels_loss: 0.0890 - sum_layer_3_loss: 1.5791e-05\n",
      "Epoch 751/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1079 - Labels_loss: 0.0922 - sum_layer_3_loss: 1.5783e-05\n",
      "Epoch 752/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1054 - Labels_loss: 0.0896 - sum_layer_3_loss: 1.5792e-05\n",
      "Epoch 753/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1000 - Labels_loss: 0.0842 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 754/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0993 - Labels_loss: 0.0835 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 755/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1017 - Labels_loss: 0.0859 - sum_layer_3_loss: 1.5777e-05\n",
      "Epoch 756/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0995 - Labels_loss: 0.0837 - sum_layer_3_loss: 1.5820e-05\n",
      "Epoch 757/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1007 - Labels_loss: 0.0849 - sum_layer_3_loss: 1.5820e-05\n",
      "Epoch 758/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1028 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5795e-05\n",
      "Epoch 759/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1026 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5847e-05\n",
      "Epoch 760/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1051 - Labels_loss: 0.0892 - sum_layer_3_loss: 1.5803e-05\n",
      "Epoch 761/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1058 - Labels_loss: 0.0900 - sum_layer_3_loss: 1.5800e-05\n",
      "Epoch 762/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1200 - Labels_loss: 0.1042 - sum_layer_3_loss: 1.5823e-05\n",
      "Epoch 763/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1046 - Labels_loss: 0.0889 - sum_layer_3_loss: 1.5758e-05\n",
      "Epoch 764/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1019 - Labels_loss: 0.0862 - sum_layer_3_loss: 1.5758e-05\n",
      "Epoch 765/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1037 - Labels_loss: 0.0879 - sum_layer_3_loss: 1.5794e-05\n",
      "Epoch 766/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1032 - Labels_loss: 0.0874 - sum_layer_3_loss: 1.5761e-05\n",
      "Epoch 767/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1030 - Labels_loss: 0.0871 - sum_layer_3_loss: 1.5843e-05\n",
      "Epoch 768/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1042 - Labels_loss: 0.0884 - sum_layer_3_loss: 1.5853e-05\n",
      "Epoch 769/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0830 - sum_layer_3_loss: 1.5791e-05\n",
      "Epoch 770/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1017 - Labels_loss: 0.0859 - sum_layer_3_loss: 1.5787e-05\n",
      "Epoch 771/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1040 - Labels_loss: 0.0883 - sum_layer_3_loss: 1.5774e-05\n",
      "Epoch 772/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1014 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5796e-05\n",
      "Epoch 773/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1003 - Labels_loss: 0.0845 - sum_layer_3_loss: 1.5800e-05\n",
      "Epoch 774/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0998 - Labels_loss: 0.0841 - sum_layer_3_loss: 1.5778e-05\n",
      "Epoch 775/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1038 - Labels_loss: 0.0880 - sum_layer_3_loss: 1.5804e-05\n",
      "Epoch 776/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1033 - Labels_loss: 0.0875 - sum_layer_3_loss: 1.5824e-05\n",
      "Epoch 777/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5814e-05\n",
      "Epoch 778/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1012 - Labels_loss: 0.0854 - sum_layer_3_loss: 1.5776e-05\n",
      "Epoch 779/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0996 - Labels_loss: 0.0838 - sum_layer_3_loss: 1.5784e-05\n",
      "Epoch 780/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1035 - Labels_loss: 0.0877 - sum_layer_3_loss: 1.5777e-05\n",
      "Epoch 781/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0848 - sum_layer_3_loss: 1.5799e-05\n",
      "Epoch 782/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1033 - Labels_loss: 0.0875 - sum_layer_3_loss: 1.5833e-05\n",
      "Epoch 783/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1021 - Labels_loss: 0.0863 - sum_layer_3_loss: 1.5785e-05\n",
      "Epoch 784/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1025 - Labels_loss: 0.0867 - sum_layer_3_loss: 1.5781e-05\n",
      "Epoch 785/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1044 - Labels_loss: 0.0886 - sum_layer_3_loss: 1.5800e-05\n",
      "Epoch 786/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1018 - Labels_loss: 0.0860 - sum_layer_3_loss: 1.5812e-05\n",
      "Epoch 787/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1036 - Labels_loss: 0.0879 - sum_layer_3_loss: 1.5769e-05\n",
      "Epoch 788/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1024 - Labels_loss: 0.0866 - sum_layer_3_loss: 1.5754e-05\n",
      "Epoch 789/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1037 - Labels_loss: 0.0879 - sum_layer_3_loss: 1.5786e-05\n",
      "Epoch 790/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1026 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5781e-05\n",
      "Epoch 791/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0990 - Labels_loss: 0.0832 - sum_layer_3_loss: 1.5798e-05\n",
      "Epoch 792/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0995 - Labels_loss: 0.0838 - sum_layer_3_loss: 1.5763e-05\n",
      "Epoch 793/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1049 - Labels_loss: 0.0891 - sum_layer_3_loss: 1.5766e-05\n",
      "Epoch 794/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0991 - Labels_loss: 0.0834 - sum_layer_3_loss: 1.5740e-05\n",
      "Epoch 795/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1034 - Labels_loss: 0.0876 - sum_layer_3_loss: 1.5765e-05\n",
      "Epoch 796/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1019 - Labels_loss: 0.0861 - sum_layer_3_loss: 1.5781e-05\n",
      "Epoch 797/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0992 - Labels_loss: 0.0835 - sum_layer_3_loss: 1.5770e-05\n",
      "Epoch 798/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0869 - sum_layer_3_loss: 1.5780e-05\n",
      "Epoch 799/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0997 - Labels_loss: 0.0839 - sum_layer_3_loss: 1.5803e-05\n",
      "Epoch 800/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0848 - sum_layer_3_loss: 1.5757e-05\n",
      "Epoch 801/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0830 - sum_layer_3_loss: 1.5754e-05\n",
      "Epoch 802/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1025 - Labels_loss: 0.0867 - sum_layer_3_loss: 1.5807e-05\n",
      "Epoch 803/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0982 - Labels_loss: 0.0825 - sum_layer_3_loss: 1.5775e-05\n",
      "Epoch 804/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1028 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5798e-05\n",
      "Epoch 805/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1004 - Labels_loss: 0.0846 - sum_layer_3_loss: 1.5760e-05\n",
      "Epoch 806/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1056 - Labels_loss: 0.0898 - sum_layer_3_loss: 1.5797e-05\n",
      "Epoch 807/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1035 - Labels_loss: 0.0877 - sum_layer_3_loss: 1.5788e-05\n",
      "Epoch 808/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1020 - Labels_loss: 0.0862 - sum_layer_3_loss: 1.5774e-05\n",
      "Epoch 809/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1016 - Labels_loss: 0.0859 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 810/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1003 - Labels_loss: 0.0845 - sum_layer_3_loss: 1.5772e-05\n",
      "Epoch 811/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0997 - Labels_loss: 0.0839 - sum_layer_3_loss: 1.5773e-05\n",
      "Epoch 812/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1069 - Labels_loss: 0.0911 - sum_layer_3_loss: 1.5774e-05\n",
      "Epoch 813/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1084 - Labels_loss: 0.0926 - sum_layer_3_loss: 1.5771e-05\n",
      "Epoch 814/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0869 - sum_layer_3_loss: 1.5752e-05\n",
      "Epoch 815/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1019 - Labels_loss: 0.0862 - sum_layer_3_loss: 1.5748e-05\n",
      "Epoch 816/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1003 - Labels_loss: 0.0846 - sum_layer_3_loss: 1.5764e-05\n",
      "Epoch 817/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0995 - Labels_loss: 0.0838 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 818/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1005 - Labels_loss: 0.0847 - sum_layer_3_loss: 1.5743e-05\n",
      "Epoch 819/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0990 - Labels_loss: 0.0833 - sum_layer_3_loss: 1.5747e-05\n",
      "Epoch 820/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1034 - Labels_loss: 0.0876 - sum_layer_3_loss: 1.5778e-05\n",
      "Epoch 821/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1067 - Labels_loss: 0.0909 - sum_layer_3_loss: 1.5786e-05\n",
      "Epoch 822/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1061 - Labels_loss: 0.0903 - sum_layer_3_loss: 1.5764e-05\n",
      "Epoch 823/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.0991 - Labels_loss: 0.0833 - sum_layer_3_loss: 1.5781e-05\n",
      "Epoch 824/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1049 - Labels_loss: 0.0891 - sum_layer_3_loss: 1.5771e-05\n",
      "Epoch 825/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1025 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5755e-05\n",
      "Epoch 826/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1005 - Labels_loss: 0.0847 - sum_layer_3_loss: 1.5756e-05\n",
      "Epoch 827/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0984 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5751e-05\n",
      "Epoch 828/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0989 - Labels_loss: 0.0832 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 829/3000\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.1030 - Labels_loss: 0.0873 - sum_layer_3_loss: 1.5759e-05\n",
      "Epoch 830/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0983 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5754e-05\n",
      "Epoch 831/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1070 - Labels_loss: 0.0913 - sum_layer_3_loss: 1.5739e-05\n",
      "Epoch 832/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1004 - Labels_loss: 0.0846 - sum_layer_3_loss: 1.5758e-05\n",
      "Epoch 833/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1038 - Labels_loss: 0.0880 - sum_layer_3_loss: 1.5760e-05\n",
      "Epoch 834/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1027 - Labels_loss: 0.0870 - sum_layer_3_loss: 1.5746e-05\n",
      "Epoch 835/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1038 - Labels_loss: 0.0881 - sum_layer_3_loss: 1.5760e-05\n",
      "Epoch 836/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1003 - Labels_loss: 0.0845 - sum_layer_3_loss: 1.5746e-05\n",
      "Epoch 837/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1022 - Labels_loss: 0.0865 - sum_layer_3_loss: 1.5731e-05\n",
      "Epoch 838/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1018 - Labels_loss: 0.0861 - sum_layer_3_loss: 1.5740e-05\n",
      "Epoch 839/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1021 - Labels_loss: 0.0863 - sum_layer_3_loss: 1.5772e-05\n",
      "Epoch 840/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0985 - Labels_loss: 0.0828 - sum_layer_3_loss: 1.5752e-05\n",
      "Epoch 841/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1031 - Labels_loss: 0.0874 - sum_layer_3_loss: 1.5736e-05\n",
      "Epoch 842/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0979 - Labels_loss: 0.0821 - sum_layer_3_loss: 1.5738e-05\n",
      "Epoch 843/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1023 - Labels_loss: 0.0865 - sum_layer_3_loss: 1.5746e-05\n",
      "Epoch 844/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1013 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5748e-05\n",
      "Epoch 845/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0948 - Labels_loss: 0.0790 - sum_layer_3_loss: 1.5761e-05\n",
      "Epoch 846/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1017 - Labels_loss: 0.0859 - sum_layer_3_loss: 1.5734e-05\n",
      "Epoch 847/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0975 - Labels_loss: 0.0817 - sum_layer_3_loss: 1.5752e-05\n",
      "Epoch 848/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0996 - Labels_loss: 0.0838 - sum_layer_3_loss: 1.5760e-05\n",
      "Epoch 849/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0830 - sum_layer_3_loss: 1.5735e-05\n",
      "Epoch 850/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0978 - Labels_loss: 0.0820 - sum_layer_3_loss: 1.5775e-05\n",
      "Epoch 851/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0985 - Labels_loss: 0.0827 - sum_layer_3_loss: 1.5724e-05\n",
      "Epoch 852/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0989 - Labels_loss: 0.0832 - sum_layer_3_loss: 1.5707e-05\n",
      "Epoch 853/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5736e-05\n",
      "Epoch 854/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1031 - Labels_loss: 0.0874 - sum_layer_3_loss: 1.5727e-05\n",
      "Epoch 855/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0980 - Labels_loss: 0.0823 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 856/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0975 - Labels_loss: 0.0817 - sum_layer_3_loss: 1.5758e-05\n",
      "Epoch 857/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0982 - Labels_loss: 0.0824 - sum_layer_3_loss: 1.5740e-05\n",
      "Epoch 858/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1013 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5735e-05\n",
      "Epoch 859/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0991 - Labels_loss: 0.0834 - sum_layer_3_loss: 1.5725e-05\n",
      "Epoch 860/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0957 - Labels_loss: 0.0799 - sum_layer_3_loss: 1.5728e-05\n",
      "Epoch 861/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1020 - Labels_loss: 0.0863 - sum_layer_3_loss: 1.5703e-05\n",
      "Epoch 862/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1013 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5720e-05\n",
      "Epoch 863/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0991 - Labels_loss: 0.0833 - sum_layer_3_loss: 1.5757e-05\n",
      "Epoch 864/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1000 - Labels_loss: 0.0843 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 865/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5725e-05\n",
      "Epoch 866/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0997 - Labels_loss: 0.0840 - sum_layer_3_loss: 1.5707e-05\n",
      "Epoch 867/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0982 - Labels_loss: 0.0825 - sum_layer_3_loss: 1.5734e-05\n",
      "Epoch 868/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0977 - Labels_loss: 0.0819 - sum_layer_3_loss: 1.5745e-05\n",
      "Epoch 869/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0992 - Labels_loss: 0.0835 - sum_layer_3_loss: 1.5708e-05\n",
      "Epoch 870/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1004 - Labels_loss: 0.0847 - sum_layer_3_loss: 1.5716e-05\n",
      "Epoch 871/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0994 - Labels_loss: 0.0837 - sum_layer_3_loss: 1.5714e-05\n",
      "Epoch 872/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1007 - Labels_loss: 0.0849 - sum_layer_3_loss: 1.5722e-05\n",
      "Epoch 873/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1024 - Labels_loss: 0.0867 - sum_layer_3_loss: 1.5753e-05\n",
      "Epoch 874/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1018 - Labels_loss: 0.0861 - sum_layer_3_loss: 1.5705e-05\n",
      "Epoch 875/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1108 - Labels_loss: 0.0950 - sum_layer_3_loss: 1.5769e-05\n",
      "Epoch 876/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1014 - Labels_loss: 0.0856 - sum_layer_3_loss: 1.5735e-05\n",
      "Epoch 877/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0979 - Labels_loss: 0.0822 - sum_layer_3_loss: 1.5741e-05\n",
      "Epoch 878/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0987 - Labels_loss: 0.0830 - sum_layer_3_loss: 1.5720e-05\n",
      "Epoch 879/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0981 - Labels_loss: 0.0823 - sum_layer_3_loss: 1.5730e-05\n",
      "Epoch 880/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1000 - Labels_loss: 0.0843 - sum_layer_3_loss: 1.5733e-05\n",
      "Epoch 881/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1010 - Labels_loss: 0.0853 - sum_layer_3_loss: 1.5725e-05\n",
      "Epoch 882/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0945 - Labels_loss: 0.0787 - sum_layer_3_loss: 1.5743e-05\n",
      "Epoch 883/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0986 - Labels_loss: 0.0829 - sum_layer_3_loss: 1.5767e-05\n",
      "Epoch 884/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0952 - Labels_loss: 0.0795 - sum_layer_3_loss: 1.5743e-05\n",
      "Epoch 885/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1002 - Labels_loss: 0.0845 - sum_layer_3_loss: 1.5727e-05\n",
      "Epoch 886/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0971 - Labels_loss: 0.0814 - sum_layer_3_loss: 1.5738e-05\n",
      "Epoch 887/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1022 - Labels_loss: 0.0864 - sum_layer_3_loss: 1.5752e-05\n",
      "Epoch 888/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1076 - Labels_loss: 0.0918 - sum_layer_3_loss: 1.5757e-05\n",
      "Epoch 889/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0993 - Labels_loss: 0.0836 - sum_layer_3_loss: 1.5714e-05\n",
      "Epoch 890/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0969 - Labels_loss: 0.0812 - sum_layer_3_loss: 1.5705e-05\n",
      "Epoch 891/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0951 - Labels_loss: 0.0794 - sum_layer_3_loss: 1.5724e-05\n",
      "Epoch 892/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.1012 - Labels_loss: 0.0855 - sum_layer_3_loss: 1.5728e-05\n",
      "Epoch 893/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0945 - Labels_loss: 0.0788 - sum_layer_3_loss: 1.5720e-05\n",
      "Epoch 894/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1011 - Labels_loss: 0.0854 - sum_layer_3_loss: 1.5733e-05\n",
      "Epoch 895/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0941 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5715e-05\n",
      "Epoch 896/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0987 - Labels_loss: 0.0829 - sum_layer_3_loss: 1.5734e-05\n",
      "Epoch 897/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0968 - Labels_loss: 0.0810 - sum_layer_3_loss: 1.5744e-05\n",
      "Epoch 898/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0989 - Labels_loss: 0.0831 - sum_layer_3_loss: 1.5731e-05\n",
      "Epoch 899/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0983 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5719e-05\n",
      "Epoch 900/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0975 - Labels_loss: 0.0817 - sum_layer_3_loss: 1.5728e-05\n",
      "Epoch 901/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0958 - Labels_loss: 0.0801 - sum_layer_3_loss: 1.5697e-05\n",
      "Epoch 902/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0976 - Labels_loss: 0.0819 - sum_layer_3_loss: 1.5705e-05\n",
      "Epoch 903/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1004 - Labels_loss: 0.0847 - sum_layer_3_loss: 1.5706e-05\n",
      "Epoch 904/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0966 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5679e-05\n",
      "Epoch 905/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1025 - Labels_loss: 0.0868 - sum_layer_3_loss: 1.5709e-05\n",
      "Epoch 906/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0992 - Labels_loss: 0.0835 - sum_layer_3_loss: 1.5685e-05\n",
      "Epoch 907/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0970 - Labels_loss: 0.0813 - sum_layer_3_loss: 1.5715e-05\n",
      "Epoch 908/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0970 - Labels_loss: 0.0813 - sum_layer_3_loss: 1.5691e-05\n",
      "Epoch 909/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0981 - Labels_loss: 0.0824 - sum_layer_3_loss: 1.5698e-05\n",
      "Epoch 910/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0986 - Labels_loss: 0.0829 - sum_layer_3_loss: 1.5683e-05\n",
      "Epoch 911/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0976 - Labels_loss: 0.0819 - sum_layer_3_loss: 1.5695e-05\n",
      "Epoch 912/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0990 - Labels_loss: 0.0833 - sum_layer_3_loss: 1.5700e-05\n",
      "Epoch 913/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1014 - Labels_loss: 0.0857 - sum_layer_3_loss: 1.5690e-05\n",
      "Epoch 914/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0968 - Labels_loss: 0.0811 - sum_layer_3_loss: 1.5674e-05\n",
      "Epoch 915/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0808 - sum_layer_3_loss: 1.5676e-05\n",
      "Epoch 916/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0849 - sum_layer_3_loss: 1.5723e-05\n",
      "Epoch 917/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0963 - Labels_loss: 0.0806 - sum_layer_3_loss: 1.5679e-05\n",
      "Epoch 918/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0983 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5709e-05\n",
      "Epoch 919/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1024 - Labels_loss: 0.0867 - sum_layer_3_loss: 1.5695e-05\n",
      "Epoch 920/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0971 - Labels_loss: 0.0814 - sum_layer_3_loss: 1.5689e-05\n",
      "Epoch 921/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5667e-05\n",
      "Epoch 922/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0960 - Labels_loss: 0.0804 - sum_layer_3_loss: 1.5659e-05\n",
      "Epoch 923/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0946 - Labels_loss: 0.0789 - sum_layer_3_loss: 1.5692e-05\n",
      "Epoch 924/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0996 - Labels_loss: 0.0839 - sum_layer_3_loss: 1.5662e-05\n",
      "Epoch 925/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0967 - Labels_loss: 0.0810 - sum_layer_3_loss: 1.5674e-05\n",
      "Epoch 926/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0950 - Labels_loss: 0.0793 - sum_layer_3_loss: 1.5688e-05\n",
      "Epoch 927/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0982 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5677e-05\n",
      "Epoch 928/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1006 - Labels_loss: 0.0850 - sum_layer_3_loss: 1.5670e-05\n",
      "Epoch 929/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5665e-05\n",
      "Epoch 930/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0979 - Labels_loss: 0.0822 - sum_layer_3_loss: 1.5680e-05\n",
      "Epoch 931/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0975 - Labels_loss: 0.0818 - sum_layer_3_loss: 1.5677e-05\n",
      "Epoch 932/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0974 - Labels_loss: 0.0817 - sum_layer_3_loss: 1.5655e-05\n",
      "Epoch 933/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1003 - Labels_loss: 0.0847 - sum_layer_3_loss: 1.5681e-05\n",
      "Epoch 934/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0952 - Labels_loss: 0.0796 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 935/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1007 - Labels_loss: 0.0851 - sum_layer_3_loss: 1.5668e-05\n",
      "Epoch 936/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0959 - Labels_loss: 0.0802 - sum_layer_3_loss: 1.5674e-05\n",
      "Epoch 937/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0998 - Labels_loss: 0.0841 - sum_layer_3_loss: 1.5676e-05\n",
      "Epoch 938/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0958 - Labels_loss: 0.0801 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 939/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5708e-05\n",
      "Epoch 940/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0961 - Labels_loss: 0.0804 - sum_layer_3_loss: 1.5694e-05\n",
      "Epoch 941/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0987 - Labels_loss: 0.0830 - sum_layer_3_loss: 1.5694e-05\n",
      "Epoch 942/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0981 - Labels_loss: 0.0825 - sum_layer_3_loss: 1.5666e-05\n",
      "Epoch 943/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1004 - Labels_loss: 0.0846 - sum_layer_3_loss: 1.5714e-05\n",
      "Epoch 944/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0986 - Labels_loss: 0.0829 - sum_layer_3_loss: 1.5666e-05\n",
      "Epoch 945/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0962 - Labels_loss: 0.0805 - sum_layer_3_loss: 1.5679e-05\n",
      "Epoch 946/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0967 - Labels_loss: 0.0810 - sum_layer_3_loss: 1.5684e-05\n",
      "Epoch 947/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0957 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5657e-05\n",
      "Epoch 948/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0942 - Labels_loss: 0.0785 - sum_layer_3_loss: 1.5682e-05\n",
      "Epoch 949/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0931 - Labels_loss: 0.0774 - sum_layer_3_loss: 1.5686e-05\n",
      "Epoch 950/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0950 - Labels_loss: 0.0793 - sum_layer_3_loss: 1.5671e-05\n",
      "Epoch 951/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0957 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5662e-05\n",
      "Epoch 952/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0941 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5689e-05\n",
      "Epoch 953/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0783 - sum_layer_3_loss: 1.5650e-05\n",
      "Epoch 954/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0961 - Labels_loss: 0.0804 - sum_layer_3_loss: 1.5668e-05\n",
      "Epoch 955/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0948 - Labels_loss: 0.0792 - sum_layer_3_loss: 1.5645e-05\n",
      "Epoch 956/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0951 - Labels_loss: 0.0795 - sum_layer_3_loss: 1.5656e-05\n",
      "Epoch 957/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0952 - Labels_loss: 0.0795 - sum_layer_3_loss: 1.5684e-05\n",
      "Epoch 958/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0929 - Labels_loss: 0.0773 - sum_layer_3_loss: 1.5652e-05\n",
      "Epoch 959/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0954 - Labels_loss: 0.0797 - sum_layer_3_loss: 1.5648e-05\n",
      "Epoch 960/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0969 - Labels_loss: 0.0812 - sum_layer_3_loss: 1.5645e-05\n",
      "Epoch 961/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0947 - Labels_loss: 0.0790 - sum_layer_3_loss: 1.5636e-05\n",
      "Epoch 962/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5665e-05\n",
      "Epoch 963/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0954 - Labels_loss: 0.0798 - sum_layer_3_loss: 1.5654e-05\n",
      "Epoch 964/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0956 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5669e-05\n",
      "Epoch 965/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0906 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 966/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0937 - Labels_loss: 0.0781 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 967/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0831 - sum_layer_3_loss: 1.5684e-05\n",
      "Epoch 968/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0968 - Labels_loss: 0.0812 - sum_layer_3_loss: 1.5662e-05\n",
      "Epoch 969/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0951 - Labels_loss: 0.0794 - sum_layer_3_loss: 1.5662e-05\n",
      "Epoch 970/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0980 - Labels_loss: 0.0823 - sum_layer_3_loss: 1.5680e-05\n",
      "Epoch 971/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1017 - Labels_loss: 0.0861 - sum_layer_3_loss: 1.5657e-05\n",
      "Epoch 972/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1035 - Labels_loss: 0.0878 - sum_layer_3_loss: 1.5680e-05\n",
      "Epoch 973/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0953 - Labels_loss: 0.0796 - sum_layer_3_loss: 1.5664e-05\n",
      "Epoch 974/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5648e-05\n",
      "Epoch 975/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0955 - Labels_loss: 0.0799 - sum_layer_3_loss: 1.5651e-05\n",
      "Epoch 976/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0962 - Labels_loss: 0.0806 - sum_layer_3_loss: 1.5644e-05\n",
      "Epoch 977/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0985 - Labels_loss: 0.0828 - sum_layer_3_loss: 1.5669e-05\n",
      "Epoch 978/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0966 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 979/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1007 - Labels_loss: 0.0850 - sum_layer_3_loss: 1.5715e-05\n",
      "Epoch 980/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0995 - Labels_loss: 0.0839 - sum_layer_3_loss: 1.5644e-05\n",
      "Epoch 981/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0956 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 982/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0945 - Labels_loss: 0.0788 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 983/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1015 - Labels_loss: 0.0858 - sum_layer_3_loss: 1.5661e-05\n",
      "Epoch 984/3000\n",
      "242/242 [==============================] - 4s 19ms/step - loss: 0.0953 - Labels_loss: 0.0796 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 985/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5655e-05\n",
      "Epoch 986/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0970 - Labels_loss: 0.0814 - sum_layer_3_loss: 1.5652e-05\n",
      "Epoch 987/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0982 - Labels_loss: 0.0825 - sum_layer_3_loss: 1.5664e-05\n",
      "Epoch 988/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 989/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5637e-05\n",
      "Epoch 990/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0962 - Labels_loss: 0.0805 - sum_layer_3_loss: 1.5663e-05\n",
      "Epoch 991/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0931 - Labels_loss: 0.0775 - sum_layer_3_loss: 1.5632e-05\n",
      "Epoch 992/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0992 - Labels_loss: 0.0836 - sum_layer_3_loss: 1.5622e-05\n",
      "Epoch 993/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0949 - Labels_loss: 0.0792 - sum_layer_3_loss: 1.5647e-05\n",
      "Epoch 994/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0981 - Labels_loss: 0.0825 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 995/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0949 - Labels_loss: 0.0792 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 996/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0950 - Labels_loss: 0.0794 - sum_layer_3_loss: 1.5656e-05\n",
      "Epoch 997/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0944 - Labels_loss: 0.0787 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 998/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0950 - Labels_loss: 0.0794 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 999/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0927 - Labels_loss: 0.0770 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 1000/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0944 - Labels_loss: 0.0787 - sum_layer_3_loss: 1.5644e-05\n",
      "Epoch 1001/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0946 - Labels_loss: 0.0790 - sum_layer_3_loss: 1.5640e-05\n",
      "Epoch 1002/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0957 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5655e-05\n",
      "Epoch 1003/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0941 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1004/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0939 - Labels_loss: 0.0782 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1005/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0983 - Labels_loss: 0.0826 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1006/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1007/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0967 - Labels_loss: 0.0811 - sum_layer_3_loss: 1.5633e-05\n",
      "Epoch 1008/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0937 - Labels_loss: 0.0781 - sum_layer_3_loss: 1.5650e-05\n",
      "Epoch 1009/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0941 - Labels_loss: 0.0785 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1010/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0948 - Labels_loss: 0.0792 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1011/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0959 - Labels_loss: 0.0803 - sum_layer_3_loss: 1.5640e-05\n",
      "Epoch 1012/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0946 - Labels_loss: 0.0789 - sum_layer_3_loss: 1.5646e-05\n",
      "Epoch 1013/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0949 - Labels_loss: 0.0793 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1014/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0958 - Labels_loss: 0.0802 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1015/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5642e-05\n",
      "Epoch 1016/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1010 - Labels_loss: 0.0854 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1017/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5637e-05\n",
      "Epoch 1018/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0970 - Labels_loss: 0.0814 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1019/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0904 - Labels_loss: 0.0748 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1020/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0959 - Labels_loss: 0.0802 - sum_layer_3_loss: 1.5672e-05\n",
      "Epoch 1021/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0971 - Labels_loss: 0.0815 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1022/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0960 - Labels_loss: 0.0803 - sum_layer_3_loss: 1.5647e-05\n",
      "Epoch 1023/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0959 - Labels_loss: 0.0803 - sum_layer_3_loss: 1.5630e-05\n",
      "Epoch 1024/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0894 - Labels_loss: 0.0738 - sum_layer_3_loss: 1.5650e-05\n",
      "Epoch 1025/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0928 - Labels_loss: 0.0772 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1026/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0927 - Labels_loss: 0.0771 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1027/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0932 - Labels_loss: 0.0776 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1028/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0949 - Labels_loss: 0.0792 - sum_layer_3_loss: 1.5649e-05\n",
      "Epoch 1029/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1030/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1031/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0929 - Labels_loss: 0.0772 - sum_layer_3_loss: 1.5652e-05\n",
      "Epoch 1032/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0761 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1033/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1034/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0757 - sum_layer_3_loss: 1.5642e-05\n",
      "Epoch 1035/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0909 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5602e-05\n",
      "Epoch 1036/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1037/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0926 - Labels_loss: 0.0770 - sum_layer_3_loss: 1.5629e-05\n",
      "Epoch 1038/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0964 - Labels_loss: 0.0807 - sum_layer_3_loss: 1.5640e-05\n",
      "Epoch 1039/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0927 - Labels_loss: 0.0771 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1040/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.1015 - Labels_loss: 0.0859 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1041/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1042/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0929 - Labels_loss: 0.0773 - sum_layer_3_loss: 1.5660e-05\n",
      "Epoch 1043/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 1044/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5622e-05\n",
      "Epoch 1045/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0917 - Labels_loss: 0.0761 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1046/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5622e-05\n",
      "Epoch 1047/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1048/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0921 - Labels_loss: 0.0765 - sum_layer_3_loss: 1.5636e-05\n",
      "Epoch 1049/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0947 - Labels_loss: 0.0791 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1050/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0937 - Labels_loss: 0.0780 - sum_layer_3_loss: 1.5677e-05\n",
      "Epoch 1051/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 1052/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0971 - Labels_loss: 0.0814 - sum_layer_3_loss: 1.5678e-05\n",
      "Epoch 1053/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0993 - Labels_loss: 0.0836 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 1054/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0956 - Labels_loss: 0.0800 - sum_layer_3_loss: 1.5622e-05\n",
      "Epoch 1055/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0930 - Labels_loss: 0.0773 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1056/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0916 - Labels_loss: 0.0760 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1057/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1058/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0920 - Labels_loss: 0.0764 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1059/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0966 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5657e-05\n",
      "Epoch 1060/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0936 - Labels_loss: 0.0780 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1061/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0954 - Labels_loss: 0.0798 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1062/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1063/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0832 - sum_layer_3_loss: 1.5629e-05\n",
      "Epoch 1064/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1065/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0921 - Labels_loss: 0.0764 - sum_layer_3_loss: 1.5648e-05\n",
      "Epoch 1066/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0931 - Labels_loss: 0.0775 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1067/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0920 - Labels_loss: 0.0764 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1068/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0932 - Labels_loss: 0.0775 - sum_layer_3_loss: 1.5649e-05\n",
      "Epoch 1069/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0757 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1070/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0938 - Labels_loss: 0.0781 - sum_layer_3_loss: 1.5634e-05\n",
      "Epoch 1071/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0909 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1072/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1073/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5618e-05\n",
      "Epoch 1074/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0930 - Labels_loss: 0.0774 - sum_layer_3_loss: 1.5667e-05\n",
      "Epoch 1075/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0900 - Labels_loss: 0.0744 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1076/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0914 - Labels_loss: 0.0758 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1077/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0946 - Labels_loss: 0.0790 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1078/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5613e-05\n",
      "Epoch 1079/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0783 - sum_layer_3_loss: 1.5659e-05\n",
      "Epoch 1080/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0965 - Labels_loss: 0.0809 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1081/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1082/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0739 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1083/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0949 - Labels_loss: 0.0793 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1084/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1085/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0761 - sum_layer_3_loss: 1.5634e-05\n",
      "Epoch 1086/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0766 - sum_layer_3_loss: 1.5637e-05\n",
      "Epoch 1087/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1088/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0905 - Labels_loss: 0.0749 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1089/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0987 - Labels_loss: 0.0831 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1090/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0757 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1091/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0959 - Labels_loss: 0.0803 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1092/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0733 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1093/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0935 - Labels_loss: 0.0779 - sum_layer_3_loss: 1.5629e-05\n",
      "Epoch 1094/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0945 - Labels_loss: 0.0789 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1095/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0923 - Labels_loss: 0.0766 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1096/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0920 - Labels_loss: 0.0763 - sum_layer_3_loss: 1.5641e-05\n",
      "Epoch 1097/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0906 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1098/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0988 - Labels_loss: 0.0832 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1099/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0908 - Labels_loss: 0.0752 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1100/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0733 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1101/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1102/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1103/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0922 - Labels_loss: 0.0766 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1104/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0919 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1105/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0894 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5649e-05\n",
      "Epoch 1106/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0958 - Labels_loss: 0.0802 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1107/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0927 - Labels_loss: 0.0771 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1108/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0917 - Labels_loss: 0.0761 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1109/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0926 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 1110/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0953 - Labels_loss: 0.0797 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1111/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0917 - Labels_loss: 0.0760 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1112/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1113/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0754 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1114/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0916 - Labels_loss: 0.0759 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1115/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1116/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0897 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1117/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0914 - Labels_loss: 0.0758 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1118/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0914 - Labels_loss: 0.0758 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1119/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0767 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1120/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0931 - Labels_loss: 0.0775 - sum_layer_3_loss: 1.5622e-05\n",
      "Epoch 1121/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5654e-05\n",
      "Epoch 1122/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1123/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1124/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0909 - Labels_loss: 0.0752 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1125/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0941 - Labels_loss: 0.0785 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1126/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1127/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0972 - Labels_loss: 0.0816 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1128/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1129/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1130/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0914 - Labels_loss: 0.0758 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1131/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0977 - Labels_loss: 0.0821 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1132/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0930 - Labels_loss: 0.0774 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1133/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1134/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0903 - Labels_loss: 0.0747 - sum_layer_3_loss: 1.5632e-05\n",
      "Epoch 1135/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1136/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0894 - Labels_loss: 0.0738 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1137/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0922 - Labels_loss: 0.0766 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1138/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1139/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0892 - Labels_loss: 0.0736 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1140/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0911 - Labels_loss: 0.0755 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1141/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0893 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1142/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1143/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0906 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1144/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0921 - Labels_loss: 0.0765 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1145/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0922 - Labels_loss: 0.0765 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1146/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1147/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1148/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0900 - Labels_loss: 0.0744 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1149/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0918 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1150/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0715 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1151/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5639e-05\n",
      "Epoch 1152/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0906 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5651e-05\n",
      "Epoch 1153/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1154/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0891 - Labels_loss: 0.0735 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1155/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0927 - Labels_loss: 0.0771 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1156/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1157/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0743 - sum_layer_3_loss: 1.5618e-05\n",
      "Epoch 1158/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1159/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0930 - Labels_loss: 0.0774 - sum_layer_3_loss: 1.5613e-05\n",
      "Epoch 1160/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1161/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0921 - Labels_loss: 0.0765 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1162/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1163/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1164/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0879 - Labels_loss: 0.0723 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1165/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1166/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1167/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1168/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0950 - Labels_loss: 0.0793 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1169/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0733 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1170/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0908 - Labels_loss: 0.0752 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1171/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1172/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1173/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1174/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0943 - Labels_loss: 0.0786 - sum_layer_3_loss: 1.5639e-05\n",
      "Epoch 1175/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0938 - Labels_loss: 0.0782 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1176/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0916 - Labels_loss: 0.0761 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1177/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0903 - Labels_loss: 0.0748 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 1178/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1179/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0912 - Labels_loss: 0.0756 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1180/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0852 - Labels_loss: 0.0696 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1181/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0893 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1182/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0912 - Labels_loss: 0.0756 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1183/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0905 - Labels_loss: 0.0749 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1184/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0731 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1185/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0870 - Labels_loss: 0.0714 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1186/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1187/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0904 - Labels_loss: 0.0748 - sum_layer_3_loss: 1.5634e-05\n",
      "Epoch 1188/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1189/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0945 - Labels_loss: 0.0789 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1190/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5618e-05\n",
      "Epoch 1191/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0754 - sum_layer_3_loss: 1.5642e-05\n",
      "Epoch 1192/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1193/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0744 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1194/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1195/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1196/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0897 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1197/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0904 - Labels_loss: 0.0748 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1198/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0897 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1199/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0864 - Labels_loss: 0.0708 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1200/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0928 - Labels_loss: 0.0772 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1201/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0929 - Labels_loss: 0.0773 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1202/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0866 - Labels_loss: 0.0710 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1203/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0952 - Labels_loss: 0.0796 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1204/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0891 - Labels_loss: 0.0735 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1205/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0938 - Labels_loss: 0.0782 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1206/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5658e-05\n",
      "Epoch 1207/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0905 - Labels_loss: 0.0749 - sum_layer_3_loss: 1.5630e-05\n",
      "Epoch 1208/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1209/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0919 - Labels_loss: 0.0763 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1210/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0881 - Labels_loss: 0.0725 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1211/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1212/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0879 - Labels_loss: 0.0723 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1213/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0892 - Labels_loss: 0.0736 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1214/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1215/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0757 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1216/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0925 - Labels_loss: 0.0769 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1217/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0743 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1218/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1219/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0895 - Labels_loss: 0.0738 - sum_layer_3_loss: 1.5613e-05\n",
      "Epoch 1220/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0743 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1221/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0878 - Labels_loss: 0.0722 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1222/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0914 - Labels_loss: 0.0758 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1223/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5636e-05\n",
      "Epoch 1224/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1225/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1226/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0933 - Labels_loss: 0.0777 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1227/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0891 - Labels_loss: 0.0735 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1228/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0903 - Labels_loss: 0.0747 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1229/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0911 - Labels_loss: 0.0754 - sum_layer_3_loss: 1.5628e-05\n",
      "Epoch 1230/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0859 - Labels_loss: 0.0703 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1231/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5650e-05\n",
      "Epoch 1232/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0929 - Labels_loss: 0.0773 - sum_layer_3_loss: 1.5633e-05\n",
      "Epoch 1233/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0906 - Labels_loss: 0.0750 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1234/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0893 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1235/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0867 - Labels_loss: 0.0711 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1236/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0891 - Labels_loss: 0.0735 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1237/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1238/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0883 - Labels_loss: 0.0727 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1239/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1240/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1241/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1242/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1243/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0871 - Labels_loss: 0.0715 - sum_layer_3_loss: 1.5628e-05\n",
      "Epoch 1244/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0733 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1245/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0878 - Labels_loss: 0.0722 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1246/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1247/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1248/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0885 - Labels_loss: 0.0729 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1249/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0893 - Labels_loss: 0.0736 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1250/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0867 - Labels_loss: 0.0710 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1251/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1252/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0928 - Labels_loss: 0.0772 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1253/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0881 - Labels_loss: 0.0725 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1254/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0916 - Labels_loss: 0.0760 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1255/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0895 - Labels_loss: 0.0739 - sum_layer_3_loss: 1.5659e-05\n",
      "Epoch 1256/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0908 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5642e-05\n",
      "Epoch 1257/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1258/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1259/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0902 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1260/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1261/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0877 - Labels_loss: 0.0721 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1262/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1263/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0884 - Labels_loss: 0.0728 - sum_layer_3_loss: 1.5628e-05\n",
      "Epoch 1264/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1265/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1266/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1267/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0871 - Labels_loss: 0.0715 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1268/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5583e-05\n",
      "Epoch 1269/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0877 - Labels_loss: 0.0721 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1270/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1271/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0866 - Labels_loss: 0.0710 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1272/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1273/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0876 - Labels_loss: 0.0720 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 1274/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1275/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0744 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1276/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1277/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0860 - Labels_loss: 0.0704 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1278/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0917 - Labels_loss: 0.0762 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1279/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1280/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1281/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0892 - Labels_loss: 0.0736 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1282/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0907 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1283/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0884 - Labels_loss: 0.0728 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1284/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0733 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1285/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0905 - Labels_loss: 0.0749 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1286/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0888 - Labels_loss: 0.0732 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1287/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1288/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0926 - Labels_loss: 0.0770 - sum_layer_3_loss: 1.5625e-05\n",
      "Epoch 1289/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1290/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0926 - Labels_loss: 0.0770 - sum_layer_3_loss: 1.5602e-05\n",
      "Epoch 1291/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0879 - Labels_loss: 0.0723 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1292/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0849 - Labels_loss: 0.0694 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1293/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1294/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0845 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1295/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0884 - Labels_loss: 0.0728 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1296/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1297/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0878 - Labels_loss: 0.0722 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1298/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5628e-05\n",
      "Epoch 1299/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0881 - Labels_loss: 0.0724 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1300/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0895 - Labels_loss: 0.0739 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1301/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0890 - Labels_loss: 0.0734 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1302/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5602e-05\n",
      "Epoch 1303/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0848 - Labels_loss: 0.0693 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1304/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1305/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0878 - Labels_loss: 0.0722 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1306/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1307/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0897 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1308/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1309/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0845 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1310/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0712 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1311/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5630e-05\n",
      "Epoch 1312/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0868 - Labels_loss: 0.0711 - sum_layer_3_loss: 1.5652e-05\n",
      "Epoch 1313/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0885 - Labels_loss: 0.0729 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1314/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1315/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0924 - Labels_loss: 0.0768 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1316/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0940 - Labels_loss: 0.0784 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1317/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0895 - Labels_loss: 0.0739 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1318/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1319/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1320/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0847 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1321/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0868 - Labels_loss: 0.0712 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1322/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1323/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0947 - Labels_loss: 0.0791 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1324/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0839 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1325/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0841 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1326/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1327/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0709 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1328/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1329/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1330/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1331/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0876 - Labels_loss: 0.0720 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1332/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0910 - Labels_loss: 0.0754 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1333/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0841 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5623e-05\n",
      "Epoch 1334/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0885 - Labels_loss: 0.0729 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1335/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0845 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1336/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0893 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1337/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0831 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1338/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1339/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0881 - Labels_loss: 0.0725 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1340/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1341/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0845 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1342/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0894 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5675e-05\n",
      "Epoch 1343/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1344/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1345/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0859 - Labels_loss: 0.0703 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1346/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0893 - Labels_loss: 0.0737 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1347/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1348/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0831 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1349/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0943 - Labels_loss: 0.0787 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 1350/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1351/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1352/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0868 - Labels_loss: 0.0712 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1353/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1354/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0836 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1355/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0709 - sum_layer_3_loss: 1.5616e-05\n",
      "Epoch 1356/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0866 - Labels_loss: 0.0711 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1357/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0897 - Labels_loss: 0.0741 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1358/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5630e-05\n",
      "Epoch 1359/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1360/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0884 - Labels_loss: 0.0728 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1361/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1362/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0889 - Labels_loss: 0.0734 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1363/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5628e-05\n",
      "Epoch 1364/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1365/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0887 - Labels_loss: 0.0731 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1366/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1367/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1368/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0757 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1369/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0877 - Labels_loss: 0.0721 - sum_layer_3_loss: 1.5613e-05\n",
      "Epoch 1370/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0862 - Labels_loss: 0.0706 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1371/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1372/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1373/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0864 - Labels_loss: 0.0708 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1374/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1375/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1376/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1377/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1378/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0708 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1379/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0868 - Labels_loss: 0.0712 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1380/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0890 - Labels_loss: 0.0734 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1381/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0847 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1382/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0852 - Labels_loss: 0.0696 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1383/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0848 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1384/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0839 - Labels_loss: 0.0683 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1385/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1386/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1387/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1388/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0860 - Labels_loss: 0.0704 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1389/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1390/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1391/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0840 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1392/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0909 - Labels_loss: 0.0753 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1393/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1394/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0843 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1395/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0745 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1396/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0857 - Labels_loss: 0.0701 - sum_layer_3_loss: 1.5602e-05\n",
      "Epoch 1397/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0830 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5583e-05\n",
      "Epoch 1398/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0885 - Labels_loss: 0.0729 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1399/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1400/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1401/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1402/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1403/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0913 - Labels_loss: 0.0756 - sum_layer_3_loss: 1.5620e-05\n",
      "Epoch 1404/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0820 - Labels_loss: 0.0664 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1405/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0699 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1406/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0840 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1407/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0870 - Labels_loss: 0.0714 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1408/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0961 - Labels_loss: 0.0805 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1409/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1410/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1411/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0847 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1412/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1413/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0845 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1414/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5627e-05\n",
      "Epoch 1415/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0833 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1416/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0870 - Labels_loss: 0.0714 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1417/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0891 - Labels_loss: 0.0735 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1418/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1419/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0830 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1420/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1421/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0826 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1422/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0849 - Labels_loss: 0.0693 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1423/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1424/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1425/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0872 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5621e-05\n",
      "Epoch 1426/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1427/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1428/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0859 - Labels_loss: 0.0703 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1429/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0875 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1430/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0848 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1431/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0850 - Labels_loss: 0.0694 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1432/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1433/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1434/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0899 - Labels_loss: 0.0743 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1435/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0840 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1436/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1437/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1438/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0827 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1439/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1440/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1441/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0901 - Labels_loss: 0.0746 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1442/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0710 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1443/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5614e-05\n",
      "Epoch 1444/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0827 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1445/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0855 - Labels_loss: 0.0699 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1446/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0836 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1447/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1448/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1449/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0819 - Labels_loss: 0.0664 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1450/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0709 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1451/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0848 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1452/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1453/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1454/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1455/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0827 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1456/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1457/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1458/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5624e-05\n",
      "Epoch 1459/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0866 - Labels_loss: 0.0709 - sum_layer_3_loss: 1.5631e-05\n",
      "Epoch 1460/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1461/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0874 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1462/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0881 - Labels_loss: 0.0725 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1463/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0851 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1464/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1465/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0855 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1466/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0867 - Labels_loss: 0.0712 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1467/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1468/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0836 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1469/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1470/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1471/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1472/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0862 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1473/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0855 - Labels_loss: 0.0699 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1474/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1475/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0839 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1476/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1477/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1478/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0838 - Labels_loss: 0.0682 - sum_layer_3_loss: 1.5653e-05\n",
      "Epoch 1479/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1480/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0852 - Labels_loss: 0.0696 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1481/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0836 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1482/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0830 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1483/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1484/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1485/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1486/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1487/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1488/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1489/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1490/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1491/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5606e-05\n",
      "Epoch 1492/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0829 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1493/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1494/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1495/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1496/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0887 - Labels_loss: 0.0731 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1497/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1498/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1499/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1500/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0827 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1501/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0850 - Labels_loss: 0.0694 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1502/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0827 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5607e-05\n",
      "Epoch 1503/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0841 - Labels_loss: 0.0686 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1504/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0830 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1505/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1506/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0841 - Labels_loss: 0.0686 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1507/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0858 - Labels_loss: 0.0702 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1508/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0862 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1509/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1510/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0709 - sum_layer_3_loss: 1.5643e-05\n",
      "Epoch 1511/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0855 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1512/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0863 - Labels_loss: 0.0707 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1513/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0905 - Labels_loss: 0.0749 - sum_layer_3_loss: 1.5638e-05\n",
      "Epoch 1514/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1515/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0856 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1516/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0837 - Labels_loss: 0.0682 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1517/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1518/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1519/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0928 - Labels_loss: 0.0771 - sum_layer_3_loss: 1.5629e-05\n",
      "Epoch 1520/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0865 - Labels_loss: 0.0710 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1521/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1522/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0847 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1523/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0808 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1524/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0814 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1525/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0842 - Labels_loss: 0.0686 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1526/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0847 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1527/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1528/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0835 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1529/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0855 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1530/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1531/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1532/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0817 - Labels_loss: 0.0662 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1533/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5617e-05\n",
      "Epoch 1534/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1535/3000\n",
      "242/242 [==============================] - 5s 19ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5583e-05\n",
      "Epoch 1536/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0898 - Labels_loss: 0.0742 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1537/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0826 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1538/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0870 - Labels_loss: 0.0714 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1539/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1540/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1541/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0838 - Labels_loss: 0.0682 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1542/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0829 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1543/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5595e-05\n",
      "Epoch 1544/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0869 - Labels_loss: 0.0713 - sum_layer_3_loss: 1.5611e-05\n",
      "Epoch 1545/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1546/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0838 - Labels_loss: 0.0682 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1547/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 1548/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1549/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0896 - Labels_loss: 0.0740 - sum_layer_3_loss: 1.5609e-05\n",
      "Epoch 1550/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0856 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1551/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1552/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1553/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0662 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1554/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1555/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1556/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0824 - Labels_loss: 0.0669 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1557/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0853 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1558/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1559/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0831 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1560/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1561/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0829 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1562/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1563/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1564/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0831 - Labels_loss: 0.0675 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1565/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0826 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1566/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0840 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1567/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1568/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0660 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1569/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1570/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0835 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1571/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0839 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1572/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0799 - Labels_loss: 0.0643 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1573/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0864 - Labels_loss: 0.0708 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1574/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1575/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0660 - sum_layer_3_loss: 1.5626e-05\n",
      "Epoch 1576/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0885 - Labels_loss: 0.0729 - sum_layer_3_loss: 1.5599e-05\n",
      "Epoch 1577/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0840 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1578/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1579/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0841 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5619e-05\n",
      "Epoch 1580/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1581/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0855 - Labels_loss: 0.0699 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1582/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1583/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0837 - Labels_loss: 0.0681 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1584/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1585/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1586/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0871 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1587/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5608e-05\n",
      "Epoch 1588/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1589/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0845 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1590/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0660 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 1591/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0820 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1592/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0856 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1593/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1594/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0829 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1595/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1596/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1597/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0830 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1598/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0839 - Labels_loss: 0.0683 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1599/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1600/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1601/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0832 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1602/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1603/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0827 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 1604/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1605/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0870 - Labels_loss: 0.0714 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1606/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0824 - Labels_loss: 0.0669 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1607/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1608/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5603e-05\n",
      "Epoch 1609/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0843 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1610/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1611/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1612/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0809 - Labels_loss: 0.0653 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1613/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0662 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1614/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1615/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1616/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1617/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1618/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 1619/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1620/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0871 - Labels_loss: 0.0716 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1621/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1622/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0830 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5635e-05\n",
      "Epoch 1623/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1624/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0882 - Labels_loss: 0.0726 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1625/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0797 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1626/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0808 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1627/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1628/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1629/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1630/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1631/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0846 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1632/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0835 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1633/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1634/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1635/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1636/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5648e-05\n",
      "Epoch 1637/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1638/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1639/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1640/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1641/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1642/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1643/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1644/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1645/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0813 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 1646/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1647/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0908 - Labels_loss: 0.0751 - sum_layer_3_loss: 1.5639e-05\n",
      "Epoch 1648/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0810 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1649/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1650/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0662 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1651/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1652/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1653/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1654/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0808 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1655/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1656/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1657/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0877 - Labels_loss: 0.0721 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1658/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0814 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1659/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1660/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1661/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1662/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5587e-05\n",
      "Epoch 1663/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0797 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1664/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1665/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1666/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0826 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1667/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0841 - Labels_loss: 0.0686 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1668/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1669/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5597e-05\n",
      "Epoch 1670/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1671/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0842 - Labels_loss: 0.0686 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1672/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 1673/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0826 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1674/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1675/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0808 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1676/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1677/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0807 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1678/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0834 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1679/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0830 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5601e-05\n",
      "Epoch 1680/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0829 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1681/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1682/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1683/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0660 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1684/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1685/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1686/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0664 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1687/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1688/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1689/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1690/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 1691/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 1692/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1693/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0799 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1694/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0802 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1695/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0823 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 1696/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 1697/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5598e-05\n",
      "Epoch 1698/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1699/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1700/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1701/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1702/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1703/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1704/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1705/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0852 - Labels_loss: 0.0696 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1706/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0859 - Labels_loss: 0.0703 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1707/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0673 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1708/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5584e-05\n",
      "Epoch 1709/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5604e-05\n",
      "Epoch 1710/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0786 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1711/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1712/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 1713/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1714/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0848 - Labels_loss: 0.0693 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1715/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1716/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1717/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1718/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1719/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1720/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1721/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 1722/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1723/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 1724/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0826 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1725/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0870 - Labels_loss: 0.0715 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1726/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1727/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1728/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0814 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1729/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1730/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 1731/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1732/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1733/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5583e-05\n",
      "Epoch 1734/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 1735/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1736/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1737/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0879 - Labels_loss: 0.0723 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1738/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5600e-05\n",
      "Epoch 1739/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 1740/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0802 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1741/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1742/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1743/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5612e-05\n",
      "Epoch 1744/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5596e-05\n",
      "Epoch 1745/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1746/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0855 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1747/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0664 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1748/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0839 - Labels_loss: 0.0683 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1749/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1750/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1751/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1752/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 1753/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1754/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 1755/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5582e-05\n",
      "Epoch 1756/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0755 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1757/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1758/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1759/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 1760/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1761/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1762/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0799 - Labels_loss: 0.0643 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 1763/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5585e-05\n",
      "Epoch 1764/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0802 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1765/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1766/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0828 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1767/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1768/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 1769/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0811 - Labels_loss: 0.0655 - sum_layer_3_loss: 1.5610e-05\n",
      "Epoch 1770/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5615e-05\n",
      "Epoch 1771/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0807 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1772/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1773/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1774/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 1775/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0861 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1776/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1777/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1778/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0797 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1779/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5583e-05\n",
      "Epoch 1780/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1781/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 1782/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1783/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1784/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0781 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 1785/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0833 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1786/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1787/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1788/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1789/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1790/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1791/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1792/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1793/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0854 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1794/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0836 - Labels_loss: 0.0680 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1795/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1796/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5593e-05\n",
      "Epoch 1797/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0827 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1798/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 1799/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1800/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1801/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1802/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0820 - Labels_loss: 0.0665 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 1803/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1804/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0847 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1805/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0777 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1806/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0856 - Labels_loss: 0.0700 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1807/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0852 - Labels_loss: 0.0697 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 1808/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1809/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1810/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1811/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0841 - Labels_loss: 0.0685 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 1812/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1813/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1814/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1815/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1816/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1817/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5589e-05\n",
      "Epoch 1818/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1819/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 1820/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0874 - Labels_loss: 0.0719 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1821/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 1822/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0848 - Labels_loss: 0.0692 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1823/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1824/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1825/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 1826/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1827/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1828/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1829/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0840 - Labels_loss: 0.0684 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1830/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1831/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0812 - Labels_loss: 0.0657 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1832/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0827 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5590e-05\n",
      "Epoch 1833/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1834/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1835/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1836/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1837/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1838/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 1839/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0813 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 1840/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0797 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5578e-05\n",
      "Epoch 1841/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0811 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1842/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1843/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1844/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1845/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1846/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1847/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0814 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1848/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5605e-05\n",
      "Epoch 1849/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1850/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5586e-05\n",
      "Epoch 1851/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0777 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1852/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1853/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1854/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0784 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1855/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1856/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0787 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1857/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1858/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 1859/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 1860/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0802 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1861/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0832 - Labels_loss: 0.0676 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1862/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0759 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1863/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1864/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 1865/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 1866/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0799 - Labels_loss: 0.0643 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1867/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 1868/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0809 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1869/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 1870/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0781 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 1871/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 1872/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1873/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1874/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0812 - Labels_loss: 0.0657 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1875/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1876/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1877/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 1878/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0811 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1879/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0763 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1880/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0808 - Labels_loss: 0.0652 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 1881/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0809 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1882/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0815 - Labels_loss: 0.0659 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1883/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0809 - Labels_loss: 0.0653 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1884/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 1885/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1886/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1887/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 1888/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 1889/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 1890/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0801 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1891/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1892/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0761 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1893/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1894/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1895/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 1896/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0873 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1897/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 1898/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0787 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 1899/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1900/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1901/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 1902/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 1903/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1904/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5575e-05\n",
      "Epoch 1905/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0874 - Labels_loss: 0.0718 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1906/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1907/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1908/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 1909/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 1910/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1911/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5592e-05\n",
      "Epoch 1912/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1913/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 1914/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1915/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1916/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0830 - Labels_loss: 0.0674 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1917/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0808 - Labels_loss: 0.0653 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1918/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 1919/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1920/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 1921/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 1922/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0844 - Labels_loss: 0.0688 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1923/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5570e-05\n",
      "Epoch 1924/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1925/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1926/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 1927/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0775 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 1928/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 1929/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 1930/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0787 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1931/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1932/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5588e-05\n",
      "Epoch 1933/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 1934/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1935/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0801 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 1936/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0810 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1937/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 1938/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 1939/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 1940/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 1941/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1942/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 1943/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0818 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1944/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 1945/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 1946/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0810 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 1947/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0824 - Labels_loss: 0.0669 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1948/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1949/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 1950/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0726 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1951/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1952/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1953/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0650 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 1954/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1955/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 1956/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0787 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 1957/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 1958/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 1959/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1960/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 1961/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0812 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1962/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 1963/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0640 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 1964/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1965/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 1966/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5574e-05\n",
      "Epoch 1967/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0767 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 1968/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0813 - Labels_loss: 0.0657 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1969/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0735 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 1970/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 1971/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 1972/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 1973/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 1974/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 1975/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 1976/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 1977/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1978/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 1979/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0786 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1980/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1981/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 1982/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 1983/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 1984/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 1985/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0817 - Labels_loss: 0.0662 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 1986/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0791 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 1987/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1988/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 1989/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 1990/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 1991/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5594e-05\n",
      "Epoch 1992/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 1993/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 1994/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 1995/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 1996/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 1997/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 1998/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 1999/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2000/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2001/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0793 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2002/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0636 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2003/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0773 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 2004/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2005/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2006/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2007/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2008/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2009/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2010/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 2011/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0788 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 2012/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0765 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2013/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2014/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 2015/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0786 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2016/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2017/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0803 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2018/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2019/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2020/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2021/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0805 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 2022/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2023/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0796 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2024/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2025/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2026/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2027/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2028/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0717 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 2029/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0838 - Labels_loss: 0.0683 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2030/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2031/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2032/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 2033/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2034/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2035/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2036/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2037/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2038/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2039/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2040/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2041/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2042/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2043/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0759 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2044/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0846 - Labels_loss: 0.0690 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 2045/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0785 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5565e-05\n",
      "Epoch 2046/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2047/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2048/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2049/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0792 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2050/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2051/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2052/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0752 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 2053/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0833 - Labels_loss: 0.0677 - sum_layer_3_loss: 1.5581e-05\n",
      "Epoch 2054/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0762 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2055/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2056/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0746 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2057/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2058/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2059/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2060/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2061/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0643 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2062/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2063/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2064/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2065/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2066/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2067/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0810 - Labels_loss: 0.0654 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2068/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0804 - Labels_loss: 0.0649 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2069/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0796 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 2070/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2071/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2072/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0643 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2073/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2074/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 2075/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2076/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2077/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5569e-05\n",
      "Epoch 2078/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0807 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2079/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2080/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 2081/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0792 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5567e-05\n",
      "Epoch 2082/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 2083/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0802 - Labels_loss: 0.0646 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2084/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5572e-05\n",
      "Epoch 2085/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2086/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0758 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2087/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0781 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2088/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2089/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2090/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2091/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0768 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2092/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2093/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2094/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2095/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2096/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2097/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2098/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 2099/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0797 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2100/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2101/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0749 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2102/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2103/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2104/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2105/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0781 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2106/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2107/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0795 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5573e-05\n",
      "Epoch 2108/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2109/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2110/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 2111/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2112/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2113/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2114/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2115/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2116/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0747 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2117/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 2118/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0886 - Labels_loss: 0.0730 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2119/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2120/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2121/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 2122/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2123/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2124/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2125/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2126/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2127/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0786 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2128/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0783 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2129/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0784 - Labels_loss: 0.0628 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2130/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2131/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2132/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2133/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2134/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5515e-05\n",
      "Epoch 2135/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2136/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2137/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2138/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 2139/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 2140/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0620 - sum_layer_3_loss: 1.5560e-05\n",
      "Epoch 2141/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0813 - Labels_loss: 0.0658 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2142/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0932 - Labels_loss: 0.0776 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2143/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2144/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 2145/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0816 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5576e-05\n",
      "Epoch 2146/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0811 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 2147/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2148/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2149/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2150/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2151/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2152/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2153/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 2154/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2155/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2156/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5577e-05\n",
      "Epoch 2157/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0775 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 2158/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2159/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2160/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2161/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2162/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2163/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 2164/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2165/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2166/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2167/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2168/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2169/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2170/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2171/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2172/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2173/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0808 - Labels_loss: 0.0653 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 2174/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2175/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2176/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2177/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0638 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2178/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2179/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0804 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2180/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2181/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 2182/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2183/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0819 - Labels_loss: 0.0663 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2184/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2185/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2186/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2187/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0857 - Labels_loss: 0.0701 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 2188/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2189/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2190/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2191/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2192/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5553e-05\n",
      "Epoch 2193/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2194/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0800 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 2195/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2196/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 2197/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2198/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2199/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2200/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 2201/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2202/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 2203/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5550e-05\n",
      "Epoch 2204/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2205/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5564e-05\n",
      "Epoch 2206/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 2207/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2208/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0789 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2209/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 2210/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2211/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2212/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0798 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2213/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2214/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2215/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2216/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2217/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2218/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2219/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 2220/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2221/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2222/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2223/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0789 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2224/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5558e-05\n",
      "Epoch 2225/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0775 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2226/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 2227/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2228/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2229/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2230/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2231/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2232/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2233/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2234/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2235/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 2236/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2237/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2238/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2239/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2240/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2241/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2242/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2243/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2244/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2245/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2246/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0860 - Labels_loss: 0.0704 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2247/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0754 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 2248/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2249/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2250/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0793 - Labels_loss: 0.0637 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2251/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0762 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2252/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2253/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2254/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0713 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2255/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2256/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5559e-05\n",
      "Epoch 2257/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2258/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2259/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2260/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2261/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2262/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2263/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 2264/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2265/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2266/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2267/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2268/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2269/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2270/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0763 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5547e-05\n",
      "Epoch 2271/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0734 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2272/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2273/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2274/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2275/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2276/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2277/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5580e-05\n",
      "Epoch 2278/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0778 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2279/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2280/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2281/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0791 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5579e-05\n",
      "Epoch 2282/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0716 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2283/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0834 - Labels_loss: 0.0679 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2284/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2285/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2286/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0781 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2287/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2288/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2289/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0860 - Labels_loss: 0.0705 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2290/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2291/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0794 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2292/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2293/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2294/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2295/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0787 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2296/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5563e-05\n",
      "Epoch 2297/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0827 - Labels_loss: 0.0672 - sum_layer_3_loss: 1.5561e-05\n",
      "Epoch 2298/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2299/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2300/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2301/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2302/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2303/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2304/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2305/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2306/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0817 - Labels_loss: 0.0661 - sum_layer_3_loss: 1.5568e-05\n",
      "Epoch 2307/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2308/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2309/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0876 - Labels_loss: 0.0720 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2310/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2311/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2312/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2313/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2314/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2315/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2316/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2317/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2318/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2319/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2320/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2321/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2322/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2323/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2324/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2325/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5541e-05\n",
      "Epoch 2326/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2327/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0763 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2328/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2329/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2330/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2331/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2332/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2333/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0764 - Labels_loss: 0.0609 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2334/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2335/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2336/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0781 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2337/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2338/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2339/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2340/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2341/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2342/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0802 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2343/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2344/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2345/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2346/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2347/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2348/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2349/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2350/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5533e-05\n",
      "Epoch 2351/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2352/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2353/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2354/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2355/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2356/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2357/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2358/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0786 - Labels_loss: 0.0631 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2359/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2360/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2361/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0833 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2362/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0872 - Labels_loss: 0.0717 - sum_layer_3_loss: 1.5555e-05\n",
      "Epoch 2363/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2364/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2365/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2366/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2367/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2368/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0724 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2369/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0778 - Labels_loss: 0.0623 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2370/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0758 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2371/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5591e-05\n",
      "Epoch 2372/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2373/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0782 - Labels_loss: 0.0626 - sum_layer_3_loss: 1.5546e-05\n",
      "Epoch 2374/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2375/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2376/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2377/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2378/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0811 - Labels_loss: 0.0655 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2379/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2380/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2381/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0717 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5571e-05\n",
      "Epoch 2382/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2383/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2384/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0717 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2385/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2386/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2387/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2388/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2389/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2390/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2391/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2392/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2393/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5534e-05\n",
      "Epoch 2394/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0850 - Labels_loss: 0.0695 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2395/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0790 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5557e-05\n",
      "Epoch 2396/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2397/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2398/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2399/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2400/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2401/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2402/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0794 - Labels_loss: 0.0639 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2403/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2404/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0705 - Labels_loss: 0.0550 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2405/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2406/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0728 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2407/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2408/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0790 - Labels_loss: 0.0635 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2409/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0821 - Labels_loss: 0.0666 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2410/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2411/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0797 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5566e-05\n",
      "Epoch 2412/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0722 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2413/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2414/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2415/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0707 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2416/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5540e-05\n",
      "Epoch 2417/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2418/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2419/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2420/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0703 - Labels_loss: 0.0548 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2421/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2422/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2423/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2424/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2425/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0740 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2426/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2427/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2428/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2429/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2430/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2431/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0770 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2432/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2433/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2434/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2435/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2436/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0788 - Labels_loss: 0.0633 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2437/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2438/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2439/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2440/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2441/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0697 - Labels_loss: 0.0542 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2442/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2443/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2444/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2445/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2446/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2447/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0748 - Labels_loss: 0.0593 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2448/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2449/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2450/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0826 - Labels_loss: 0.0671 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2451/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0764 - Labels_loss: 0.0608 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2452/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2453/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2454/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2455/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2456/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2457/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2458/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2459/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2460/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2461/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2462/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2463/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0802 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2464/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0712 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5562e-05\n",
      "Epoch 2465/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2466/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2467/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2468/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2469/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2470/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0849 - Labels_loss: 0.0694 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2471/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2472/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2473/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0780 - Labels_loss: 0.0625 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2474/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0713 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2475/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0778 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2476/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2477/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0707 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2478/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2479/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2480/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5536e-05\n",
      "Epoch 2481/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2482/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2483/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2484/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2485/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2486/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2487/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2488/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0702 - Labels_loss: 0.0547 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2489/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2490/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2491/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2492/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5542e-05\n",
      "Epoch 2493/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2494/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2495/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2496/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0713 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2497/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2498/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2499/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0772 - Labels_loss: 0.0617 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2500/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2501/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0784 - Labels_loss: 0.0629 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2502/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2503/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2504/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5543e-05\n",
      "Epoch 2505/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0691 - Labels_loss: 0.0536 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2506/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2507/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2508/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0776 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2509/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2510/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2511/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2512/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2513/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0717 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2514/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0782 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2515/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2516/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2517/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2518/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0693 - Labels_loss: 0.0538 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2519/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2520/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0822 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2521/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2522/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2523/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2524/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2525/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0705 - Labels_loss: 0.0550 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2526/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2527/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2528/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2529/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5454e-05\n",
      "Epoch 2530/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2531/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2532/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2533/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0844 - Labels_loss: 0.0689 - sum_layer_3_loss: 1.5544e-05\n",
      "Epoch 2534/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2535/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0754 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2536/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2537/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2538/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0713 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2539/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0811 - Labels_loss: 0.0656 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2540/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2541/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2542/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2543/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0688 - Labels_loss: 0.0533 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2544/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2545/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0705 - Labels_loss: 0.0550 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2546/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0768 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2547/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2548/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2549/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0810 - Labels_loss: 0.0655 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2550/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0687 - Labels_loss: 0.0532 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2551/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0789 - Labels_loss: 0.0634 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2552/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2553/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2554/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2555/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0718 - Labels_loss: 0.0563 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2556/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2557/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0803 - Labels_loss: 0.0647 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2558/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2559/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2560/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2561/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0718 - Labels_loss: 0.0563 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2562/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2563/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2564/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0846 - Labels_loss: 0.0691 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2565/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5548e-05\n",
      "Epoch 2566/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5551e-05\n",
      "Epoch 2567/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2568/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2569/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2570/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2571/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2572/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2573/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2574/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2575/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0702 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5556e-05\n",
      "Epoch 2576/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2577/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0684 - Labels_loss: 0.0529 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2578/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0703 - Labels_loss: 0.0548 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2579/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2580/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0717 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2581/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2582/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2583/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2584/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2585/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5468e-05\n",
      "Epoch 2586/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2587/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2588/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2589/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2590/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5537e-05\n",
      "Epoch 2591/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2592/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2593/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5525e-05\n",
      "Epoch 2594/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2595/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0773 - Labels_loss: 0.0618 - sum_layer_3_loss: 1.5529e-05\n",
      "Epoch 2596/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2597/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2598/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2599/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2600/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2601/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0707 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2602/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2603/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2604/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2605/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2606/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0938 - Labels_loss: 0.0782 - sum_layer_3_loss: 1.5552e-05\n",
      "Epoch 2607/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0738 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2608/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5530e-05\n",
      "Epoch 2609/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0705 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5526e-05\n",
      "Epoch 2610/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0695 - Labels_loss: 0.0540 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2611/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0762 - Labels_loss: 0.0607 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2612/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0800 - Labels_loss: 0.0645 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2613/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0797 - Labels_loss: 0.0641 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2614/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2615/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0553 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2616/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0697 - Labels_loss: 0.0542 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2617/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2618/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2619/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2620/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2621/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5545e-05\n",
      "Epoch 2622/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2623/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2624/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2625/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0689 - Labels_loss: 0.0534 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2626/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2627/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0779 - Labels_loss: 0.0624 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2628/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2629/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2630/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2631/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2632/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2633/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2634/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2635/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2636/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0692 - Labels_loss: 0.0537 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2637/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2638/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2639/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2640/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0679 - Labels_loss: 0.0524 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2641/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2642/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2643/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0713 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2644/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2645/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2646/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2647/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5554e-05\n",
      "Epoch 2648/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5538e-05\n",
      "Epoch 2649/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2650/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0749 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2651/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2652/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2653/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2654/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2655/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2656/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0698 - Labels_loss: 0.0543 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2657/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2658/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2659/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0747 - Labels_loss: 0.0592 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2660/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2661/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2662/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2663/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0833 - Labels_loss: 0.0678 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2664/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2665/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2666/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0737 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2667/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2668/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0722 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5524e-05\n",
      "Epoch 2669/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2670/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0689 - Labels_loss: 0.0534 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2671/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2672/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2673/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2674/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0771 - Labels_loss: 0.0616 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2675/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5515e-05\n",
      "Epoch 2676/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2677/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2678/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2679/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5465e-05\n",
      "Epoch 2680/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0853 - Labels_loss: 0.0698 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2681/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2682/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2683/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2684/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0693 - Labels_loss: 0.0539 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2685/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2686/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2687/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2688/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2689/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0718 - Labels_loss: 0.0563 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2690/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0698 - Labels_loss: 0.0543 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2691/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2692/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5516e-05\n",
      "Epoch 2693/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2694/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5515e-05\n",
      "Epoch 2695/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2696/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2697/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2698/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0755 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2699/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2700/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2701/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0758 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2702/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0770 - Labels_loss: 0.0615 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2703/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0686 - Labels_loss: 0.0531 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2704/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2705/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2706/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2707/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2708/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2709/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2710/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2711/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2712/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0705 - Labels_loss: 0.0550 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2713/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2714/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0690 - Labels_loss: 0.0536 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2715/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2716/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0696 - Labels_loss: 0.0541 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2717/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2718/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2719/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0758 - Labels_loss: 0.0603 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2720/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2721/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0696 - Labels_loss: 0.0541 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2722/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0572 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2723/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2724/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0803 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2725/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2726/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0749 - Labels_loss: 0.0594 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2727/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2728/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0747 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2729/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2730/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0688 - Labels_loss: 0.0533 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2731/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0688 - Labels_loss: 0.0533 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2732/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0680 - Labels_loss: 0.0526 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2733/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0798 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2734/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2735/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2736/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2737/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0696 - Labels_loss: 0.0541 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2738/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5469e-05\n",
      "Epoch 2739/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2740/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2741/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2742/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2743/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0803 - Labels_loss: 0.0648 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2744/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2745/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0678 - Labels_loss: 0.0523 - sum_layer_3_loss: 1.5469e-05\n",
      "Epoch 2746/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2747/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5505e-05\n",
      "Epoch 2748/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0736 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2749/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2750/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0553 - sum_layer_3_loss: 1.5531e-05\n",
      "Epoch 2751/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5467e-05\n",
      "Epoch 2752/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2753/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0741 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2754/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2755/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2756/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2757/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2758/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2759/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2760/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2761/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0718 - Labels_loss: 0.0563 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2762/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2763/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2764/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2765/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2766/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2767/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5511e-05\n",
      "Epoch 2768/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2769/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0825 - Labels_loss: 0.0670 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2770/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5452e-05\n",
      "Epoch 2771/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5468e-05\n",
      "Epoch 2772/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2773/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2774/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0743 - Labels_loss: 0.0588 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2775/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2776/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0766 - Labels_loss: 0.0611 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2777/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0683 - Labels_loss: 0.0528 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2778/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0681 - Labels_loss: 0.0526 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2779/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2780/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0702 - Labels_loss: 0.0548 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2781/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2782/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0736 - Labels_loss: 0.0581 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2783/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0689 - Labels_loss: 0.0535 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2784/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2785/3000\n",
      "242/242 [==============================] - 5s 22ms/step - loss: 0.0724 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5549e-05\n",
      "Epoch 2786/3000\n",
      "242/242 [==============================] - 5s 22ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2787/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0600 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2788/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0760 - Labels_loss: 0.0605 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2789/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2790/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2791/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2792/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0691 - Labels_loss: 0.0536 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2793/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0822 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2794/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0837 - Labels_loss: 0.0682 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2795/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2796/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2797/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2798/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2799/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2800/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0742 - Labels_loss: 0.0587 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2801/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0695 - Labels_loss: 0.0540 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2802/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2803/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2804/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0586 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2805/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0751 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2806/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0765 - Labels_loss: 0.0610 - sum_layer_3_loss: 1.5523e-05\n",
      "Epoch 2807/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5460e-05\n",
      "Epoch 2808/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2809/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0696 - Labels_loss: 0.0541 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2810/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2811/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5528e-05\n",
      "Epoch 2812/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2813/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0842 - Labels_loss: 0.0687 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2814/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2815/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0690 - Labels_loss: 0.0535 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2816/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0674 - Labels_loss: 0.0519 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2817/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2818/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0711 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2819/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2820/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2821/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2822/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2823/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2824/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0759 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2825/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0683 - Labels_loss: 0.0528 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2826/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0690 - Labels_loss: 0.0535 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2827/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2828/3000\n",
      "242/242 [==============================] - 5s 22ms/step - loss: 0.0767 - Labels_loss: 0.0612 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2829/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2830/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2831/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2832/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2833/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0665 - Labels_loss: 0.0510 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2834/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0719 - Labels_loss: 0.0564 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2835/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5455e-05\n",
      "Epoch 2836/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0689 - Labels_loss: 0.0534 - sum_layer_3_loss: 1.5468e-05\n",
      "Epoch 2837/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2838/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0684 - Labels_loss: 0.0530 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2839/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2840/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2841/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0785 - Labels_loss: 0.0630 - sum_layer_3_loss: 1.5517e-05\n",
      "Epoch 2842/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0752 - Labels_loss: 0.0597 - sum_layer_3_loss: 1.5519e-05\n",
      "Epoch 2843/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2844/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2845/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5468e-05\n",
      "Epoch 2846/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2847/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0767 - Labels_loss: 0.0613 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2848/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2849/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2850/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2851/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2852/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5460e-05\n",
      "Epoch 2853/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2854/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0750 - Labels_loss: 0.0595 - sum_layer_3_loss: 1.5498e-05\n",
      "Epoch 2855/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2856/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5474e-05\n",
      "Epoch 2857/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0698 - Labels_loss: 0.0543 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2858/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2859/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5465e-05\n",
      "Epoch 2860/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0761 - Labels_loss: 0.0606 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2861/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0757 - Labels_loss: 0.0602 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2862/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0570 - sum_layer_3_loss: 1.5513e-05\n",
      "Epoch 2863/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5532e-05\n",
      "Epoch 2864/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2865/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5520e-05\n",
      "Epoch 2866/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5476e-05\n",
      "Epoch 2867/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0687 - Labels_loss: 0.0533 - sum_layer_3_loss: 1.5469e-05\n",
      "Epoch 2868/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2869/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2870/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2871/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0690 - Labels_loss: 0.0535 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2872/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0739 - Labels_loss: 0.0584 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2873/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0822 - Labels_loss: 0.0667 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2874/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0725 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5465e-05\n",
      "Epoch 2875/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0777 - Labels_loss: 0.0622 - sum_layer_3_loss: 1.5522e-05\n",
      "Epoch 2876/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5527e-05\n",
      "Epoch 2877/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5514e-05\n",
      "Epoch 2878/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0590 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2879/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0654 - Labels_loss: 0.0499 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2880/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2881/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5463e-05\n",
      "Epoch 2882/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5510e-05\n",
      "Epoch 2883/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0682 - Labels_loss: 0.0527 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2884/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0553 - sum_layer_3_loss: 1.5467e-05\n",
      "Epoch 2885/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2886/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0706 - Labels_loss: 0.0551 - sum_layer_3_loss: 1.5508e-05\n",
      "Epoch 2887/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2888/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0550 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2889/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0698 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2890/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0739 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2891/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0781 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2892/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0756 - Labels_loss: 0.0601 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2893/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0733 - Labels_loss: 0.0578 - sum_layer_3_loss: 1.5506e-05\n",
      "Epoch 2894/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0694 - Labels_loss: 0.0539 - sum_layer_3_loss: 1.5535e-05\n",
      "Epoch 2895/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5539e-05\n",
      "Epoch 2896/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2897/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0697 - Labels_loss: 0.0542 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2898/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0727 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2899/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2900/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0695 - Labels_loss: 0.0540 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2901/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5500e-05\n",
      "Epoch 2902/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0547 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2903/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0717 - Labels_loss: 0.0563 - sum_layer_3_loss: 1.5455e-05\n",
      "Epoch 2904/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5464e-05\n",
      "Epoch 2905/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2906/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0717 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2907/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0769 - Labels_loss: 0.0614 - sum_layer_3_loss: 1.5509e-05\n",
      "Epoch 2908/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5497e-05\n",
      "Epoch 2909/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2910/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0682 - Labels_loss: 0.0527 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2911/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0672 - Labels_loss: 0.0517 - sum_layer_3_loss: 1.5487e-05\n",
      "Epoch 2912/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5518e-05\n",
      "Epoch 2913/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0693 - Labels_loss: 0.0538 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2914/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0677 - Labels_loss: 0.0522 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2915/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2916/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0754 - Labels_loss: 0.0599 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2917/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0694 - Labels_loss: 0.0539 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2918/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0713 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5467e-05\n",
      "Epoch 2919/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0561 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2920/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0556 - sum_layer_3_loss: 1.5451e-05\n",
      "Epoch 2921/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0781 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2922/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2923/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0774 - Labels_loss: 0.0619 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2924/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0726 - Labels_loss: 0.0571 - sum_layer_3_loss: 1.5474e-05\n",
      "Epoch 2925/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0740 - Labels_loss: 0.0585 - sum_layer_3_loss: 1.5447e-05\n",
      "Epoch 2926/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0806 - Labels_loss: 0.0651 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2927/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2928/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0701 - Labels_loss: 0.0546 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2929/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2930/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5492e-05\n",
      "Epoch 2931/3000\n",
      "242/242 [==============================] - 5s 20ms/step - loss: 0.0750 - Labels_loss: 0.0596 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2932/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5461e-05\n",
      "Epoch 2933/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0730 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5521e-05\n",
      "Epoch 2934/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5489e-05\n",
      "Epoch 2935/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0707 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5464e-05\n",
      "Epoch 2936/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2937/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0799 - Labels_loss: 0.0644 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2938/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2939/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0676 - Labels_loss: 0.0521 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2940/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0738 - Labels_loss: 0.0583 - sum_layer_3_loss: 1.5452e-05\n",
      "Epoch 2941/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0558 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2942/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0702 - Labels_loss: 0.0547 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2943/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5494e-05\n",
      "Epoch 2944/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0729 - Labels_loss: 0.0575 - sum_layer_3_loss: 1.5470e-05\n",
      "Epoch 2945/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0735 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2946/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0745 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2947/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0681 - Labels_loss: 0.0527 - sum_layer_3_loss: 1.5456e-05\n",
      "Epoch 2948/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2949/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0710 - Labels_loss: 0.0555 - sum_layer_3_loss: 1.5495e-05\n",
      "Epoch 2950/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0732 - Labels_loss: 0.0577 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2951/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0693 - Labels_loss: 0.0538 - sum_layer_3_loss: 1.5490e-05\n",
      "Epoch 2952/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0700 - Labels_loss: 0.0545 - sum_layer_3_loss: 1.5501e-05\n",
      "Epoch 2953/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5459e-05\n",
      "Epoch 2954/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0694 - Labels_loss: 0.0540 - sum_layer_3_loss: 1.5438e-05\n",
      "Epoch 2955/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0744 - Labels_loss: 0.0589 - sum_layer_3_loss: 1.5465e-05\n",
      "Epoch 2956/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0553 - sum_layer_3_loss: 1.5474e-05\n",
      "Epoch 2957/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0714 - Labels_loss: 0.0559 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2958/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0699 - Labels_loss: 0.0544 - sum_layer_3_loss: 1.5468e-05\n",
      "Epoch 2959/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0723 - Labels_loss: 0.0568 - sum_layer_3_loss: 1.5488e-05\n",
      "Epoch 2960/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0697 - Labels_loss: 0.0542 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2961/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0693 - Labels_loss: 0.0538 - sum_layer_3_loss: 1.5457e-05\n",
      "Epoch 2962/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2963/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0707 - Labels_loss: 0.0552 - sum_layer_3_loss: 1.5448e-05\n",
      "Epoch 2964/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0712 - Labels_loss: 0.0557 - sum_layer_3_loss: 1.5462e-05\n",
      "Epoch 2965/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5465e-05\n",
      "Epoch 2966/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0691 - Labels_loss: 0.0536 - sum_layer_3_loss: 1.5496e-05\n",
      "Epoch 2967/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0721 - Labels_loss: 0.0566 - sum_layer_3_loss: 1.5466e-05\n",
      "Epoch 2968/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0574 - sum_layer_3_loss: 1.5469e-05\n",
      "Epoch 2969/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0685 - Labels_loss: 0.0531 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2970/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2971/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0708 - Labels_loss: 0.0553 - sum_layer_3_loss: 1.5483e-05\n",
      "Epoch 2972/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0580 - sum_layer_3_loss: 1.5469e-05\n",
      "Epoch 2973/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5504e-05\n",
      "Epoch 2974/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0782 - Labels_loss: 0.0627 - sum_layer_3_loss: 1.5472e-05\n",
      "Epoch 2975/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0709 - Labels_loss: 0.0554 - sum_layer_3_loss: 1.5486e-05\n",
      "Epoch 2976/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0797 - Labels_loss: 0.0642 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2977/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0720 - Labels_loss: 0.0565 - sum_layer_3_loss: 1.5502e-05\n",
      "Epoch 2978/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0704 - Labels_loss: 0.0549 - sum_layer_3_loss: 1.5503e-05\n",
      "Epoch 2979/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0734 - Labels_loss: 0.0579 - sum_layer_3_loss: 1.5473e-05\n",
      "Epoch 2980/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0702 - Labels_loss: 0.0547 - sum_layer_3_loss: 1.5499e-05\n",
      "Epoch 2981/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0691 - Labels_loss: 0.0536 - sum_layer_3_loss: 1.5491e-05\n",
      "Epoch 2982/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0695 - Labels_loss: 0.0540 - sum_layer_3_loss: 1.5481e-05\n",
      "Epoch 2983/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0737 - Labels_loss: 0.0582 - sum_layer_3_loss: 1.5475e-05\n",
      "Epoch 2984/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0724 - Labels_loss: 0.0569 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2985/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0702 - Labels_loss: 0.0547 - sum_layer_3_loss: 1.5485e-05\n",
      "Epoch 2986/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5471e-05\n",
      "Epoch 2987/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0683 - Labels_loss: 0.0529 - sum_layer_3_loss: 1.5477e-05\n",
      "Epoch 2988/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0776 - Labels_loss: 0.0621 - sum_layer_3_loss: 1.5493e-05\n",
      "Epoch 2989/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0722 - Labels_loss: 0.0567 - sum_layer_3_loss: 1.5479e-05\n",
      "Epoch 2990/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0715 - Labels_loss: 0.0560 - sum_layer_3_loss: 1.5484e-05\n",
      "Epoch 2991/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0688 - Labels_loss: 0.0533 - sum_layer_3_loss: 1.5482e-05\n",
      "Epoch 2992/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0758 - Labels_loss: 0.0604 - sum_layer_3_loss: 1.5461e-05\n",
      "Epoch 2993/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0676 - Labels_loss: 0.0522 - sum_layer_3_loss: 1.5451e-05\n",
      "Epoch 2994/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0731 - Labels_loss: 0.0576 - sum_layer_3_loss: 1.5480e-05\n",
      "Epoch 2995/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0787 - Labels_loss: 0.0632 - sum_layer_3_loss: 1.5478e-05\n",
      "Epoch 2996/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0746 - Labels_loss: 0.0591 - sum_layer_3_loss: 1.5512e-05\n",
      "Epoch 2997/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0728 - Labels_loss: 0.0573 - sum_layer_3_loss: 1.5507e-05\n",
      "Epoch 2998/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0680 - Labels_loss: 0.0525 - sum_layer_3_loss: 1.5454e-05\n",
      "Epoch 2999/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0716 - Labels_loss: 0.0562 - sum_layer_3_loss: 1.5444e-05\n",
      "Epoch 3000/3000\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.0753 - Labels_loss: 0.0598 - sum_layer_3_loss: 1.5497e-05\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=[get_weighted_loss(weights),KLDerror],optimizer='adam',loss_weights=[1,1000])\n",
    "history=model.fit(X_train,[y_train,X_train],epochs=3000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a3b3ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "242/242 [==============================] - 129s 529ms/step - loss: 0.0731\n",
      "Epoch 2/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0560\n",
      "Epoch 3/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0630\n",
      "Epoch 4/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0507\n",
      "Epoch 5/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0510\n",
      "Epoch 6/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0551\n",
      "Epoch 7/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0480\n",
      "Epoch 8/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0505\n",
      "Epoch 9/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0585\n",
      "Epoch 10/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0597\n",
      "Epoch 11/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0518\n",
      "Epoch 12/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0461\n",
      "Epoch 13/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0522\n",
      "Epoch 14/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0528\n",
      "Epoch 15/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0481\n",
      "Epoch 16/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0457\n",
      "Epoch 17/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0744\n",
      "Epoch 18/2000\n",
      "242/242 [==============================] - 132s 546ms/step - loss: 0.1050\n",
      "Epoch 19/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0555\n",
      "Epoch 20/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0497\n",
      "Epoch 21/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0497\n",
      "Epoch 22/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0475\n",
      "Epoch 23/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0423\n",
      "Epoch 24/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0430\n",
      "Epoch 25/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0402\n",
      "Epoch 26/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0410\n",
      "Epoch 27/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0437\n",
      "Epoch 28/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0421\n",
      "Epoch 29/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0512\n",
      "Epoch 30/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0455\n",
      "Epoch 31/2000\n",
      "242/242 [==============================] - 129s 533ms/step - loss: 0.0474\n",
      "Epoch 32/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0499\n",
      "Epoch 33/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0622\n",
      "Epoch 34/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0504\n",
      "Epoch 35/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0442\n",
      "Epoch 36/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0541\n",
      "Epoch 37/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0469\n",
      "Epoch 38/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0436\n",
      "Epoch 39/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0429\n",
      "Epoch 40/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0654\n",
      "Epoch 41/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0681\n",
      "Epoch 42/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0478\n",
      "Epoch 43/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0469\n",
      "Epoch 44/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0425\n",
      "Epoch 45/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0431\n",
      "Epoch 46/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0399\n",
      "Epoch 47/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0434\n",
      "Epoch 48/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0419\n",
      "Epoch 49/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0425\n",
      "Epoch 50/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0380\n",
      "Epoch 51/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0480\n",
      "Epoch 52/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0467\n",
      "Epoch 53/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0455\n",
      "Epoch 54/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0470\n",
      "Epoch 55/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0459\n",
      "Epoch 56/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0428\n",
      "Epoch 57/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0700\n",
      "Epoch 58/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0526\n",
      "Epoch 59/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0519\n",
      "Epoch 60/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0541\n",
      "Epoch 61/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0572\n",
      "Epoch 62/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0653\n",
      "Epoch 63/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0448\n",
      "Epoch 64/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0539\n",
      "Epoch 65/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0465\n",
      "Epoch 66/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 67/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0396\n",
      "Epoch 68/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0411\n",
      "Epoch 69/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0428\n",
      "Epoch 70/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0416\n",
      "Epoch 71/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0398\n",
      "Epoch 72/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0398\n",
      "Epoch 73/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0505\n",
      "Epoch 74/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0468\n",
      "Epoch 75/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0392\n",
      "Epoch 76/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 77/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0534\n",
      "Epoch 78/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0632\n",
      "Epoch 79/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0637\n",
      "Epoch 80/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0529\n",
      "Epoch 81/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0432\n",
      "Epoch 82/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0427\n",
      "Epoch 83/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0404\n",
      "Epoch 84/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0401\n",
      "Epoch 85/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0569\n",
      "Epoch 86/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0501\n",
      "Epoch 87/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0453\n",
      "Epoch 88/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0415\n",
      "Epoch 89/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0618\n",
      "Epoch 90/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0475\n",
      "Epoch 91/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 92/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0385\n",
      "Epoch 93/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0566\n",
      "Epoch 94/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0475\n",
      "Epoch 95/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0385\n",
      "Epoch 96/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0438\n",
      "Epoch 97/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 98/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0374\n",
      "Epoch 99/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0583\n",
      "Epoch 100/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0494\n",
      "Epoch 101/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0425\n",
      "Epoch 102/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0389\n",
      "Epoch 103/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0473\n",
      "Epoch 104/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0511\n",
      "Epoch 105/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0516\n",
      "Epoch 106/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0396\n",
      "Epoch 107/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0410\n",
      "Epoch 108/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0382\n",
      "Epoch 109/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0467\n",
      "Epoch 110/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0444\n",
      "Epoch 111/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0594\n",
      "Epoch 112/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0662\n",
      "Epoch 113/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0778\n",
      "Epoch 114/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0597\n",
      "Epoch 115/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0477\n",
      "Epoch 116/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0391\n",
      "Epoch 117/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0380\n",
      "Epoch 118/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0409\n",
      "Epoch 119/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0497\n",
      "Epoch 120/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0511\n",
      "Epoch 121/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0398\n",
      "Epoch 122/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 123/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0392\n",
      "Epoch 124/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0490\n",
      "Epoch 125/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0450\n",
      "Epoch 126/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0523\n",
      "Epoch 127/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0403\n",
      "Epoch 128/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0381\n",
      "Epoch 129/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0369\n",
      "Epoch 130/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0359\n",
      "Epoch 131/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0352\n",
      "Epoch 132/2000\n",
      "242/242 [==============================] - 129s 534ms/step - loss: 0.0372\n",
      "Epoch 133/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 134/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0394\n",
      "Epoch 135/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0419\n",
      "Epoch 136/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0394\n",
      "Epoch 137/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0400\n",
      "Epoch 138/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0439\n",
      "Epoch 139/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0438\n",
      "Epoch 140/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0524\n",
      "Epoch 141/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0420\n",
      "Epoch 142/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0709\n",
      "Epoch 143/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0513\n",
      "Epoch 144/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0417\n",
      "Epoch 145/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0421\n",
      "Epoch 146/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0426\n",
      "Epoch 147/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 148/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0355\n",
      "Epoch 149/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0481\n",
      "Epoch 150/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 151/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0474\n",
      "Epoch 152/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0688\n",
      "Epoch 153/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0417\n",
      "Epoch 154/2000\n",
      "242/242 [==============================] - 131s 539ms/step - loss: 0.0381\n",
      "Epoch 155/2000\n",
      "242/242 [==============================] - 131s 540ms/step - loss: 0.0824\n",
      "Epoch 156/2000\n",
      "242/242 [==============================] - 132s 547ms/step - loss: 0.0585\n",
      "Epoch 157/2000\n",
      "242/242 [==============================] - 133s 549ms/step - loss: 0.0440\n",
      "Epoch 158/2000\n",
      "242/242 [==============================] - 133s 549ms/step - loss: 0.0378\n",
      "Epoch 159/2000\n",
      "242/242 [==============================] - 132s 544ms/step - loss: 0.0436\n",
      "Epoch 160/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0501\n",
      "Epoch 161/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0416\n",
      "Epoch 162/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 163/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0358\n",
      "Epoch 164/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0372\n",
      "Epoch 165/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0372\n",
      "Epoch 166/2000\n",
      "242/242 [==============================] - 130s 536ms/step - loss: 0.0398\n",
      "Epoch 167/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0389\n",
      "Epoch 168/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0450\n",
      "Epoch 169/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0887\n",
      "Epoch 170/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0622\n",
      "Epoch 171/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0447\n",
      "Epoch 172/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0439\n",
      "Epoch 173/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0496\n",
      "Epoch 174/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0451\n",
      "Epoch 175/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0431\n",
      "Epoch 176/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0399\n",
      "Epoch 177/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0387\n",
      "Epoch 178/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0406\n",
      "Epoch 179/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0404\n",
      "Epoch 180/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0528\n",
      "Epoch 181/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0369\n",
      "Epoch 182/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0440\n",
      "Epoch 183/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0383\n",
      "Epoch 184/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0392\n",
      "Epoch 185/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0364\n",
      "Epoch 186/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0410\n",
      "Epoch 187/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0606\n",
      "Epoch 188/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0506\n",
      "Epoch 189/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0439\n",
      "Epoch 190/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0419\n",
      "Epoch 191/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0425\n",
      "Epoch 192/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0396\n",
      "Epoch 193/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0466\n",
      "Epoch 194/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0501\n",
      "Epoch 195/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0438\n",
      "Epoch 196/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0435\n",
      "Epoch 197/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0421\n",
      "Epoch 198/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0395\n",
      "Epoch 199/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0465\n",
      "Epoch 200/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0388\n",
      "Epoch 201/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0440\n",
      "Epoch 202/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0549\n",
      "Epoch 203/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0398\n",
      "Epoch 204/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0386\n",
      "Epoch 205/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0358\n",
      "Epoch 206/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0338\n",
      "Epoch 207/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0383\n",
      "Epoch 208/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0362\n",
      "Epoch 209/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0542\n",
      "Epoch 210/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0522\n",
      "Epoch 211/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0451\n",
      "Epoch 212/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0653\n",
      "Epoch 213/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0461\n",
      "Epoch 214/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0415\n",
      "Epoch 215/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0388\n",
      "Epoch 216/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 217/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0440\n",
      "Epoch 218/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0632\n",
      "Epoch 219/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0388\n",
      "Epoch 220/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0404\n",
      "Epoch 221/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0409\n",
      "Epoch 222/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 223/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0460\n",
      "Epoch 224/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0371\n",
      "Epoch 225/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0652\n",
      "Epoch 226/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0660\n",
      "Epoch 227/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0457\n",
      "Epoch 228/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0387\n",
      "Epoch 229/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0839\n",
      "Epoch 230/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0486\n",
      "Epoch 231/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0448\n",
      "Epoch 232/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 233/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0356\n",
      "Epoch 234/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0350\n",
      "Epoch 235/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0421\n",
      "Epoch 236/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0363\n",
      "Epoch 237/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0387\n",
      "Epoch 238/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0430\n",
      "Epoch 239/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0388\n",
      "Epoch 240/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0354\n",
      "Epoch 241/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0403\n",
      "Epoch 242/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0352\n",
      "Epoch 243/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 244/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 245/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0350\n",
      "Epoch 246/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0753\n",
      "Epoch 247/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0438\n",
      "Epoch 248/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0533\n",
      "Epoch 249/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0430\n",
      "Epoch 250/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0744\n",
      "Epoch 251/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0520\n",
      "Epoch 252/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0472\n",
      "Epoch 253/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0393\n",
      "Epoch 254/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0410\n",
      "Epoch 255/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0347\n",
      "Epoch 256/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0337\n",
      "Epoch 257/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0338\n",
      "Epoch 258/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0447\n",
      "Epoch 259/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0442\n",
      "Epoch 260/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0379\n",
      "Epoch 261/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0336\n",
      "Epoch 262/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0438\n",
      "Epoch 263/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0422\n",
      "Epoch 264/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0417\n",
      "Epoch 265/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0532\n",
      "Epoch 266/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0392\n",
      "Epoch 267/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 268/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0398\n",
      "Epoch 269/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0399\n",
      "Epoch 270/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0408\n",
      "Epoch 271/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0510\n",
      "Epoch 272/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0407\n",
      "Epoch 273/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0364\n",
      "Epoch 274/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0375\n",
      "Epoch 275/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0354\n",
      "Epoch 276/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 277/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 278/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0442\n",
      "Epoch 279/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0593\n",
      "Epoch 280/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0603\n",
      "Epoch 281/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0370\n",
      "Epoch 282/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0344\n",
      "Epoch 283/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0411\n",
      "Epoch 284/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0412\n",
      "Epoch 285/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0355\n",
      "Epoch 286/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0367\n",
      "Epoch 287/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0402\n",
      "Epoch 288/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0423\n",
      "Epoch 289/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0490\n",
      "Epoch 290/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0362\n",
      "Epoch 291/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0364\n",
      "Epoch 292/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0336\n",
      "Epoch 293/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0380\n",
      "Epoch 294/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0352\n",
      "Epoch 295/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0423\n",
      "Epoch 296/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.1005\n",
      "Epoch 297/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0607\n",
      "Epoch 298/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0535\n",
      "Epoch 299/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0450\n",
      "Epoch 300/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0371\n",
      "Epoch 301/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0319\n",
      "Epoch 302/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0360\n",
      "Epoch 303/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 304/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0315\n",
      "Epoch 305/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0324\n",
      "Epoch 306/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0459\n",
      "Epoch 307/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0427\n",
      "Epoch 308/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 309/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0424\n",
      "Epoch 310/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0388\n",
      "Epoch 311/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0470\n",
      "Epoch 312/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0433\n",
      "Epoch 313/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0377\n",
      "Epoch 314/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0360\n",
      "Epoch 315/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 316/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0599\n",
      "Epoch 317/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0588\n",
      "Epoch 318/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0465\n",
      "Epoch 319/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0381\n",
      "Epoch 320/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0522\n",
      "Epoch 321/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0381\n",
      "Epoch 322/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0319\n",
      "Epoch 323/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0313\n",
      "Epoch 324/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0301\n",
      "Epoch 325/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0322\n",
      "Epoch 326/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0325\n",
      "Epoch 327/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 328/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0849\n",
      "Epoch 329/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0437\n",
      "Epoch 330/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0374\n",
      "Epoch 331/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0601\n",
      "Epoch 332/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0381\n",
      "Epoch 333/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0369\n",
      "Epoch 334/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0340\n",
      "Epoch 335/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0343\n",
      "Epoch 336/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 337/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0528\n",
      "Epoch 338/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 339/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0332\n",
      "Epoch 340/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0364\n",
      "Epoch 341/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0585\n",
      "Epoch 342/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0508\n",
      "Epoch 343/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0384\n",
      "Epoch 344/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0335\n",
      "Epoch 345/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0504\n",
      "Epoch 346/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 347/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0364\n",
      "Epoch 348/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0485\n",
      "Epoch 349/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0465\n",
      "Epoch 350/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0351\n",
      "Epoch 351/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0335\n",
      "Epoch 352/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0326\n",
      "Epoch 353/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0294\n",
      "Epoch 354/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0358\n",
      "Epoch 355/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0415\n",
      "Epoch 356/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 357/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0353\n",
      "Epoch 358/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0786\n",
      "Epoch 359/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0551\n",
      "Epoch 360/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0437\n",
      "Epoch 361/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0364\n",
      "Epoch 362/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 363/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0489\n",
      "Epoch 364/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0412\n",
      "Epoch 365/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0407\n",
      "Epoch 366/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0396\n",
      "Epoch 367/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0328\n",
      "Epoch 368/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0374\n",
      "Epoch 369/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 370/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0360\n",
      "Epoch 371/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0392\n",
      "Epoch 372/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0461\n",
      "Epoch 373/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0487\n",
      "Epoch 374/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0344\n",
      "Epoch 375/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0313\n",
      "Epoch 376/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0365\n",
      "Epoch 377/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0350\n",
      "Epoch 378/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0586\n",
      "Epoch 379/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0428\n",
      "Epoch 380/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0371\n",
      "Epoch 381/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0332\n",
      "Epoch 382/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0427\n",
      "Epoch 383/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0397\n",
      "Epoch 384/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0393\n",
      "Epoch 385/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0379\n",
      "Epoch 386/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0460\n",
      "Epoch 387/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0570\n",
      "Epoch 388/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0431\n",
      "Epoch 389/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 390/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0430\n",
      "Epoch 391/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0348\n",
      "Epoch 392/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0434\n",
      "Epoch 393/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0674\n",
      "Epoch 394/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0400\n",
      "Epoch 395/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0363\n",
      "Epoch 396/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0402\n",
      "Epoch 397/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0314\n",
      "Epoch 398/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0833\n",
      "Epoch 399/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0439\n",
      "Epoch 400/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0380\n",
      "Epoch 401/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0338\n",
      "Epoch 402/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0281\n",
      "Epoch 403/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 404/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0366\n",
      "Epoch 405/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0653\n",
      "Epoch 406/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0473\n",
      "Epoch 407/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0426\n",
      "Epoch 408/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0356\n",
      "Epoch 409/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0315\n",
      "Epoch 410/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0493\n",
      "Epoch 411/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0721\n",
      "Epoch 412/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0465\n",
      "Epoch 413/2000\n",
      "242/242 [==============================] - 130s 537ms/step - loss: 0.0410\n",
      "Epoch 414/2000\n",
      "242/242 [==============================] - 131s 541ms/step - loss: 0.0345\n",
      "Epoch 415/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0342\n",
      "Epoch 416/2000\n",
      "242/242 [==============================] - 133s 549ms/step - loss: 0.0373\n",
      "Epoch 417/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0313\n",
      "Epoch 418/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0430\n",
      "Epoch 419/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0479\n",
      "Epoch 420/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0376\n",
      "Epoch 421/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0462\n",
      "Epoch 422/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0590\n",
      "Epoch 423/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0591\n",
      "Epoch 424/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0444\n",
      "Epoch 425/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0436\n",
      "Epoch 426/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0362\n",
      "Epoch 427/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0323\n",
      "Epoch 428/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0317\n",
      "Epoch 429/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0341\n",
      "Epoch 430/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0442\n",
      "Epoch 431/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0349\n",
      "Epoch 432/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0504\n",
      "Epoch 433/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0375\n",
      "Epoch 434/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0296\n",
      "Epoch 435/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0630\n",
      "Epoch 436/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0408\n",
      "Epoch 437/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0324\n",
      "Epoch 438/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0320\n",
      "Epoch 439/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0316\n",
      "Epoch 440/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0467\n",
      "Epoch 441/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0426\n",
      "Epoch 442/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0365\n",
      "Epoch 443/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0375\n",
      "Epoch 444/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0391\n",
      "Epoch 445/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0702\n",
      "Epoch 446/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0400\n",
      "Epoch 447/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0610\n",
      "Epoch 448/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0440\n",
      "Epoch 449/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0334\n",
      "Epoch 450/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0334\n",
      "Epoch 451/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0371\n",
      "Epoch 452/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0313\n",
      "Epoch 453/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0315\n",
      "Epoch 454/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0376\n",
      "Epoch 455/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 456/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 457/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0378\n",
      "Epoch 458/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0655\n",
      "Epoch 459/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0630\n",
      "Epoch 460/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1175\n",
      "Epoch 461/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0756\n",
      "Epoch 462/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0418\n",
      "Epoch 463/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 464/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0343\n",
      "Epoch 465/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0315\n",
      "Epoch 466/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0309\n",
      "Epoch 467/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0351\n",
      "Epoch 468/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0360\n",
      "Epoch 469/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0344\n",
      "Epoch 470/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0673\n",
      "Epoch 471/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0466\n",
      "Epoch 472/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0370\n",
      "Epoch 473/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 474/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0292\n",
      "Epoch 475/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0357\n",
      "Epoch 476/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 477/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0347\n",
      "Epoch 478/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0357\n",
      "Epoch 479/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 480/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 481/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0315\n",
      "Epoch 482/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0289\n",
      "Epoch 483/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 484/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0423\n",
      "Epoch 485/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0328\n",
      "Epoch 486/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0382\n",
      "Epoch 487/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0544\n",
      "Epoch 488/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0355\n",
      "Epoch 489/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0473\n",
      "Epoch 490/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0472\n",
      "Epoch 491/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0432\n",
      "Epoch 492/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0588\n",
      "Epoch 493/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0830\n",
      "Epoch 494/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0446\n",
      "Epoch 495/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0431\n",
      "Epoch 496/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0336\n",
      "Epoch 497/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0375\n",
      "Epoch 498/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0394\n",
      "Epoch 499/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0330\n",
      "Epoch 500/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0342\n",
      "Epoch 501/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 502/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0324\n",
      "Epoch 503/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0281\n",
      "Epoch 504/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0307\n",
      "Epoch 505/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0315\n",
      "Epoch 506/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0420\n",
      "Epoch 507/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0431\n",
      "Epoch 508/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0408\n",
      "Epoch 509/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 510/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0366\n",
      "Epoch 511/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0316\n",
      "Epoch 512/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 513/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0453\n",
      "Epoch 514/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0515\n",
      "Epoch 515/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0427\n",
      "Epoch 516/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0388\n",
      "Epoch 517/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 518/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0380\n",
      "Epoch 519/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 520/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 521/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0709\n",
      "Epoch 522/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0526\n",
      "Epoch 523/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0396\n",
      "Epoch 524/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0380\n",
      "Epoch 525/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0381\n",
      "Epoch 526/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 527/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0404\n",
      "Epoch 528/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0393\n",
      "Epoch 529/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0302\n",
      "Epoch 530/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0354\n",
      "Epoch 531/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0424\n",
      "Epoch 532/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0370\n",
      "Epoch 533/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 534/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0384\n",
      "Epoch 535/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0522\n",
      "Epoch 536/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0451\n",
      "Epoch 537/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0365\n",
      "Epoch 538/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0386\n",
      "Epoch 539/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0341\n",
      "Epoch 540/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 541/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0336\n",
      "Epoch 542/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0376\n",
      "Epoch 543/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0357\n",
      "Epoch 544/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0346\n",
      "Epoch 545/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0335\n",
      "Epoch 546/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0294\n",
      "Epoch 547/2000\n",
      "242/242 [==============================] - 131s 540ms/step - loss: 0.0291\n",
      "Epoch 548/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0784\n",
      "Epoch 549/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0807\n",
      "Epoch 550/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0466\n",
      "Epoch 551/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0360\n",
      "Epoch 552/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0299\n",
      "Epoch 553/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 554/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 555/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0324\n",
      "Epoch 556/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0406\n",
      "Epoch 557/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0351\n",
      "Epoch 558/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0290\n",
      "Epoch 559/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0493\n",
      "Epoch 560/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0506\n",
      "Epoch 561/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0453\n",
      "Epoch 562/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0357\n",
      "Epoch 563/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0363\n",
      "Epoch 564/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0333\n",
      "Epoch 565/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0783\n",
      "Epoch 566/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0378\n",
      "Epoch 567/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0428\n",
      "Epoch 568/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 569/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0317\n",
      "Epoch 570/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0560\n",
      "Epoch 571/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0934\n",
      "Epoch 572/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0798\n",
      "Epoch 573/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0716\n",
      "Epoch 574/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0574\n",
      "Epoch 575/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0437\n",
      "Epoch 576/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0395\n",
      "Epoch 577/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0346\n",
      "Epoch 578/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0308\n",
      "Epoch 579/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 580/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0320\n",
      "Epoch 581/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0282\n",
      "Epoch 582/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 583/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0288\n",
      "Epoch 584/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0320\n",
      "Epoch 585/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0340\n",
      "Epoch 586/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0526\n",
      "Epoch 587/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0633\n",
      "Epoch 588/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0471\n",
      "Epoch 589/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0370\n",
      "Epoch 590/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 591/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0355\n",
      "Epoch 592/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0395\n",
      "Epoch 593/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0359\n",
      "Epoch 594/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0394\n",
      "Epoch 595/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0361\n",
      "Epoch 596/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0510\n",
      "Epoch 597/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0481\n",
      "Epoch 598/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0370\n",
      "Epoch 599/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0350\n",
      "Epoch 600/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0361\n",
      "Epoch 601/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0364\n",
      "Epoch 602/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0422\n",
      "Epoch 603/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0315\n",
      "Epoch 604/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 605/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0367\n",
      "Epoch 606/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0328\n",
      "Epoch 607/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0378\n",
      "Epoch 608/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0423\n",
      "Epoch 609/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0644\n",
      "Epoch 610/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0581\n",
      "Epoch 611/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0380\n",
      "Epoch 612/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0396\n",
      "Epoch 613/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0366\n",
      "Epoch 614/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0373\n",
      "Epoch 615/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0307\n",
      "Epoch 616/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0288\n",
      "Epoch 617/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0328\n",
      "Epoch 618/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0283\n",
      "Epoch 619/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0295\n",
      "Epoch 620/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0339\n",
      "Epoch 621/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0326\n",
      "Epoch 622/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0370\n",
      "Epoch 623/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0323\n",
      "Epoch 624/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 625/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0382\n",
      "Epoch 626/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0409\n",
      "Epoch 627/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0359\n",
      "Epoch 628/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0615\n",
      "Epoch 629/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0459\n",
      "Epoch 630/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0467\n",
      "Epoch 631/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0320\n",
      "Epoch 632/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0494\n",
      "Epoch 633/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0518\n",
      "Epoch 634/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0327\n",
      "Epoch 635/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0505\n",
      "Epoch 636/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0387\n",
      "Epoch 637/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 638/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 639/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 640/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0304\n",
      "Epoch 641/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 642/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0350\n",
      "Epoch 643/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0310\n",
      "Epoch 644/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0363\n",
      "Epoch 645/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0422\n",
      "Epoch 646/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0432\n",
      "Epoch 647/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 648/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 649/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0343\n",
      "Epoch 650/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0477\n",
      "Epoch 651/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0309\n",
      "Epoch 652/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0634\n",
      "Epoch 653/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.1475\n",
      "Epoch 654/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0470\n",
      "Epoch 655/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0393\n",
      "Epoch 656/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 657/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0348\n",
      "Epoch 658/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0342\n",
      "Epoch 659/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0342\n",
      "Epoch 660/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0295\n",
      "Epoch 661/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0555\n",
      "Epoch 662/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0385\n",
      "Epoch 663/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0361\n",
      "Epoch 664/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0369\n",
      "Epoch 665/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 666/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0419\n",
      "Epoch 667/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0365\n",
      "Epoch 668/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0330\n",
      "Epoch 669/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0385\n",
      "Epoch 670/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0375\n",
      "Epoch 671/2000\n",
      "242/242 [==============================] - 130s 535ms/step - loss: 0.0810\n",
      "Epoch 672/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0382\n",
      "Epoch 673/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 674/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0330\n",
      "Epoch 675/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0306\n",
      "Epoch 676/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 677/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 678/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0309\n",
      "Epoch 679/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0333\n",
      "Epoch 680/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0361\n",
      "Epoch 681/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0284\n",
      "Epoch 682/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0649\n",
      "Epoch 683/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0819\n",
      "Epoch 684/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0441\n",
      "Epoch 685/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 686/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0365\n",
      "Epoch 687/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0293\n",
      "Epoch 688/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0323\n",
      "Epoch 689/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0456\n",
      "Epoch 690/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0357\n",
      "Epoch 691/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 692/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0645\n",
      "Epoch 693/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0362\n",
      "Epoch 694/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0301\n",
      "Epoch 695/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 696/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0281\n",
      "Epoch 697/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0329\n",
      "Epoch 698/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0289\n",
      "Epoch 699/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0427\n",
      "Epoch 700/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0440\n",
      "Epoch 701/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0662\n",
      "Epoch 702/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0403\n",
      "Epoch 703/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0392\n",
      "Epoch 704/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 705/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0373\n",
      "Epoch 706/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0545\n",
      "Epoch 707/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0343\n",
      "Epoch 708/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0335\n",
      "Epoch 709/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0431\n",
      "Epoch 710/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0344\n",
      "Epoch 711/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0332\n",
      "Epoch 712/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0349\n",
      "Epoch 713/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0371\n",
      "Epoch 714/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0383\n",
      "Epoch 715/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0308\n",
      "Epoch 716/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0340\n",
      "Epoch 717/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0324\n",
      "Epoch 718/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0310\n",
      "Epoch 719/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0592\n",
      "Epoch 720/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0597\n",
      "Epoch 721/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0383\n",
      "Epoch 722/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0372\n",
      "Epoch 723/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0627\n",
      "Epoch 724/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0622\n",
      "Epoch 725/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0374\n",
      "Epoch 726/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0328\n",
      "Epoch 727/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0337\n",
      "Epoch 728/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 729/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0281\n",
      "Epoch 730/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0254\n",
      "Epoch 731/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0307\n",
      "Epoch 732/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0366\n",
      "Epoch 733/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0492\n",
      "Epoch 734/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0405\n",
      "Epoch 735/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0663\n",
      "Epoch 736/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0449\n",
      "Epoch 737/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0603\n",
      "Epoch 738/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0451\n",
      "Epoch 739/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0342\n",
      "Epoch 740/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0361\n",
      "Epoch 741/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0672\n",
      "Epoch 742/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0503\n",
      "Epoch 743/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 744/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 745/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 746/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 747/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0280\n",
      "Epoch 748/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0270\n",
      "Epoch 749/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0380\n",
      "Epoch 750/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0385\n",
      "Epoch 751/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0310\n",
      "Epoch 752/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0411\n",
      "Epoch 753/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0541\n",
      "Epoch 754/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0362\n",
      "Epoch 755/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0320\n",
      "Epoch 756/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0461\n",
      "Epoch 757/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0386\n",
      "Epoch 758/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0335\n",
      "Epoch 759/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0310\n",
      "Epoch 760/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0289\n",
      "Epoch 761/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0283\n",
      "Epoch 762/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0300\n",
      "Epoch 763/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0348\n",
      "Epoch 764/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0404\n",
      "Epoch 765/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0481\n",
      "Epoch 766/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 767/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0397\n",
      "Epoch 768/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0350\n",
      "Epoch 769/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0424\n",
      "Epoch 770/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0403\n",
      "Epoch 771/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0358\n",
      "Epoch 772/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 773/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 774/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0335\n",
      "Epoch 775/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0403\n",
      "Epoch 776/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0396\n",
      "Epoch 777/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0500\n",
      "Epoch 778/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0360\n",
      "Epoch 779/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0277\n",
      "Epoch 780/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0725\n",
      "Epoch 781/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0461\n",
      "Epoch 782/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0324\n",
      "Epoch 783/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0342\n",
      "Epoch 784/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 785/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 786/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0293\n",
      "Epoch 787/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0379\n",
      "Epoch 788/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0374\n",
      "Epoch 789/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 790/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0397\n",
      "Epoch 791/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0418\n",
      "Epoch 792/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0355\n",
      "Epoch 793/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0404\n",
      "Epoch 794/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0442\n",
      "Epoch 795/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0631\n",
      "Epoch 796/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0730\n",
      "Epoch 797/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0430\n",
      "Epoch 798/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0358\n",
      "Epoch 799/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0349\n",
      "Epoch 800/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 801/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0325\n",
      "Epoch 802/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 803/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0279\n",
      "Epoch 804/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0279\n",
      "Epoch 805/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0300\n",
      "Epoch 806/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0653\n",
      "Epoch 807/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0644\n",
      "Epoch 808/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0383\n",
      "Epoch 809/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 810/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0446\n",
      "Epoch 811/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 812/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0416\n",
      "Epoch 813/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0389\n",
      "Epoch 814/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0392\n",
      "Epoch 815/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 816/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0319\n",
      "Epoch 817/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0358\n",
      "Epoch 818/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0430\n",
      "Epoch 819/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0715\n",
      "Epoch 820/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0420\n",
      "Epoch 821/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0310\n",
      "Epoch 822/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0442\n",
      "Epoch 823/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 824/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0299\n",
      "Epoch 825/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0282\n",
      "Epoch 826/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0273\n",
      "Epoch 827/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0268\n",
      "Epoch 828/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0334\n",
      "Epoch 829/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0351\n",
      "Epoch 830/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0491\n",
      "Epoch 831/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 832/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0524\n",
      "Epoch 833/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 834/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0417\n",
      "Epoch 835/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0445\n",
      "Epoch 836/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0320\n",
      "Epoch 837/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0276\n",
      "Epoch 838/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0594\n",
      "Epoch 839/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0389\n",
      "Epoch 840/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0317\n",
      "Epoch 841/2000\n",
      "242/242 [==============================] - 130s 535ms/step - loss: 0.0266\n",
      "Epoch 842/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0350\n",
      "Epoch 843/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0357\n",
      "Epoch 844/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0362\n",
      "Epoch 845/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0362\n",
      "Epoch 846/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0860\n",
      "Epoch 847/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0457\n",
      "Epoch 848/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0340\n",
      "Epoch 849/2000\n",
      "242/242 [==============================] - 129s 533ms/step - loss: 0.0325\n",
      "Epoch 850/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1051\n",
      "Epoch 851/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0603\n",
      "Epoch 852/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0456\n",
      "Epoch 853/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 854/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0295\n",
      "Epoch 855/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0307\n",
      "Epoch 856/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0331\n",
      "Epoch 857/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0302\n",
      "Epoch 858/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0394\n",
      "Epoch 859/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0263\n",
      "Epoch 860/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 861/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0401\n",
      "Epoch 862/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0400\n",
      "Epoch 863/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 864/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0335\n",
      "Epoch 865/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0612\n",
      "Epoch 866/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0304\n",
      "Epoch 867/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0308\n",
      "Epoch 868/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0259\n",
      "Epoch 869/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 870/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0329\n",
      "Epoch 871/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0472\n",
      "Epoch 872/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0457\n",
      "Epoch 873/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 874/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0284\n",
      "Epoch 875/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 876/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0349\n",
      "Epoch 877/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 878/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0276\n",
      "Epoch 879/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0599\n",
      "Epoch 880/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 881/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0340\n",
      "Epoch 882/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0318\n",
      "Epoch 883/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 884/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0442\n",
      "Epoch 885/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 886/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0480\n",
      "Epoch 887/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0401\n",
      "Epoch 888/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 889/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0334\n",
      "Epoch 890/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0737\n",
      "Epoch 891/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0386\n",
      "Epoch 892/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0352\n",
      "Epoch 893/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0293\n",
      "Epoch 894/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0345\n",
      "Epoch 895/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0335\n",
      "Epoch 896/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0431\n",
      "Epoch 897/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0515\n",
      "Epoch 898/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0377\n",
      "Epoch 899/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0272\n",
      "Epoch 900/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 901/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0266\n",
      "Epoch 902/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0332\n",
      "Epoch 903/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 904/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 905/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0624\n",
      "Epoch 906/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0368\n",
      "Epoch 907/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0394\n",
      "Epoch 908/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0385\n",
      "Epoch 909/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0309\n",
      "Epoch 910/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0279\n",
      "Epoch 911/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0306\n",
      "Epoch 912/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0334\n",
      "Epoch 913/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0281\n",
      "Epoch 914/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0303\n",
      "Epoch 915/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0296\n",
      "Epoch 916/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0279\n",
      "Epoch 917/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0290\n",
      "Epoch 918/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0438\n",
      "Epoch 919/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0557\n",
      "Epoch 920/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0644\n",
      "Epoch 921/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0537\n",
      "Epoch 922/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0420\n",
      "Epoch 923/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0330\n",
      "Epoch 924/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0341\n",
      "Epoch 925/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0284\n",
      "Epoch 926/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 927/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0469\n",
      "Epoch 928/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0406\n",
      "Epoch 929/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0335\n",
      "Epoch 930/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 931/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 932/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0244\n",
      "Epoch 933/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0295\n",
      "Epoch 934/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0311\n",
      "Epoch 935/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0344\n",
      "Epoch 936/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0433\n",
      "Epoch 937/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0330\n",
      "Epoch 938/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 939/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0762\n",
      "Epoch 940/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0603\n",
      "Epoch 941/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0667\n",
      "Epoch 942/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0965\n",
      "Epoch 943/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0605\n",
      "Epoch 944/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0455\n",
      "Epoch 945/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0684\n",
      "Epoch 946/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0364\n",
      "Epoch 947/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0372\n",
      "Epoch 948/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0408\n",
      "Epoch 949/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0356\n",
      "Epoch 950/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 951/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 952/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0272\n",
      "Epoch 953/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0259\n",
      "Epoch 954/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0269\n",
      "Epoch 955/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 956/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 957/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 958/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0561\n",
      "Epoch 959/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0538\n",
      "Epoch 960/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0332\n",
      "Epoch 961/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0623\n",
      "Epoch 962/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0407\n",
      "Epoch 963/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0347\n",
      "Epoch 964/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0364\n",
      "Epoch 965/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0635\n",
      "Epoch 966/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0371\n",
      "Epoch 967/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0377\n",
      "Epoch 968/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0408\n",
      "Epoch 969/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0361\n",
      "Epoch 970/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 971/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0314\n",
      "Epoch 972/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0447\n",
      "Epoch 973/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0497\n",
      "Epoch 974/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0440\n",
      "Epoch 975/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 976/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0310\n",
      "Epoch 977/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0338\n",
      "Epoch 978/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 979/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0311\n",
      "Epoch 980/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0305\n",
      "Epoch 981/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0349\n",
      "Epoch 982/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 983/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 984/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0697\n",
      "Epoch 985/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0540\n",
      "Epoch 986/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.1098\n",
      "Epoch 987/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0482\n",
      "Epoch 988/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0324\n",
      "Epoch 989/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 990/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 991/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0311\n",
      "Epoch 992/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0329\n",
      "Epoch 993/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0331\n",
      "Epoch 994/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0270\n",
      "Epoch 995/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0297\n",
      "Epoch 996/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0349\n",
      "Epoch 997/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0425\n",
      "Epoch 998/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0328\n",
      "Epoch 999/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0432\n",
      "Epoch 1000/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 1001/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0624\n",
      "Epoch 1002/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0385\n",
      "Epoch 1003/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0314\n",
      "Epoch 1004/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0336\n",
      "Epoch 1005/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0277\n",
      "Epoch 1006/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0374\n",
      "Epoch 1007/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0333\n",
      "Epoch 1008/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0307\n",
      "Epoch 1009/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0330\n",
      "Epoch 1010/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0739\n",
      "Epoch 1011/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0401\n",
      "Epoch 1012/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 1013/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0363\n",
      "Epoch 1014/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1015/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0300\n",
      "Epoch 1016/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0273\n",
      "Epoch 1017/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0295\n",
      "Epoch 1018/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0395\n",
      "Epoch 1019/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0407\n",
      "Epoch 1020/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1021/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0392\n",
      "Epoch 1022/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0588\n",
      "Epoch 1023/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0331\n",
      "Epoch 1024/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0318\n",
      "Epoch 1025/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0357\n",
      "Epoch 1026/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 1027/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 1028/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0284\n",
      "Epoch 1029/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1030/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 1031/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0560\n",
      "Epoch 1032/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0338\n",
      "Epoch 1033/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0305\n",
      "Epoch 1034/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0276\n",
      "Epoch 1035/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0301\n",
      "Epoch 1036/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1037/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0475\n",
      "Epoch 1038/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0432\n",
      "Epoch 1039/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0392\n",
      "Epoch 1040/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0549\n",
      "Epoch 1041/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0561\n",
      "Epoch 1042/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0345\n",
      "Epoch 1043/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 1044/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0302\n",
      "Epoch 1045/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0280\n",
      "Epoch 1046/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 1047/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0308\n",
      "Epoch 1048/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0331\n",
      "Epoch 1049/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0295\n",
      "Epoch 1050/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0325\n",
      "Epoch 1051/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 1052/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0406\n",
      "Epoch 1053/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0393\n",
      "Epoch 1054/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0439\n",
      "Epoch 1055/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0331\n",
      "Epoch 1056/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0373\n",
      "Epoch 1057/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0337\n",
      "Epoch 1058/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0654\n",
      "Epoch 1059/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0453\n",
      "Epoch 1060/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0971\n",
      "Epoch 1061/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0412\n",
      "Epoch 1062/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0425\n",
      "Epoch 1063/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0654\n",
      "Epoch 1064/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0524\n",
      "Epoch 1065/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0434\n",
      "Epoch 1066/2000\n",
      "242/242 [==============================] - 129s 533ms/step - loss: 0.0318\n",
      "Epoch 1067/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0403\n",
      "Epoch 1068/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0826\n",
      "Epoch 1069/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0439\n",
      "Epoch 1070/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0537\n",
      "Epoch 1071/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0348\n",
      "Epoch 1072/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 1073/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0298\n",
      "Epoch 1074/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0257\n",
      "Epoch 1075/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0389\n",
      "Epoch 1076/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0349\n",
      "Epoch 1077/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0325\n",
      "Epoch 1078/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0295\n",
      "Epoch 1079/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0413\n",
      "Epoch 1080/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0504\n",
      "Epoch 1081/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0341\n",
      "Epoch 1082/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 1083/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 1084/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0352\n",
      "Epoch 1085/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0326\n",
      "Epoch 1086/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0404\n",
      "Epoch 1087/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0701\n",
      "Epoch 1088/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0412\n",
      "Epoch 1089/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0354\n",
      "Epoch 1090/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0294\n",
      "Epoch 1091/2000\n",
      "242/242 [==============================] - 129s 535ms/step - loss: 0.0501\n",
      "Epoch 1092/2000\n",
      "242/242 [==============================] - 131s 541ms/step - loss: 0.0651\n",
      "Epoch 1093/2000\n",
      "242/242 [==============================] - 130s 539ms/step - loss: 0.0371\n",
      "Epoch 1094/2000\n",
      "242/242 [==============================] - 132s 546ms/step - loss: 0.0375\n",
      "Epoch 1095/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0700\n",
      "Epoch 1096/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0374\n",
      "Epoch 1097/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0385\n",
      "Epoch 1098/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0392\n",
      "Epoch 1099/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 1100/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 1101/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0274\n",
      "Epoch 1102/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0251\n",
      "Epoch 1103/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0260\n",
      "Epoch 1104/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 1105/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0375\n",
      "Epoch 1106/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0329\n",
      "Epoch 1107/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0337\n",
      "Epoch 1108/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0357\n",
      "Epoch 1109/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0514\n",
      "Epoch 1110/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0766\n",
      "Epoch 1111/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0625\n",
      "Epoch 1112/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0415\n",
      "Epoch 1113/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0363\n",
      "Epoch 1114/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0316\n",
      "Epoch 1115/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0335\n",
      "Epoch 1116/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0292\n",
      "Epoch 1117/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0278\n",
      "Epoch 1118/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0268\n",
      "Epoch 1119/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0284\n",
      "Epoch 1120/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0823\n",
      "Epoch 1121/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0467\n",
      "Epoch 1122/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0415\n",
      "Epoch 1123/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1124/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0966\n",
      "Epoch 1125/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0507\n",
      "Epoch 1126/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0568\n",
      "Epoch 1127/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0297\n",
      "Epoch 1128/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0332\n",
      "Epoch 1129/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1130/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0278\n",
      "Epoch 1131/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0349\n",
      "Epoch 1132/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0281\n",
      "Epoch 1133/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0367\n",
      "Epoch 1134/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0299\n",
      "Epoch 1135/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1000\n",
      "Epoch 1136/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0425\n",
      "Epoch 1137/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0378\n",
      "Epoch 1138/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0280\n",
      "Epoch 1139/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0280\n",
      "Epoch 1140/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0321\n",
      "Epoch 1141/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0339\n",
      "Epoch 1142/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0309\n",
      "Epoch 1143/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0320\n",
      "Epoch 1144/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0330\n",
      "Epoch 1145/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0281\n",
      "Epoch 1146/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0442\n",
      "Epoch 1147/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1148/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0401\n",
      "Epoch 1149/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0377\n",
      "Epoch 1150/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0288\n",
      "Epoch 1151/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0303\n",
      "Epoch 1152/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0263\n",
      "Epoch 1153/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0308\n",
      "Epoch 1154/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0351\n",
      "Epoch 1155/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0567\n",
      "Epoch 1156/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1068\n",
      "Epoch 1157/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0465\n",
      "Epoch 1158/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0444\n",
      "Epoch 1159/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0432\n",
      "Epoch 1160/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0576\n",
      "Epoch 1161/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0431\n",
      "Epoch 1162/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0408\n",
      "Epoch 1163/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0367\n",
      "Epoch 1164/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0375\n",
      "Epoch 1165/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0335\n",
      "Epoch 1166/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0622\n",
      "Epoch 1167/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0432\n",
      "Epoch 1168/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0474\n",
      "Epoch 1169/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0369\n",
      "Epoch 1170/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0367\n",
      "Epoch 1171/2000\n",
      "242/242 [==============================] - 130s 536ms/step - loss: 0.0342\n",
      "Epoch 1172/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0344\n",
      "Epoch 1173/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 1174/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1175/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 1176/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0299\n",
      "Epoch 1177/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0274\n",
      "Epoch 1178/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0384\n",
      "Epoch 1179/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0413\n",
      "Epoch 1180/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0453\n",
      "Epoch 1181/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 1182/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0321\n",
      "Epoch 1183/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0269\n",
      "Epoch 1184/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0349\n",
      "Epoch 1185/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0385\n",
      "Epoch 1186/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0293\n",
      "Epoch 1187/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0402\n",
      "Epoch 1188/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0296\n",
      "Epoch 1189/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0296\n",
      "Epoch 1190/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0555\n",
      "Epoch 1191/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0467\n",
      "Epoch 1192/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 1193/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0309\n",
      "Epoch 1194/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0342\n",
      "Epoch 1195/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0589\n",
      "Epoch 1196/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1126\n",
      "Epoch 1197/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0471\n",
      "Epoch 1198/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0339\n",
      "Epoch 1199/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0275\n",
      "Epoch 1200/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0283\n",
      "Epoch 1201/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0438\n",
      "Epoch 1202/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0407\n",
      "Epoch 1203/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0373\n",
      "Epoch 1204/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0289\n",
      "Epoch 1205/2000\n",
      "242/242 [==============================] - 129s 534ms/step - loss: 0.0258\n",
      "Epoch 1206/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0391\n",
      "Epoch 1207/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0359\n",
      "Epoch 1208/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0568\n",
      "Epoch 1209/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0318\n",
      "Epoch 1210/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0295\n",
      "Epoch 1211/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0345\n",
      "Epoch 1212/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0290\n",
      "Epoch 1213/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0935\n",
      "Epoch 1214/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0487\n",
      "Epoch 1215/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0489\n",
      "Epoch 1216/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 1217/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0306\n",
      "Epoch 1218/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0325\n",
      "Epoch 1219/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0305\n",
      "Epoch 1220/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1221/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0320\n",
      "Epoch 1222/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0327\n",
      "Epoch 1223/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0302\n",
      "Epoch 1224/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0308\n",
      "Epoch 1225/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0414\n",
      "Epoch 1226/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0414\n",
      "Epoch 1227/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0544\n",
      "Epoch 1228/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0480\n",
      "Epoch 1229/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0349\n",
      "Epoch 1230/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 1231/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0661\n",
      "Epoch 1232/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0460\n",
      "Epoch 1233/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0901\n",
      "Epoch 1234/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0418\n",
      "Epoch 1235/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0354\n",
      "Epoch 1236/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0305\n",
      "Epoch 1237/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0282\n",
      "Epoch 1238/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0241\n",
      "Epoch 1239/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0275\n",
      "Epoch 1240/2000\n",
      "242/242 [==============================] - 129s 535ms/step - loss: 0.0391\n",
      "Epoch 1241/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0275\n",
      "Epoch 1242/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0319\n",
      "Epoch 1243/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0325\n",
      "Epoch 1244/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 1245/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0316\n",
      "Epoch 1246/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0231\n",
      "Epoch 1247/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1248/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 1249/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 1250/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 1251/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0492\n",
      "Epoch 1252/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0399\n",
      "Epoch 1253/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0471\n",
      "Epoch 1254/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0302\n",
      "Epoch 1255/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0416\n",
      "Epoch 1256/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 1257/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 1258/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0358\n",
      "Epoch 1259/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0424\n",
      "Epoch 1260/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0870\n",
      "Epoch 1261/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0511\n",
      "Epoch 1262/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0365\n",
      "Epoch 1263/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1264/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0300\n",
      "Epoch 1265/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0254\n",
      "Epoch 1266/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0261\n",
      "Epoch 1267/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0282\n",
      "Epoch 1268/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0531\n",
      "Epoch 1269/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 1270/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0344\n",
      "Epoch 1271/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1272/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 1273/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0332\n",
      "Epoch 1274/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0437\n",
      "Epoch 1275/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0452\n",
      "Epoch 1276/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0352\n",
      "Epoch 1277/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1121\n",
      "Epoch 1278/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0587\n",
      "Epoch 1279/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0402\n",
      "Epoch 1280/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0356\n",
      "Epoch 1281/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0371\n",
      "Epoch 1282/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0558\n",
      "Epoch 1283/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0633\n",
      "Epoch 1284/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0367\n",
      "Epoch 1285/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0478\n",
      "Epoch 1286/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0370\n",
      "Epoch 1287/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0706\n",
      "Epoch 1288/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0342\n",
      "Epoch 1289/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0289\n",
      "Epoch 1290/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 1291/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 1292/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0285\n",
      "Epoch 1293/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0295\n",
      "Epoch 1294/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0484\n",
      "Epoch 1295/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0371\n",
      "Epoch 1296/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0368\n",
      "Epoch 1297/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0373\n",
      "Epoch 1298/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0306\n",
      "Epoch 1299/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1300/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 1301/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0336\n",
      "Epoch 1302/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0566\n",
      "Epoch 1303/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0432\n",
      "Epoch 1304/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0592\n",
      "Epoch 1305/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0663\n",
      "Epoch 1306/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0518\n",
      "Epoch 1307/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0406\n",
      "Epoch 1308/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0385\n",
      "Epoch 1309/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0343\n",
      "Epoch 1310/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0290\n",
      "Epoch 1311/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0255\n",
      "Epoch 1312/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0310\n",
      "Epoch 1313/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0268\n",
      "Epoch 1314/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0290\n",
      "Epoch 1315/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0292\n",
      "Epoch 1316/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0544\n",
      "Epoch 1317/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0307\n",
      "Epoch 1318/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0296\n",
      "Epoch 1319/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 1320/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0409\n",
      "Epoch 1321/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 1322/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0374\n",
      "Epoch 1323/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 1324/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0460\n",
      "Epoch 1325/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0683\n",
      "Epoch 1326/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0723\n",
      "Epoch 1327/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0365\n",
      "Epoch 1328/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 1329/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0343\n",
      "Epoch 1330/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0307\n",
      "Epoch 1331/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 1332/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0546\n",
      "Epoch 1333/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0490\n",
      "Epoch 1334/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0469\n",
      "Epoch 1335/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0314\n",
      "Epoch 1336/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0302\n",
      "Epoch 1337/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0457\n",
      "Epoch 1338/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0290\n",
      "Epoch 1339/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0310\n",
      "Epoch 1340/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0270\n",
      "Epoch 1341/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0398\n",
      "Epoch 1342/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0280\n",
      "Epoch 1343/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0256\n",
      "Epoch 1344/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0360\n",
      "Epoch 1345/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0535\n",
      "Epoch 1346/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0324\n",
      "Epoch 1347/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0387\n",
      "Epoch 1348/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0338\n",
      "Epoch 1349/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 1350/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0328\n",
      "Epoch 1351/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0265\n",
      "Epoch 1352/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1353/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0275\n",
      "Epoch 1354/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0414\n",
      "Epoch 1355/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0534\n",
      "Epoch 1356/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0765\n",
      "Epoch 1357/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0732\n",
      "Epoch 1358/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0477\n",
      "Epoch 1359/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0462\n",
      "Epoch 1360/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0360\n",
      "Epoch 1361/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0321\n",
      "Epoch 1362/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0291\n",
      "Epoch 1363/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0251\n",
      "Epoch 1364/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0241\n",
      "Epoch 1365/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0235\n",
      "Epoch 1366/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0273\n",
      "Epoch 1367/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0279\n",
      "Epoch 1368/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0314\n",
      "Epoch 1369/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0456\n",
      "Epoch 1370/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0509\n",
      "Epoch 1371/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 1372/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0348\n",
      "Epoch 1373/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0324\n",
      "Epoch 1374/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 1375/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0353\n",
      "Epoch 1376/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0323\n",
      "Epoch 1377/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 1378/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 1379/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0356\n",
      "Epoch 1380/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0266\n",
      "Epoch 1381/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0487\n",
      "Epoch 1382/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0622\n",
      "Epoch 1383/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1398\n",
      "Epoch 1384/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0447\n",
      "Epoch 1385/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0406\n",
      "Epoch 1386/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0348\n",
      "Epoch 1387/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0342\n",
      "Epoch 1388/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0385\n",
      "Epoch 1389/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0457\n",
      "Epoch 1390/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0319\n",
      "Epoch 1391/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0317\n",
      "Epoch 1392/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 1393/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0403\n",
      "Epoch 1394/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0315\n",
      "Epoch 1395/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0301\n",
      "Epoch 1396/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0414\n",
      "Epoch 1397/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0428\n",
      "Epoch 1398/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0317\n",
      "Epoch 1399/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 1400/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0682\n",
      "Epoch 1401/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0731\n",
      "Epoch 1402/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0951\n",
      "Epoch 1403/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0474\n",
      "Epoch 1404/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0436\n",
      "Epoch 1405/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0322\n",
      "Epoch 1406/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1407/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0270\n",
      "Epoch 1408/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0270\n",
      "Epoch 1409/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0330\n",
      "Epoch 1410/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0442\n",
      "Epoch 1411/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0529\n",
      "Epoch 1412/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0497\n",
      "Epoch 1413/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0428\n",
      "Epoch 1414/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1415/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0270\n",
      "Epoch 1416/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0316\n",
      "Epoch 1417/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0258\n",
      "Epoch 1418/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 1419/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0340\n",
      "Epoch 1420/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0279\n",
      "Epoch 1421/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0307\n",
      "Epoch 1422/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0331\n",
      "Epoch 1423/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 1424/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0282\n",
      "Epoch 1425/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0342\n",
      "Epoch 1426/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0392\n",
      "Epoch 1427/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0361\n",
      "Epoch 1428/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0341\n",
      "Epoch 1429/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0337\n",
      "Epoch 1430/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0367\n",
      "Epoch 1431/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0290\n",
      "Epoch 1432/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0314\n",
      "Epoch 1433/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0470\n",
      "Epoch 1434/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 1435/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0271\n",
      "Epoch 1436/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 1437/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0500\n",
      "Epoch 1438/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0552\n",
      "Epoch 1439/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0403\n",
      "Epoch 1440/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0364\n",
      "Epoch 1441/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1184\n",
      "Epoch 1442/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0701\n",
      "Epoch 1443/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0365\n",
      "Epoch 1444/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0295\n",
      "Epoch 1445/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0289\n",
      "Epoch 1446/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0254\n",
      "Epoch 1447/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0301\n",
      "Epoch 1448/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0254\n",
      "Epoch 1449/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0254\n",
      "Epoch 1450/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0266\n",
      "Epoch 1451/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0258\n",
      "Epoch 1452/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 1453/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0599\n",
      "Epoch 1454/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0313\n",
      "Epoch 1455/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0313\n",
      "Epoch 1456/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0267\n",
      "Epoch 1457/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0314\n",
      "Epoch 1458/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1459/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0598\n",
      "Epoch 1460/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0354\n",
      "Epoch 1461/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 1462/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0325\n",
      "Epoch 1463/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0309\n",
      "Epoch 1464/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0370\n",
      "Epoch 1465/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0320\n",
      "Epoch 1466/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0480\n",
      "Epoch 1467/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0357\n",
      "Epoch 1468/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0281\n",
      "Epoch 1469/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0373\n",
      "Epoch 1470/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0282\n",
      "Epoch 1471/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0329\n",
      "Epoch 1472/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0819\n",
      "Epoch 1473/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0499\n",
      "Epoch 1474/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 1475/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0288\n",
      "Epoch 1476/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0240\n",
      "Epoch 1477/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0302\n",
      "Epoch 1478/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0266\n",
      "Epoch 1479/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0339\n",
      "Epoch 1480/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0443\n",
      "Epoch 1481/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0418\n",
      "Epoch 1482/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 1483/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0255\n",
      "Epoch 1484/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0284\n",
      "Epoch 1485/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0308\n",
      "Epoch 1486/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0408\n",
      "Epoch 1487/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0450\n",
      "Epoch 1488/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0343\n",
      "Epoch 1489/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0271\n",
      "Epoch 1490/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0273\n",
      "Epoch 1491/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0357\n",
      "Epoch 1492/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0328\n",
      "Epoch 1493/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0351\n",
      "Epoch 1494/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0767\n",
      "Epoch 1495/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0787\n",
      "Epoch 1496/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0408\n",
      "Epoch 1497/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0440\n",
      "Epoch 1498/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0533\n",
      "Epoch 1499/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0485\n",
      "Epoch 1500/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0381\n",
      "Epoch 1501/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0424\n",
      "Epoch 1502/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0260\n",
      "Epoch 1503/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0274\n",
      "Epoch 1504/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0261\n",
      "Epoch 1505/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0450\n",
      "Epoch 1506/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0481\n",
      "Epoch 1507/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0371\n",
      "Epoch 1508/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0381\n",
      "Epoch 1509/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0518\n",
      "Epoch 1510/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0338\n",
      "Epoch 1511/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0322\n",
      "Epoch 1512/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0503\n",
      "Epoch 1513/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0602\n",
      "Epoch 1514/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0409\n",
      "Epoch 1515/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0298\n",
      "Epoch 1516/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0272\n",
      "Epoch 1517/2000\n",
      "242/242 [==============================] - 130s 537ms/step - loss: 0.0450\n",
      "Epoch 1518/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0303\n",
      "Epoch 1519/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0263\n",
      "Epoch 1520/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0267\n",
      "Epoch 1521/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0311\n",
      "Epoch 1522/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0458\n",
      "Epoch 1523/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0403\n",
      "Epoch 1524/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0271\n",
      "Epoch 1525/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0261\n",
      "Epoch 1526/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0292\n",
      "Epoch 1527/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0321\n",
      "Epoch 1528/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0303\n",
      "Epoch 1529/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0331\n",
      "Epoch 1530/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0615\n",
      "Epoch 1531/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0416\n",
      "Epoch 1532/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0310\n",
      "Epoch 1533/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0254\n",
      "Epoch 1534/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0502\n",
      "Epoch 1535/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0425\n",
      "Epoch 1536/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0413\n",
      "Epoch 1537/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0438\n",
      "Epoch 1538/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0387\n",
      "Epoch 1539/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0417\n",
      "Epoch 1540/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0571\n",
      "Epoch 1541/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0347\n",
      "Epoch 1542/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0288\n",
      "Epoch 1543/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0278\n",
      "Epoch 1544/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0268\n",
      "Epoch 1545/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0296\n",
      "Epoch 1546/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0259\n",
      "Epoch 1547/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0310\n",
      "Epoch 1548/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0402\n",
      "Epoch 1549/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0394\n",
      "Epoch 1550/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0367\n",
      "Epoch 1551/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1552/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1553/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0332\n",
      "Epoch 1554/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0550\n",
      "Epoch 1555/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0862\n",
      "Epoch 1556/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0601\n",
      "Epoch 1557/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0362\n",
      "Epoch 1558/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1559/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 1560/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0292\n",
      "Epoch 1561/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0275\n",
      "Epoch 1562/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0295\n",
      "Epoch 1563/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0258\n",
      "Epoch 1564/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0261\n",
      "Epoch 1565/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0263\n",
      "Epoch 1566/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0458\n",
      "Epoch 1567/2000\n",
      "242/242 [==============================] - 129s 531ms/step - loss: 0.0550\n",
      "Epoch 1568/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0489\n",
      "Epoch 1569/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0887\n",
      "Epoch 1570/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1192\n",
      "Epoch 1571/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1134\n",
      "Epoch 1572/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0718\n",
      "Epoch 1573/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0455\n",
      "Epoch 1574/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0353\n",
      "Epoch 1575/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0308\n",
      "Epoch 1576/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0339\n",
      "Epoch 1577/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 1578/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0266\n",
      "Epoch 1579/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0458\n",
      "Epoch 1580/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0301\n",
      "Epoch 1581/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0285\n",
      "Epoch 1582/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0414\n",
      "Epoch 1583/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0315\n",
      "Epoch 1584/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 1585/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0313\n",
      "Epoch 1586/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0267\n",
      "Epoch 1587/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0290\n",
      "Epoch 1588/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0283\n",
      "Epoch 1589/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0282\n",
      "Epoch 1590/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0301\n",
      "Epoch 1591/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0304\n",
      "Epoch 1592/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 1593/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0347\n",
      "Epoch 1594/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0572\n",
      "Epoch 1595/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0576\n",
      "Epoch 1596/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0377\n",
      "Epoch 1597/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0488\n",
      "Epoch 1598/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0505\n",
      "Epoch 1599/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0456\n",
      "Epoch 1600/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1601/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0263\n",
      "Epoch 1602/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0339\n",
      "Epoch 1603/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0478\n",
      "Epoch 1604/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0299\n",
      "Epoch 1605/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0296\n",
      "Epoch 1606/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0638\n",
      "Epoch 1607/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0347\n",
      "Epoch 1608/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0304\n",
      "Epoch 1609/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1610/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0340\n",
      "Epoch 1611/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1612/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0372\n",
      "Epoch 1613/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0307\n",
      "Epoch 1614/2000\n",
      "242/242 [==============================] - 129s 533ms/step - loss: 0.0246\n",
      "Epoch 1615/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0764\n",
      "Epoch 1616/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0459\n",
      "Epoch 1617/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0356\n",
      "Epoch 1618/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 1619/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0265\n",
      "Epoch 1620/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0305\n",
      "Epoch 1621/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0362\n",
      "Epoch 1622/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0411\n",
      "Epoch 1623/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0498\n",
      "Epoch 1624/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1298\n",
      "Epoch 1625/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0858\n",
      "Epoch 1626/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0589\n",
      "Epoch 1627/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1628/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0343\n",
      "Epoch 1629/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0354\n",
      "Epoch 1630/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0386\n",
      "Epoch 1631/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 1632/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0292\n",
      "Epoch 1633/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0305\n",
      "Epoch 1634/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 1635/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0242\n",
      "Epoch 1636/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0331\n",
      "Epoch 1637/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0323\n",
      "Epoch 1638/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0262\n",
      "Epoch 1639/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0321\n",
      "Epoch 1640/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0359\n",
      "Epoch 1641/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0332\n",
      "Epoch 1642/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0278\n",
      "Epoch 1643/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0344\n",
      "Epoch 1644/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0426\n",
      "Epoch 1645/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0466\n",
      "Epoch 1646/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0496\n",
      "Epoch 1647/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0357\n",
      "Epoch 1648/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0254\n",
      "Epoch 1649/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0389\n",
      "Epoch 1650/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0398\n",
      "Epoch 1651/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0336\n",
      "Epoch 1652/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0431\n",
      "Epoch 1653/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0294\n",
      "Epoch 1654/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0257\n",
      "Epoch 1655/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0300\n",
      "Epoch 1656/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0338\n",
      "Epoch 1657/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0300\n",
      "Epoch 1658/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0682\n",
      "Epoch 1659/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0573\n",
      "Epoch 1660/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0610\n",
      "Epoch 1661/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0386\n",
      "Epoch 1662/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0463\n",
      "Epoch 1663/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0628\n",
      "Epoch 1664/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0334\n",
      "Epoch 1665/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0247\n",
      "Epoch 1666/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0335\n",
      "Epoch 1667/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0277\n",
      "Epoch 1668/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0313\n",
      "Epoch 1669/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0239\n",
      "Epoch 1670/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0232\n",
      "Epoch 1671/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0271\n",
      "Epoch 1672/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0331\n",
      "Epoch 1673/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0430\n",
      "Epoch 1674/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0267\n",
      "Epoch 1675/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0306\n",
      "Epoch 1676/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0566\n",
      "Epoch 1677/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0472\n",
      "Epoch 1678/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0498\n",
      "Epoch 1679/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0358\n",
      "Epoch 1680/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0311\n",
      "Epoch 1681/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0245\n",
      "Epoch 1682/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0256\n",
      "Epoch 1683/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0302\n",
      "Epoch 1684/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0413\n",
      "Epoch 1685/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0609\n",
      "Epoch 1686/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1687/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0395\n",
      "Epoch 1688/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0303\n",
      "Epoch 1689/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0333\n",
      "Epoch 1690/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0478\n",
      "Epoch 1691/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 1692/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 1693/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0267\n",
      "Epoch 1694/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0366\n",
      "Epoch 1695/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0395\n",
      "Epoch 1696/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0409\n",
      "Epoch 1697/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0677\n",
      "Epoch 1698/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0555\n",
      "Epoch 1699/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0362\n",
      "Epoch 1700/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0369\n",
      "Epoch 1701/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0262\n",
      "Epoch 1702/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0258\n",
      "Epoch 1703/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0251\n",
      "Epoch 1704/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0258\n",
      "Epoch 1705/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0366\n",
      "Epoch 1706/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0535\n",
      "Epoch 1707/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0375\n",
      "Epoch 1708/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0386\n",
      "Epoch 1709/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0326\n",
      "Epoch 1710/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0410\n",
      "Epoch 1711/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0399\n",
      "Epoch 1712/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0317\n",
      "Epoch 1713/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0279\n",
      "Epoch 1714/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0291\n",
      "Epoch 1715/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0560\n",
      "Epoch 1716/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0576\n",
      "Epoch 1717/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0518\n",
      "Epoch 1718/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0502\n",
      "Epoch 1719/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0451\n",
      "Epoch 1720/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0294\n",
      "Epoch 1721/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0298\n",
      "Epoch 1722/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0351\n",
      "Epoch 1723/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0303\n",
      "Epoch 1724/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0373\n",
      "Epoch 1725/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 1726/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0253\n",
      "Epoch 1727/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0290\n",
      "Epoch 1728/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0278\n",
      "Epoch 1729/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1013\n",
      "Epoch 1730/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0417\n",
      "Epoch 1731/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0325\n",
      "Epoch 1732/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0310\n",
      "Epoch 1733/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0305\n",
      "Epoch 1734/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0292\n",
      "Epoch 1735/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0247\n",
      "Epoch 1736/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0263\n",
      "Epoch 1737/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0290\n",
      "Epoch 1738/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1739/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1480\n",
      "Epoch 1740/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0762\n",
      "Epoch 1741/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0480\n",
      "Epoch 1742/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0376\n",
      "Epoch 1743/2000\n",
      "242/242 [==============================] - 132s 547ms/step - loss: 0.0342\n",
      "Epoch 1744/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0296\n",
      "Epoch 1745/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0240\n",
      "Epoch 1746/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0279\n",
      "Epoch 1747/2000\n",
      "242/242 [==============================] - 128s 531ms/step - loss: 0.0513\n",
      "Epoch 1748/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0734\n",
      "Epoch 1749/2000\n",
      "242/242 [==============================] - 129s 532ms/step - loss: 0.0417\n",
      "Epoch 1750/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0324\n",
      "Epoch 1751/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.1174\n",
      "Epoch 1752/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0480\n",
      "Epoch 1753/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0311\n",
      "Epoch 1754/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0442\n",
      "Epoch 1755/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0335\n",
      "Epoch 1756/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0323\n",
      "Epoch 1757/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 1758/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0268\n",
      "Epoch 1759/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0288\n",
      "Epoch 1760/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0257\n",
      "Epoch 1761/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0312\n",
      "Epoch 1762/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0555\n",
      "Epoch 1763/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0322\n",
      "Epoch 1764/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0285\n",
      "Epoch 1765/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0342\n",
      "Epoch 1766/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0370\n",
      "Epoch 1767/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0324\n",
      "Epoch 1768/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0379\n",
      "Epoch 1769/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0686\n",
      "Epoch 1770/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0830\n",
      "Epoch 1771/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0432\n",
      "Epoch 1772/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0475\n",
      "Epoch 1773/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0327\n",
      "Epoch 1774/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0257\n",
      "Epoch 1775/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0253\n",
      "Epoch 1776/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0262\n",
      "Epoch 1777/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0238\n",
      "Epoch 1778/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0293\n",
      "Epoch 1779/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0347\n",
      "Epoch 1780/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0283\n",
      "Epoch 1781/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0269\n",
      "Epoch 1782/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0349\n",
      "Epoch 1783/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 1784/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 1785/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0397\n",
      "Epoch 1786/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0375\n",
      "Epoch 1787/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0320\n",
      "Epoch 1788/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0327\n",
      "Epoch 1789/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0430\n",
      "Epoch 1790/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0355\n",
      "Epoch 1791/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1231\n",
      "Epoch 1792/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0506\n",
      "Epoch 1793/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0423\n",
      "Epoch 1794/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0309\n",
      "Epoch 1795/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0252\n",
      "Epoch 1796/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0562\n",
      "Epoch 1797/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0364\n",
      "Epoch 1798/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0276\n",
      "Epoch 1799/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0308\n",
      "Epoch 1800/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 1801/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0268\n",
      "Epoch 1802/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0313\n",
      "Epoch 1803/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0248\n",
      "Epoch 1804/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 1805/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0537\n",
      "Epoch 1806/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0401\n",
      "Epoch 1807/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0294\n",
      "Epoch 1808/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0419\n",
      "Epoch 1809/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0469\n",
      "Epoch 1810/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0279\n",
      "Epoch 1811/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0291\n",
      "Epoch 1812/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0300\n",
      "Epoch 1813/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0342\n",
      "Epoch 1814/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0275\n",
      "Epoch 1815/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0286\n",
      "Epoch 1816/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0800\n",
      "Epoch 1817/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0800\n",
      "Epoch 1818/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0726\n",
      "Epoch 1819/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0383\n",
      "Epoch 1820/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0341\n",
      "Epoch 1821/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0502\n",
      "Epoch 1822/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 1823/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0264\n",
      "Epoch 1824/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0234\n",
      "Epoch 1825/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0250\n",
      "Epoch 1826/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0504\n",
      "Epoch 1827/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0292\n",
      "Epoch 1828/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0414\n",
      "Epoch 1829/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0324\n",
      "Epoch 1830/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0292\n",
      "Epoch 1831/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0314\n",
      "Epoch 1832/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0276\n",
      "Epoch 1833/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0495\n",
      "Epoch 1834/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0616\n",
      "Epoch 1835/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0400\n",
      "Epoch 1836/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0746\n",
      "Epoch 1837/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 1838/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0297\n",
      "Epoch 1839/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0297\n",
      "Epoch 1840/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0273\n",
      "Epoch 1841/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0273\n",
      "Epoch 1842/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0292\n",
      "Epoch 1843/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0334\n",
      "Epoch 1844/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0279\n",
      "Epoch 1845/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0329\n",
      "Epoch 1846/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0457\n",
      "Epoch 1847/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0697\n",
      "Epoch 1848/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0430\n",
      "Epoch 1849/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1850/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0283\n",
      "Epoch 1851/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0317\n",
      "Epoch 1852/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0272\n",
      "Epoch 1853/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0387\n",
      "Epoch 1854/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0299\n",
      "Epoch 1855/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0287\n",
      "Epoch 1856/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0257\n",
      "Epoch 1857/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0263\n",
      "Epoch 1858/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0258\n",
      "Epoch 1859/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0262\n",
      "Epoch 1860/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1022\n",
      "Epoch 1861/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0684\n",
      "Epoch 1862/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0432\n",
      "Epoch 1863/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0336\n",
      "Epoch 1864/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0302\n",
      "Epoch 1865/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0264\n",
      "Epoch 1866/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0241\n",
      "Epoch 1867/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0353\n",
      "Epoch 1868/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0565\n",
      "Epoch 1869/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1012\n",
      "Epoch 1870/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0682\n",
      "Epoch 1871/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0434\n",
      "Epoch 1872/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0347\n",
      "Epoch 1873/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0328\n",
      "Epoch 1874/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0288\n",
      "Epoch 1875/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0300\n",
      "Epoch 1876/2000\n",
      "242/242 [==============================] - 130s 536ms/step - loss: 0.0264\n",
      "Epoch 1877/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0243\n",
      "Epoch 1878/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0234\n",
      "Epoch 1879/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0257\n",
      "Epoch 1880/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0270\n",
      "Epoch 1881/2000\n",
      "242/242 [==============================] - 130s 537ms/step - loss: 0.0248\n",
      "Epoch 1882/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0272\n",
      "Epoch 1883/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0850\n",
      "Epoch 1884/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0636\n",
      "Epoch 1885/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0420\n",
      "Epoch 1886/2000\n",
      "242/242 [==============================] - 130s 538ms/step - loss: 0.0273\n",
      "Epoch 1887/2000\n",
      "242/242 [==============================] - 134s 553ms/step - loss: 0.0268\n",
      "Epoch 1888/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0605\n",
      "Epoch 1889/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0529\n",
      "Epoch 1890/2000\n",
      "242/242 [==============================] - 127s 525ms/step - loss: 0.0352\n",
      "Epoch 1891/2000\n",
      "242/242 [==============================] - 127s 525ms/step - loss: 0.0314\n",
      "Epoch 1892/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0404\n",
      "Epoch 1893/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0262\n",
      "Epoch 1894/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0371\n",
      "Epoch 1895/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0334\n",
      "Epoch 1896/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0324\n",
      "Epoch 1897/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0339\n",
      "Epoch 1898/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0357\n",
      "Epoch 1899/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0356\n",
      "Epoch 1900/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0464\n",
      "Epoch 1901/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0310\n",
      "Epoch 1902/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0285\n",
      "Epoch 1903/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0391\n",
      "Epoch 1904/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0308\n",
      "Epoch 1905/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0288\n",
      "Epoch 1906/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0243\n",
      "Epoch 1907/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0273\n",
      "Epoch 1908/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0467\n",
      "Epoch 1909/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0320\n",
      "Epoch 1910/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0318\n",
      "Epoch 1911/2000\n",
      "242/242 [==============================] - 127s 526ms/step - loss: 0.0494\n",
      "Epoch 1912/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0363\n",
      "Epoch 1913/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0304\n",
      "Epoch 1914/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0306\n",
      "Epoch 1915/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0332\n",
      "Epoch 1916/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0630\n",
      "Epoch 1917/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0368\n",
      "Epoch 1918/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0436\n",
      "Epoch 1919/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0427\n",
      "Epoch 1920/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.1073\n",
      "Epoch 1921/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0417\n",
      "Epoch 1922/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0519\n",
      "Epoch 1923/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0504\n",
      "Epoch 1924/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0340\n",
      "Epoch 1925/2000\n",
      "242/242 [==============================] - 127s 527ms/step - loss: 0.0367\n",
      "Epoch 1926/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0278\n",
      "Epoch 1927/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0259\n",
      "Epoch 1928/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0236\n",
      "Epoch 1929/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0251\n",
      "Epoch 1930/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0246\n",
      "Epoch 1931/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0268\n",
      "Epoch 1932/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0345\n",
      "Epoch 1933/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0319\n",
      "Epoch 1934/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0285\n",
      "Epoch 1935/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0263\n",
      "Epoch 1936/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0346\n",
      "Epoch 1937/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0756\n",
      "Epoch 1938/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0674\n",
      "Epoch 1939/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0403\n",
      "Epoch 1940/2000\n",
      "242/242 [==============================] - 128s 530ms/step - loss: 0.0323\n",
      "Epoch 1941/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0407\n",
      "Epoch 1942/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0283\n",
      "Epoch 1943/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0275\n",
      "Epoch 1944/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0313\n",
      "Epoch 1945/2000\n",
      "242/242 [==============================] - 128s 529ms/step - loss: 0.0257\n",
      "Epoch 1946/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0287\n",
      "Epoch 1947/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0312\n",
      "Epoch 1948/2000\n",
      "242/242 [==============================] - 128s 527ms/step - loss: 0.0288\n",
      "Epoch 1949/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0416\n",
      "Epoch 1950/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0265\n",
      "Epoch 1951/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0379\n",
      "Epoch 1952/2000\n",
      "242/242 [==============================] - 128s 528ms/step - loss: 0.0304\n",
      "Epoch 1953/2000\n",
      "212/242 [=========================>....] - ETA: 16s - loss: 0.0509"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m red_model\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(model\u001b[38;5;241m.\u001b[39minput,model\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m red_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mget_weighted_loss(weights),optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mred_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ANN-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "red_model=keras.Model(model.input,model.output[0])\n",
    "red_model.compile(loss=get_weighted_loss(weights),optimizer='adam')\n",
    "history=red_model.fit(X_train,y_train,epochs=2000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "287d30da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "ind=1055\n",
    "pred=model.predict(np.expand_dims(X_test[ind],axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a722c7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wave=np.linspace(400,4000,901)\n",
    "fig,ax=pyplot.subplots(1,1, figsize=(5,5))\n",
    "ax.plot(wave,pred[1].reshape((901,1)),'r',linewidth=3,label='NN Output')\n",
    "#ax.plot(wave,Y_recon,'b',linewidth=3,label='Reconstructed')\n",
    "ax.plot(wave,X_test[ind],'g',linewidth=3, label= 'Original')\n",
    "ax.legend(loc=\"upper right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a7a6f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[1 0 0 1 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(pred[0].round())\n",
    "print(y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "801b549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Gamma--------------------\n",
      "[ 72.51072    57.120792   35.902256  101.66737    29.56556    68.18281\n",
      "  84.66051    56.45956    67.912155   32.40725    77.5422     34.615086\n",
      " 159.9694     61.264687  176.26796    96.63508     4.2520146  36.533073\n",
      "   0.8269087  49.707355 ]\n",
      "---------------Sum Weights-----------------\n",
      "[array([[ 7.3575921e-02],\n",
      "       [ 9.5886886e-02],\n",
      "       [-0.0000000e+00],\n",
      "       [ 6.7601926e-05],\n",
      "       [-0.0000000e+00],\n",
      "       [ 2.6302306e-02],\n",
      "       [-0.0000000e+00],\n",
      "       [ 2.9896477e-02],\n",
      "       [-0.0000000e+00],\n",
      "       [ 6.3186981e-02],\n",
      "       [ 8.5553899e-02],\n",
      "       [ 2.5847759e-02],\n",
      "       [ 1.4040120e+00],\n",
      "       [ 3.9668322e-02],\n",
      "       [ 7.6239222e-01],\n",
      "       [ 3.3212916e-03],\n",
      "       [ 1.2025350e-03],\n",
      "       [-0.0000000e+00],\n",
      "       [ 5.5336483e-02],\n",
      "       [ 1.5163042e-02]], dtype=float32)]\n",
      "----------------Sig Weights-----------------\n",
      "[array([[-3.6813587e-02, -1.5150027e-02, -6.5650232e-02,  3.4214661e-04,\n",
      "        -9.2430174e-02, -1.0782003e-01, -2.3207391e-02,  5.7430573e-02,\n",
      "        -6.5685011e-02,  3.8918933e-01, -1.5961012e-02,  6.1389683e-03,\n",
      "        -8.5419960e-02],\n",
      "       [ 9.0237193e-02, -3.9693616e-02, -2.5360176e-01,  3.6756564e-02,\n",
      "         1.2098318e-02, -1.3337974e-01,  3.3766691e-02,  4.2590950e-02,\n",
      "         1.5575816e-01,  1.0209222e-02, -6.0877218e-03,  2.1883504e-01,\n",
      "        -6.5929443e-02],\n",
      "       [ 4.7386736e-02, -1.5486250e-02,  2.2245069e-01,  2.6316365e-02,\n",
      "        -4.2208788e-01, -1.2699860e+00, -1.6984993e-01, -3.3629248e-01,\n",
      "         6.4953111e-02,  7.5312089e-03,  1.5718671e-02,  8.4398538e-03,\n",
      "        -1.1357064e-01],\n",
      "       [ 2.0919114e-01, -1.6851810e-01, -6.1406732e-01,  6.8955738e-03,\n",
      "        -8.8487521e-02, -4.6118024e-01, -9.9690057e-02,  5.3216305e-02,\n",
      "        -1.8429691e-02, -4.6410218e-02,  2.1535525e-01, -1.4057244e-01,\n",
      "        -2.5880605e-02],\n",
      "       [-8.2495503e-02, -7.7003305e-04, -1.8641286e-01,  2.3160741e-02,\n",
      "        -1.0464118e-01,  5.2707976e-01, -2.4550734e-02, -9.6929997e-02,\n",
      "        -1.1606420e-01, -7.3324693e-03, -6.8676896e-02, -1.6335221e-02,\n",
      "        -7.0172027e-02],\n",
      "       [-2.5549108e-02, -2.4530509e-02,  3.7673688e-01,  3.4874961e-02,\n",
      "        -2.4789205e-01, -2.4196455e-01, -7.4496634e-02,  3.1338900e-01,\n",
      "        -5.8012595e-03, -1.3516147e-01, -1.4862238e-01, -7.2738521e-02,\n",
      "         4.5834884e-02],\n",
      "       [ 2.0587321e-01, -1.0407330e-02, -1.8532874e-01, -1.3862751e-02,\n",
      "        -5.4244623e-02,  3.5469347e-01, -4.8505921e-02, -4.4090118e-02,\n",
      "        -3.8427559e-01,  9.0349857e-03,  4.2971551e-02, -8.4041217e-03,\n",
      "        -1.2986109e-01],\n",
      "       [ 1.8631869e-03, -8.7077223e-04,  6.4318791e-02,  1.4658841e-02,\n",
      "         3.3248343e-02, -1.3095099e-02, -7.5038434e-03, -5.3628460e-02,\n",
      "        -1.5300929e-02,  3.2657210e-02, -1.4294064e-02, -8.1170984e-03,\n",
      "         4.1288045e-01],\n",
      "       [-7.4906021e-02, -4.9650673e-02,  8.6360760e-02,  1.0265414e-01,\n",
      "         1.7238878e-01, -4.8117110e-01, -5.2880989e-03,  6.6736706e-02,\n",
      "        -7.3988505e-02, -1.2932292e-01,  1.2007814e-01, -1.5118273e-01,\n",
      "        -1.3289726e-01],\n",
      "       [ 4.4540241e-02, -2.2796536e-02,  1.6368861e-01, -2.3721179e-01,\n",
      "         4.3541353e-02, -1.5151188e-01, -2.9882904e-02, -2.3166850e-02,\n",
      "         1.2948501e-01,  2.9937698e-02,  2.0568522e-02, -3.7761025e-02,\n",
      "        -3.7294518e-02],\n",
      "       [ 4.2825313e+00, -1.5178612e-01, -2.4998820e+00, -4.0500042e-01,\n",
      "        -4.7400837e+00, -2.4378030e+00, -3.2384398e+00, -2.7532248e+00,\n",
      "        -2.8805797e+00, -2.5999587e+00, -1.9374460e+00, -1.1443937e+00,\n",
      "        -1.9669590e+00],\n",
      "       [ 6.3643441e-02, -2.2809070e-02, -4.4910446e-01,  4.7011875e-02,\n",
      "         5.9032038e-02, -4.1481405e-01, -5.6582924e-02, -1.4677444e-01,\n",
      "         1.1544070e-01,  5.5640388e-02, -2.0384412e-01, -1.1821010e-01,\n",
      "        -1.2568370e-01],\n",
      "       [-3.9524534e-03,  3.2677954e-01, -4.2125862e-02, -1.9667309e-02,\n",
      "         6.6943094e-02, -1.8317159e-01, -3.7112648e-03, -3.0040199e-02,\n",
      "        -1.8973738e-02, -5.6998571e-03, -4.4383731e-02,  1.4526271e-02,\n",
      "        -2.8778648e-02],\n",
      "       [-5.4837555e-01, -9.9274367e-01, -5.9474927e-01,  8.9697406e-02,\n",
      "        -1.0744492e+00, -4.1094685e-01, -1.8018051e+00, -1.3187937e+00,\n",
      "        -3.6208713e-01, -1.6036634e+00, -1.1395539e+00, -2.0140979e+00,\n",
      "        -9.5929408e-01],\n",
      "       [ 1.2517763e-02,  6.3769735e-02,  1.5740381e-01, -1.5664319e-02,\n",
      "        -1.1136077e-01, -2.2094777e-01,  1.6355044e-01, -9.1529816e-02,\n",
      "        -5.8273848e-02,  1.3841711e-02, -4.6184808e-02,  5.8074880e-02,\n",
      "        -2.4774496e-02]], dtype=float32), array([-0.61833096, -1.0642762 , -6.764169  , -3.1257184 , -0.5285486 ,\n",
      "        2.2382596 ,  0.40895075, -1.949818  , -2.6277034 , -3.6957378 ,\n",
      "        1.3993654 ,  0.793009  , -4.078826  ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "wt=model.layers[8].get_weights()\n",
    "wt1=model.layers[10].get_weights()\n",
    "wt2=model.layers[9].get_weights()\n",
    "print('------------------------Gamma--------------------')\n",
    "print(wt[1])\n",
    "print('---------------Sum Weights-----------------')\n",
    "print(wt1)\n",
    "print('----------------Sig Weights-----------------')\n",
    "print(wt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fa9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 901, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 901, 10)      60          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 450, 10)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 450, 10)      510         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 225, 10)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 225, 10)      0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2250)         0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           45020       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " X0 (Dense)                     (None, 20)           320         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " lorentz_layer (Lorentz_layer)  (None, 901, 20)      420         ['X0[0][0]']                     \n",
      "                                                                                                  \n",
      " Labels (Dense)                 (None, 13)           208         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " sum_layer (sum_layer)          (None, 901, 1)       20          ['lorentz_layer[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,873\n",
      "Trainable params: 46,873\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e84828f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "[[ 64890.555   -7730.7686  62273.074   89422.445   15659.493   63024.22\n",
      "    7532.898   49987.73    -8007.0166  57083.312   54375.457   47279.746\n",
      "   -5330.151   54576.93   -11270.295  -11099.08    67420.19     4520.674\n",
      "  -10351.006   64089.957 ]]\n"
     ]
    }
   ],
   "source": [
    "newmod=keras.Model(model.input,model.layers[7].output)\n",
    "x0=newmod.predict(np.expand_dims(X_test[ind],axis=0))\n",
    "print(x0@wt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615af8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: SavedModels/NewArch_WithPooling_09032323\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: SavedModels/NewArch_WithPooling_09032323\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('SavedModels/NewArch_WithPooling_09032323')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ef13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
